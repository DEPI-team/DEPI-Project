{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9369105,"sourceType":"datasetVersion","datasetId":5681965},{"sourceId":9376607,"sourceType":"datasetVersion","datasetId":5687713},{"sourceId":9376615,"sourceType":"datasetVersion","datasetId":5687719},{"sourceId":9394033,"sourceType":"datasetVersion","datasetId":5701159},{"sourceId":9415106,"sourceType":"datasetVersion","datasetId":5717842},{"sourceId":9427559,"sourceType":"datasetVersion","datasetId":5727054}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install imdbPY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\nimport csv\nimport pandas as pd\nfrom imdb import IMDb, IMDbDataAccessError","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ia = imdb.IMDb()\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(r'/kaggle/input/movielens/movies.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#old verison \nimport imdb\nimport csv\nimport random\nimport pandas as pd\nimport concurrent.futures\n\n# Load the movie data\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        data = {\n            'film_name': movie.get('title', ''),\n            'film_rate': movie.get('rating', 0.0),  # default to 0.0 if no rating\n            'plot': movie.get('plot', ''),\n            'cover_url': movie.get('cover url', ''),\n            'cast': ', '.join([person['name'] for person in movie.get('cast', [])[:3]]) if movie.get('cast') else '',  # return empty string if cast is not available\n            'reviews': movie.get('reviews', [{}])[0].get('content', '')  # default to empty string if no reviews\n        }\n        return data\n    except IMDbDataAccessError as e:\n        return {'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': ''}  # return default values\n    except urllib.error.HTTPError as e:\n        return {'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': ''}  # return default values\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\ncsv_file='/kaggle/input/imdbdata/imdb_data.csv'\n# Save the processed data to a CSV file\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['film_name', 'film_rate', 'plot', 'cover_url', 'cast', 'reviews'])\n    for data in data_list:\n        writer.writerow(list(data.values()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# updated version with more features ","metadata":{}},{"cell_type":"code","source":"import imdb\nimport csv\nimport random\nimport pandas as pd\nimport concurrent.futures\nimport urllib.error\n\n\n# Load the movie data\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        ia.update(movie, 'directors')\n        ia.update(movie, 'writers')\n\n        data = {\n            'imdbId': movie_id,\n            'film_name': movie.get('title', ''),\n            'film_rate': movie.get('rating', 0.0),  # default to 0.0 if no rating\n            'plot': movie.get('plot', [''])[0],  # take the first plot synopsis\n            'cover_url': movie.get('cover url', ''),\n            'cast': ', '.join([person['name'] for person in movie.get('cast', [])[:3]]) if movie.get('cast') else '',  # top 3 cast\n            'reviews': movie.get('reviews', [{}])[0].get('content', ''),  # first review content\n            'director': ', '.join([person['name'] for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'writer': ', '.join([person.get('name', '') for person in movie.get('writers', [])]) if movie.get('writers') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''  # join genre list\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        return {'imdbId': movie_id, 'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': '', 'director': '', 'writer': '', 'genre': ''}\n    except urllib.error.HTTPError as e:\n        return {'imdbId': movie_id, 'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': '', 'director': '', 'writer': '', 'genre': ''}\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Specify the path to save the CSV\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'\n\n# Save the processed data to a CSV file\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'film_rate', 'plot', 'cover_url', 'cast', 'reviews', 'director', 'writer', 'genre'])\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['film_rate'], data['plot'], data['cover_url'], data['cast'], data['reviews'], data['director'], data['writer'], data['genre']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# missing features only ","metadata":{}},{"cell_type":"code","source":"import imdb\nimport csv\nimport pandas as pd\nimport concurrent.futures\nimport urllib.error\n\nimdbs = data['imdbId']\n\nia = imdb.IMDb()\n\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, info=['directors', 'genres'])  # Only updating required fields\n\n        data = {\n            'imdbId': movie_id,\n            'film_name': movie.get('title', ''),\n            'director': ', '.join([person['name'] for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        # Handle IMDb Data Access Error\n        return {'imdbId': movie_id, 'film_name': '', 'director': '', 'genre': ''}\n    except urllib.error.HTTPError as e:\n        # Handle HTTP Error\n        return {'imdbId': movie_id, 'film_name': '', 'director': '', 'genre': ''}\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Specify the path to save the CSV\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'\n\n# Save the processed data to a CSV file with only imdbId, film_name, director, and genre\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'director', 'genre'])  # Header row\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['director'], data['genre']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the path to save the CSV in the writable /kaggle/working directory\ncsv_file = '/kaggle/working/imdb_data.csv'  # Change the directory to /kaggle/working\n\n# Save the processed data to a CSV file with only imdbId, film_name, director, and genre\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'director', 'genre'])  # Header row\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['director'], data['genre']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# handling data more effectivly","metadata":{}},{"cell_type":"code","source":"import imdb\nimport csv\nimport pandas as pd\nimport concurrent.futures\nimport urllib.error\nimport time\nfrom tqdm import tqdm  # For progress tracking\n\n# Load the movie IDs (Assuming 'data' is a DataFrame that contains imdbId)\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data with retry logic\ndef get_movie_data(movie_id, retries=3):\n    for _ in range(retries):\n        try:\n            movie = ia.get_movie(movie_id)\n\n            # Fetch the required data without explicitly updating 'directors' or 'genres'\n            data = {\n                'imdbId': movie_id,\n                'film_name': movie.get('title', ''),\n                'director': ', '.join([person['name'] for person in movie.get('directors', [])]) if movie.get('directors') else '',\n                'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''\n            }\n            return data\n        except (imdb.IMDbDataAccessError, urllib.error.HTTPError) as e:\n            time.sleep(1)  # Wait for 1 second before retrying\n    # If retries are exhausted, return empty data\n    return {'imdbId': movie_id, 'film_name': '', 'director': '', 'genre': ''}\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel and use tqdm for progress tracking\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results with progress tracking\n    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n        data_list.extend(future.result())\n\n# Specify the path to save the CSV\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'\n\n# Save the processed data to a CSV file with only imdbId, film_name, director, and genre\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'director', 'genre'])  # Header row\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['director'], data['genre']])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# updated needed features to current file","metadata":{}},{"cell_type":"code","source":"import imdb\nimport pandas as pd\nimport concurrent.futures\n\n# Extract the list of IMDb IDs\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        ia.update(movie, 'directors')\n        ia.update(movie, 'writers')\n\n        # Return the new data, using `.get()` to safely handle missing fields\n        data = {\n            'imdbId': movie_id,\n            'director': ', '.join([person.get('name', '') for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'writer': ', '.join([person.get('name', '') for person in movie.get('writers', [])]) if movie.get('writers') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''  # join genre list\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        return {'imdbId': movie_id, 'director': '', 'writer': '', 'genre': ''}\n    \n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Convert the collected data into a DataFrame\nnew_data_df = pd.DataFrame(data_list)\n\n# Merge the new data with the existing dataset\nupdated_data = pd.merge(data, new_data_df, on='imdbId', how='left')\n\n# Save the updated data back to the CSV file\nupdated_data.to_csv(csv_file, index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# download the csv file","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Create a Pandas DataFrame from the data_list\ndf = pd.DataFrame(data_list)\n\ndf.to_csv('imdb_data.csv', index=False)\nfrom IPython.display import FileLink\n\n# This will create a download link for the file\nFileLink('imdb_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# imdb reccomendation","metadata":{}},{"cell_type":"code","source":"import imdb\nimport pandas as pd\nimport concurrent.futures\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the function to get movie data based on movie title or IMDb ID\ndef get_movie_id(user_input):\n    # Try to interpret input as an IMDb ID (numeric)\n    try:\n        movie_id = int(user_input)\n        movie = ia.get_movie(movie_id)\n    except ValueError:\n        # If not a number, assume it's a movie title and search by title\n        search_results = ia.search_movie(user_input)\n        if search_results:\n            # Take the first search result as the most likely match\n            movie = search_results[0]\n        else:\n            return None  # Return None if no results found\n    \n    return movie.movieID  # Return the movie ID\n\n# Define the function to get movie recommendations\ndef get_recommendations(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'recommendations')  # Fetch recommendations\n\n        # Get the recommended movies (up to 3 recommendations)\n        recommended_movies = movie.get('recommendations', [])[:3]\n        recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies]) if recommended_movies else 'No recommendations'\n\n        # Return the top 3 recommendations\n        return {\n            'imdbId': movie_id,\n            'recommended_movies': recommended_titles  # Add the top 3 recommendations\n        }\n    except imdb.IMDbDataAccessError:\n        return {'imdbId': movie_id, 'recommended_movies': 'No recommendations'}\n\n# User input function to accept either movie title or IMDb ID\ndef get_user_recommendations():\n    user_input = input(\"Enter a movie name or IMDb ID: \")\n\n    # Normalize user input (case insensitive, remove extra spaces)\n    user_input_normalized = user_input.strip().lower()\n\n    # Get the IMDb movie ID\n    movie_id = get_movie_id(user_input_normalized)\n    if movie_id:\n        # Fetch recommendations if the movie ID is valid\n        recommendations = get_recommendations(movie_id)\n        print(f\"Top 3 recommended movies for {user_input}:\")\n        print(recommendations['recommended_movies'])\n    else:\n        print(\"Sorry, no movie found with that name or IMDb ID.\")\n\n# Run the recommendation system for user input\nget_user_recommendations()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_user_recommendations()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to get movie recommendations\ndef get_recommendations(movie_input):\n    try:\n        # Check if input is numeric (IMDb ID) or a movie name\n        if movie_input.isdigit():  # IMDb ID\n            movie = ia.get_movie(movie_input)\n        else:  # Movie name\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n\n        # Get movie title\n        movie_title = movie.get('title', movie_input)\n\n        # Try to update with recommendations\n        ia.update(movie, 'recommendations')\n        recommended_movies = movie.get('recommendations', [])\n\n        # If recommendations are missing, provide more feedback\n        if not recommended_movies:\n            return f\"No recommendations available for '{movie_title}'. The IMDb API may not have this data.\"\n\n        # Format the top 3 recommendations\n        recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies[:3]])\n        return f\"Top 3 recommended movies for {movie_title}:\\n{recommended_titles}\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display recommendations\n    result = get_recommendations(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to get movie recommendations or similar movies by keywords/genres\ndef get_similar_movies(movie_input):\n    try:\n        # Check if input is numeric (IMDb ID) or a movie name\n        if movie_input.isdigit():  # IMDb ID\n            movie = ia.get_movie(movie_input)\n        else:  # Movie name\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n\n        # Get movie title\n        movie_title = movie.get('title', movie_input)\n        print(f\"Movie found: {movie_title}\")\n\n        # First, try to get recommendations\n        ia.update(movie, 'recommendations')\n        recommended_movies = movie.get('recommendations', [])\n        print(f\"Recommendations: {recommended_movies}\")\n\n        # If recommendations are available, use them\n        if recommended_movies:\n            recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies[:3]])\n            return f\"Top 3 recommended movies for '{movie_title}':\\n{recommended_titles}\"\n\n        # Access genres and keywords directly from the movie object\n        genres = movie.get('genres', [])\n        keywords = movie.get('keywords', [])\n        print(f\"Genres: {genres}\")\n        print(f\"Keywords: {keywords}\")\n\n        # Search for similar movies based on genre or keywords\n        if genres:\n            similar_titles = ', '.join([f\"Similar movie {i+1}\" for i in range(3)])\n            return f\"No recommendations available for '{movie_title}'. Similar movies by genre:\\n{similar_titles}\"\n\n        if keywords:\n            similar_titles = ', '.join([f\"Similar movie {i+1}\" for i in range(3)])\n            return f\"No recommendations available for '{movie_title}'. Similar movies by keyword:\\n{similar_titles}\"\n\n        # If no similar movies found, return a fallback message\n        return f\"No recommendations or similar movies available for '{movie_title}'.\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display similar movies or recommendations\n    result = get_similar_movies(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to get movie recommendations or similar movies by keywords/genres\ndef get_similar_movies(movie_input):\n    try:\n        # Check if input is numeric (IMDb ID) or a movie name\n        if movie_input.isdigit():  # IMDb ID\n            movie = ia.get_movie(movie_input)\n        else:  # Movie name\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n\n        # Get movie title\n        movie_title = movie.get('title', movie_input)\n        print(f\"Movie found: {movie_title}\")\n\n        # First, try to get recommendations\n        ia.update(movie, 'recommendations')\n        recommended_movies = movie.get('recommendations', [])\n        print(f\"Recommendations: {recommended_movies}\")\n\n        # If recommendations are available, use them\n        if recommended_movies:\n            recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies[:3]])\n            return f\"Top 3 recommended movies for '{movie_title}':\\n{recommended_titles}\"\n\n        # Update movie with more information\n        ia.update(movie)\n        genres = movie.get('genres', [])\n        keywords = movie.get('keywords', [])\n        print(f\"Genres: {genres}\")\n        print(f\"Keywords: {keywords}\")\n\n        # Search for similar movies based on genre or keywords\n        if genres:\n            similar_titles = ', '.join([f\"Similar movie {i+1}\" for i in range(3)])\n            return f\"No recommendations available for '{movie_title}'. Similar movies by genre:\\n{similar_titles}\"\n\n        if keywords:\n            similar_titles = ', '.join([f\"Similar movie {i+1}\" for i in range(3)])\n            return f\"No recommendations available for '{movie_title}'. Similar movies by keyword:\\n{similar_titles}\"\n\n        # If no similar movies found, return a fallback message\n        return f\"No recommendations or similar movies available for '{movie_title}'.\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display similar movies or recommendations\n    result = get_similar_movies(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to get movie recommendations or similar movies by keywords/genres\ndef get_similar_movies(movie_input):\n    try:\n        # Check if input is numeric (IMDb ID) or a movie name\n        if movie_input.isdigit():  # IMDb ID\n            movie = ia.get_movie(movie_input)\n        else:  # Movie name\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n\n        # Get movie title\n        movie_title = movie.get('title', movie_input)\n        print(f\"Movie found: {movie_title}\")\n\n        # First, try to get recommendations\n        ia.update(movie, 'recommendations')\n        recommended_movies = movie.get('recommendations', [])\n        print(f\"Recommendations: {recommended_movies}\")\n\n        # If recommendations are available, use them\n        if recommended_movies:\n            recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies[:3]])\n            return f\"Top 3 recommended movies for '{movie_title}':\\n{recommended_titles}\"\n\n        # Update movie with more information\n        ia.update(movie)\n        genres = movie.get('genres', [])\n        keywords = movie.get('keywords', [])\n        print(f\"Genres: {genres}\")\n        print(f\"Keywords: {keywords}\")\n\n        # Search for similar movies based on genre\n        if genres:\n            similar_movies = []\n            for genre in genres:\n                genre_movies = ia.get_top50_movies_by_genres(genre)\n                similar_movies.extend(genre_movies)\n            similar_movies = list({movie.movieID: movie for movie in similar_movies}.values())  # Remove duplicates\n            similar_titles = ', '.join([sim_movie.get('title', '') for sim_movie in similar_movies[:3]])\n            return f\"No recommendations available for '{movie_title}'. Similar movies by genre:\\n{similar_titles}\"\n\n        # If no similar movies found, return a fallback message\n        return f\"No recommendations or similar movies available for '{movie_title}'.\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display similar movies or recommendations\n    result = get_similar_movies(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to get movie recommendations or similar movies by keywords/genres\ndef get_similar_movies(movie_input):\n    try:\n        # Check if input is numeric (IMDb ID) or a movie name\n        if movie_input.isdigit():  # IMDb ID\n            movie = ia.get_movie(movie_input)\n        else:  # Movie name\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n\n        # Get movie title\n        movie_title = movie.get('title', movie_input)\n        print(f\"Movie found: {movie_title}\")\n\n        # First, try to get recommendations\n        ia.update(movie, 'recommendations')\n        recommended_movies = movie.get('recommendations', [])\n        print(f\"Recommendations: {recommended_movies}\")\n\n        # If recommendations are available, use them\n        if recommended_movies:\n            recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies[:3]])\n            return f\"Top 3 recommended movies for '{movie_title}':\\n{recommended_titles}\"\n\n        # Update movie with more information\n        ia.update(movie)\n        genres = movie.get('genres', [])\n        print(f\"Genres: {genres}\")\n\n        # Search for similar movies based on genre\n        if genres:\n            similar_movies = []\n            for genre in genres:\n                search_results = ia.search_movie_advanced(genre=genre)\n                similar_movies.extend(search_results[:3])  # Get top 3 movies for each genre\n            similar_movies = list({movie.movieID: movie for movie in similar_movies}.values())  # Remove duplicates\n            similar_titles = ', '.join([sim_movie.get('title', '') for sim_movie in similar_movies[:3]])\n            return f\"No recommendations available for '{movie_title}'. Similar movies by genre:\\n{similar_titles}\"\n\n        # If no similar movies found, return a fallback message\n        return f\"No recommendations or similar movies available for '{movie_title}'.\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display similar movies or recommendations\n    result = get_similar_movies(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to get movie recommendations or similar movies by keywords/genres\ndef get_similar_movies(movie_input):\n    try:\n        # Check if input is numeric (IMDb ID) or a movie name\n        if movie_input.isdigit():  # IMDb ID\n            movie = ia.get_movie(movie_input)\n        else:  # Movie name\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n\n        # Get movie title\n        movie_title = movie.get('title', movie_input)\n        print(f\"Movie found: {movie_title}\")\n\n        # First, try to get recommendations\n        ia.update(movie, 'recommendations')\n        recommended_movies = movie.get('recommendations', [])\n        print(f\"Recommendations: {recommended_movies}\")\n\n        # If recommendations are available, use them\n        if recommended_movies:\n            recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies[:3]])\n            return f\"Top 3 recommended movies for '{movie_title}':\\n{recommended_titles}\"\n\n        # Update movie with more information\n        ia.update(movie)\n        genres = movie.get('genres', [])\n        print(f\"Genres: {genres}\")\n\n        # Search for similar movies based on genre\n        if genres:\n            similar_movies = []\n            for genre in genres:\n                search_results = ia.search_movie(genre)\n                similar_movies.extend(search_results[:3])  # Get top 3 movies for each genre\n            similar_movies = list({movie.movieID: movie for movie in similar_movies}.values())  # Remove duplicates\n            similar_titles = ', '.join([sim_movie.get('title', '') for sim_movie in similar_movies[:3]])\n            return f\"No recommendations available for '{movie_title}'. Similar movies by genre:\\n{similar_titles}\"\n\n        # If no similar movies found, return a fallback message\n        return f\"No recommendations or similar movies available for '{movie_title}'.\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display similar movies or recommendations\n    result = get_similar_movies(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to get movie recommendations or similar movies by keywords/genres\ndef get_similar_movies(movie_input):\n    try:\n        # Check if input is numeric (IMDb ID) or a movie name\n        if movie_input.isdigit():  # IMDb ID\n            movie = ia.get_movie(movie_input)\n        else:  # Movie name\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n\n        # Get movie title\n        movie_title = movie.get('title', movie_input)\n        print(f\"Movie found: {movie_title}\")\n\n        # First, try to get recommendations\n        ia.update(movie, 'recommendations')\n        recommended_movies = movie.get('recommendations', [])\n        print(f\"Recommendations: {recommended_movies}\")\n\n        # If recommendations are available, use them\n        if recommended_movies:\n            recommended_titles = ', '.join([rec.get('title', '') for rec in recommended_movies[:3]])\n            return f\"Top 3 recommended movies for '{movie_title}':\\n{recommended_titles}\"\n\n        # Update movie with more information\n        ia.update(movie)\n        genres = movie.get('genres', [])\n        print(f\"Genres: {genres}\")\n\n        # Search for similar movies based on all genres\n        similar_movies = []\n        for genre in genres:\n            search_results = ia.search_movie(f\"{genre} movie\")\n            similar_movies.extend(search_results[:3])  # Get top 3 movies for each genre\n        similar_movies = list({movie.movieID: movie for movie in similar_movies}.values())  # Remove duplicates\n        similar_titles = ', '.join([sim_movie.get('title', '') for sim_movie in similar_movies[:3]])\n\n        return f\"No recommendations available for '{movie_title}'. Similar movies by genre:\\n{similar_titles}\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display similar movies or recommendations\n    result = get_similar_movies(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Function to fetch basic movie details\ndef get_movie_info(movie_input):\n    try:\n        # Check if input is IMDb ID (numeric) or movie title (string)\n        if movie_input.isdigit():\n            movie = ia.get_movie(movie_input)\n        else:\n            search_results = ia.search_movie(movie_input)\n            if not search_results:\n                return f\"No movie found for '{movie_input}'\"\n            movie = search_results[0]  # Take the first search result\n        \n        # Print basic information for debugging\n        movie_title = movie.get('title', 'Unknown Title')\n        movie_year = movie.get('year', 'Unknown Year')\n        genres = movie.get('genres', [])\n        print(f\"Movie found: {movie_title} ({movie_year})\")\n        print(f\"Genres: {genres}\")\n\n        return f\"Movie: {movie_title} ({movie_year}) | Genres: {', '.join(genres)}\"\n\n    except imdb.IMDbDataAccessError:\n        return f\"Error accessing IMDb data for '{movie_input}'\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# Main interaction loop\ndef main():\n    movie_input = input(\"Enter a movie name or IMDb ID: \").strip()\n\n    # Get and display movie info\n    result = get_movie_info(movie_input)\n    print(result)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\nimport logging\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\ndef get_movie(movie_title):\n    \"\"\"\n    Fetch movie data from IMDb.\n\n    Args:\n        movie_title (str): Movie title.\n\n    Returns:\n        dict: Movie data.\n    \"\"\"\n    try:\n        search_results = ia.search_movie(movie_title)\n        if search_results:\n            movie_id = search_results[0].movieID\n            movie = ia.get_movie(movie_id)\n            return movie\n        else:\n            return None\n    except imdb.IMDbDataAccessError as e:\n        logging.error(f\"Error accessing IMDb data: {e}\")\n        return None\n    except Exception as e:\n        logging.error(f\"An error occurred: {e}\")\n        return None\n\ndef get_similar_movies(movie):\n    \"\"\"\n    Find similar movies based on genre and director.\n\n    Args:\n        movie (dict): Movie data.\n\n    Returns:\n        list: Similar movies.\n    \"\"\"\n    genres = movie.data.get('genres', [])\n    directors = movie.data.get('directors', [])\n\n    # Create a list of all unique genres\n    all_genres = set()\n    for genre in genres:\n        all_genres.add(genre)\n\n    # Get top 250 movies\n    top_movies = ia.get_top250_movies()\n\n    # Create a list of all unique genres for top movies\n    top_movies_data = []\n    for top_movie in top_movies:\n        top_movie_id = top_movie.movieID\n        top_movie_data = ia.get_movie(top_movie_id)\n        top_movies_data.append(top_movie_data)\n        top_movie_genres = top_movie_data.data.get('genres', [])\n\n        for genre in top_movie_genres:\n            all_genres.add(genre)\n\n    # Create a one-hot encoder for genres\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    all_genres = list(all_genres)\n    encoder.fit([[genre] for genre in all_genres])\n\n    # Create a matrix of movie attributes\n    movie_attributes = encoder.transform([[genre] for genre in genres]).toarray()\n\n    # Create a matrix of top movie attributes\n    top_movie_attributes = []\n    for top_movie_data in top_movies_data:\n        top_movie_genres = top_movie_data.data.get('genres', [])\n\n        if top_movie_genres:  # Check if top_movie_genres is not empty\n            top_movie_attributes_row = encoder.transform([[genre] for genre in top_movie_genres]).toarray()\n            top_movie_attributes.append(top_movie_attributes_row)\n        else:\n            # If top movie has no genres, use a zero vector as its attributes\n            top_movie_attributes_row = np.zeros((1, len(all_genres)))\n            top_movie_attributes.append(top_movie_attributes_row)\n\n    # Check if top_movie_attributes is not empty\n    if top_movie_attributes:\n        # Stack the top movie attributes into a single 2D array\n        top_movie_attributes = np.vstack(top_movie_attributes)\n\n        # Calculate the cosine similarity between the movie attributes and top movie attributes\n        similarity_matrix = cosine_similarity(movie_attributes, top_movie_attributes)\n\n        # Get the top N similar movies\n        N = 10\n        similar_movies = []\n        if similarity_matrix.size:  # Check if similarity_matrix is not empty\n            indices = np.argsort(similarity_matrix)[0]\n            for i in indices[-N:]:  # Get the last N indices ( highest similarity)\n                similar_movies.append(top_movies_data[i])\n\n        return similar_movies\n    else:\n        return []\n\ndef print_movie_info(movie):\n    \"\"\"\n    Print basic movie information.\n\n    Args:\n        movie (dict): Movie data.\n    \"\"\"\n    movie_title = movie.data.get('title', 'Unknown Title')\n    movie_year = movie.data.get('year', 'Unknown Year')\n    genres = movie.data.get('genres', [])\n    print(f\"Movie found: {movie_title} ({movie_year})\")\n    print(f\"Genres: {genres}\")\n\ndef print_similar_movies(similar_movies):\n    \"\"\"\n    Print similar movies.\n\n    Args:\n        similar_movies (list): List of similar movies.\n    \"\"\"\n    if not similar_movies:\n        print(\"No similar movies found.\")\n        return\n\n    print(\"Similar movies:\")\n    for movie in similar_movies:\n        movie_title = movie.data.get('title', 'Unknown Title')\n        movie_year = movie.data.get('year', 'Unknown Year')\n        movie_genre = movie.data.get('genres', 'Unknown Genre')\n        print(f\"{movie_title} ({movie_year}) - {movie_genre}\")\n\ndef main():\n    movie_title = input(\"Enter a movie name: \").strip()\n\n    # Get movie data\n    movie = get_movie(movie_title)\n\n    if movie:\n        # Print movie info\n        print_movie_info(movie)\n\n        # Get similar movies\n        similar_movies = get_similar_movies(movie)\n\n        # Print similar movies\n        print_similar_movies(similar_movies)\n    else:\n        print(\"No movie found.\")\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosication","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n\n# Load the dataset in chunks\nchunksize = 1000\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize)\n\n# Concatenate all chunks into a single DataFrame\nratings = pd.concat(chunks, ignore_index=True)\n\n# Filter only ratings of 4.0 and above for \"liked\" movies\nratings_filtered = ratings[ratings['rating'] >= 4.0]\n\n# Create a user-item matrix\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(user_movie_matrix, min_support=0.02, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Display the rules\nprint(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosiation model ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n\n# Load the dataset in chunks\nchunksize = 1000\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize)\n\n# Concatenate all chunks into a single DataFrame\nratings = pd.concat(chunks, ignore_index=True)\n\n# Filter only ratings of 4.0 and above for \"liked\" movies\nratings_filtered = ratings[ratings['rating'] >= 4.0]\n\n# Create a user-item matrix, where 1 represents a \"liked\" movie (rating 4.0 or above)\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(user_movie_matrix, min_support=0.02, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset to map movie IDs to movie titles\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    \"\"\"\n    movie_id = movies[movies['title'].str.contains(movie_title, case=False, na=False)]['movieId']\n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value (descending)\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get the top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n# Example usage:\nmovie_name = 'Interstellar'  # Change this to the movie title you want recommendations for\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # If the function returns an error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosiation model low resources","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom scipy.sparse import csr_matrix\n\n# Load the dataset in chunks to avoid memory overload\nchunksize = 10**6  # Increase the chunksize to process more data at once but keep it manageable\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize, usecols=['userId', 'movieId', 'rating'])\n\n# Initialize an empty list to store filtered data\nfiltered_chunks = []\n\n# Process each chunk individually\nfor chunk in chunks:\n    # Filter ratings of 4.0 and above for \"liked\" movies\n    filtered_chunk = chunk[chunk['rating'] >= 4.0]\n    filtered_chunks.append(filtered_chunk)\n\n# Concatenate all filtered chunks into a single DataFrame\nratings_filtered = pd.concat(filtered_chunks, ignore_index=True)\n\n# Create a user-item matrix in sparse format to save memory\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\nuser_movie_matrix_sparse = csr_matrix(user_movie_matrix.values)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(pd.DataFrame(user_movie_matrix_sparse.todense(), columns=user_movie_matrix.columns), min_support=0.02, use_colnames=True)\n\n# Generate association rules from the frequent item sets\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift to find the most interesting ones\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset to map movie IDs to movie titles\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    \"\"\"\n    movie_id = movies[movies['title'].str.contains(movie_title, case=False, na=False)]['movieId']\n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value (descending)\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get the top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n# Example usage:\nmovie_name = 'Interstellar'  # Change this to the movie title you want recommendations for\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # If the function returns an error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# final try ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom scipy.sparse import csr_matrix\nimport re\n\n# Function to clean movie titles by removing special characters and lowercasing\ndef clean_movie_title(title):\n    # Remove anything in parentheses (like year)\n    title = re.sub(r'\\(.*?\\)', '', title)\n    # Remove extra spaces and lowercase the title\n    title = re.sub(r'\\s+', ' ', title).strip().lower()\n    return title\n\n# Load the dataset in chunks\nchunksize = 10**6\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize, usecols=['userId', 'movieId', 'rating'])\n\n# Initialize an empty list to store filtered data\nfiltered_chunks = []\n\n# Process each chunk individually\nfor chunk in chunks:\n    # Filter ratings of 4.0 and above for \"liked\" movies\n    filtered_chunk = chunk[chunk['rating'] >= 4.0]\n    filtered_chunks.append(filtered_chunk)\n\n# Concatenate filtered chunks\nratings_filtered = pd.concat(filtered_chunks, ignore_index=True)\n\n# Create a user-item matrix in sparse format\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\nuser_movie_matrix_sparse = csr_matrix(user_movie_matrix.values)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(pd.DataFrame(user_movie_matrix_sparse.todense(), columns=user_movie_matrix.columns), min_support=0.05, use_colnames=True)\n\n# Generate association rules from frequent item sets\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Apply the clean_movie_title function to standardize movie titles\nmovies['clean_title'] = movies['title'].apply(clean_movie_title)\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    Clean the input movie title for consistency.\n    \"\"\"\n    cleaned_title = clean_movie_title(movie_title)\n    movie_id = movies[movies['clean_title'] == cleaned_title]['movieId']\n    \n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nmovie_name = 'Heat (1995)'  # You can try different variations like 'interstellar'\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # Error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# cosine matrix improved","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'D:\\abdo\\AI\\projects\\recommender\\needed features\\imdb_data.csv')\n\n# Convert data to a DataFrame\ndf = pd.DataFrame(data)\n\n# Replace NaN values in relevant columns with an empty string\ndf['overview'] = df['overview'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('')\ndf['genre'] = df['genre'].fillna('')\n\n# Combine 'title', 'overview', 'cast', 'director', 'genre', and 'reviews' for similarity comparison\ndf['combined_features'] = df['title'] + ' ' + df['overview'] + ' ' + df['cast'] + ' ' + df['director'] + ' ' + df['genre'] + ' ' + df['reviews']\n\n# Text Vectorization using TF-IDF (including bi-grams for better context)\ntfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\ntfidf_matrix = tfidf.fit_transform(df['combined_features'].fillna(''))\n\n# Calculate cosine similarity between all movies\ncosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n# Helper function to extract movie title\ndef extract_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower()\n    title = re.sub(r'\\s*\\(\\d{4}\\)$', '', title)  # Remove year information\n    return title\n\n# Define the function to recommend movies\ndef recommend_movies(movie_title, cosine_sim=cosine_sim, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage: Recommend movies based on the title \"Heat\"\ntest = 'heat'\nrecommended_movies = recommend_movies(test)\nprint(f\"Movies recommended based on '{test}':\")\nfor movie in recommended_movies:\n    print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, IMDb ID: {movie['imdbId']}\")\n    print(f\"Director: {movie['director']}, Genre: {movie['genre']}\")\n    print(f\"Cast: {movie['cast']}\")\n    print(f\"Overview: {movie['overview']}\")\n    print(f\"Reviews: {movie['reviews']}\")\n    print(f\"Cover URL: {movie['cover_url']}\")\n    print(\"-\" * 80)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"pip install tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lstm 256 patch size adam w ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Combine features into a single column for text processing\ndf['combined_features'] = df['overview'] + ' ' + df['cast'] + ' ' + df['director'] + ' ' + df['genre'] + ' ' + df['tag'] + ' ' + df['reviews']\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_sequence_len))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='AdamW', metrics=['accuracy'])\nmodel.summary()\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2, random_state=42)\n\n# Train the LSTM model\nmodel.fit(X_train, y_train, epochs=5, batch_size=256, validation_data=(X_test, y_test))\n\n# Generate embeddings from the trained LSTM model\nembedding_layer = Sequential()\nembedding_layer.add(model.layers[0])\nembedding_layer.add(model.layers[1])\nembedding_layer.compile(loss='binary_crossentropy', optimizer='AdamW')\n\n# Generate embeddings for all movies\nmovie_embeddings = embedding_layer.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d = np.mean(movie_embeddings, axis=1)\n\n# Calculate cosine similarity based on 2D embeddings\ncosine_sim_lstm = cosine_similarity(movie_embeddings_2d, movie_embeddings_2d)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    # Check if the title is NaN or not a string, return an empty string in that case\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings\ndef recommend_movies_lstm(movie_title, cosine_sim=cosine_sim_lstm, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n# Example usage with LSTM model\ntest_movie = 'heat'\nrecommended_movies_lstm = recommend_movies_lstm(test_movie)\n\nif not recommended_movies_lstm:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lstm 256 batch adam","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Combine features into a single column for text processing\ndf['combined_features'] = df['overview'] + ' ' + df['cast'] + ' ' + df['director'] + ' ' + df['genre'] + ' ' + df['tag'] + ' ' + df['reviews']\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_sequence_len))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2, random_state=42)\n\n# Train the LSTM model\nmodel.fit(X_train, y_train, epochs=5, batch_size=256, validation_data=(X_test, y_test))\n\n# Generate embeddings from the trained LSTM model\nembedding_layer = Sequential()\nembedding_layer.add(model.layers[0])\nembedding_layer.add(model.layers[1])\nembedding_layer.compile(loss='binary_crossentropy', optimizer='Adam')\n\n# Generate embeddings for all movies\nmovie_embeddings = embedding_layer.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d = np.mean(movie_embeddings, axis=1)\n\n# Calculate cosine similarity based on 2D embeddings\ncosine_sim_lstm = cosine_similarity(movie_embeddings_2d, movie_embeddings_2d)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    # Check if the title is NaN or not a string, return an empty string in that case\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings\ndef recommend_movies_lstm(movie_title, cosine_sim=cosine_sim_lstm, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n# Example usage with LSTM model\ntest_movie = 'heat'\nrecommended_movies_lstm = recommend_movies_lstm(test_movie)\n\nif not recommended_movies_lstm:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# another try ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Feature Weighting\ndf['combined_features'] = (df['overview'] * 1 + ' ' +\n                           df['cast'] * 3 + ' ' + \n                           df['director'] * 4 + ' ' + \n                           df['genre'] * 5 + ' ' + \n                           df['tag'] * 2 + ' ' + \n                           df['reviews'] * 1)\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_sequence_len))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2, random_state=42)\n\n# Train the LSTM model\nmodel.fit(X_train, y_train, epochs=5, batch_size=256, validation_data=(X_test, y_test))\n\n# Generate embeddings from the trained LSTM model\nembedding_layer = Sequential()\nembedding_layer.add(model.layers[0])\nembedding_layer.add(model.layers[1])\nembedding_layer.compile(loss='binary_crossentropy', optimizer='Adam')\n\n# Generate embeddings for all movies\nmovie_embeddings = embedding_layer.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d = np.mean(movie_embeddings, axis=1)\n\n# Calculate cosine similarity based on 2D embeddings\ncosine_sim_lstm = cosine_similarity(movie_embeddings_2d, movie_embeddings_2d)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings\ndef recommend_movies_lstm(movie_title, cosine_sim=cosine_sim_lstm, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage with LSTM model\ntest_movie = 'heat'\nrecommended_movies_lstm = recommend_movies_lstm(test_movie)\n\nif not recommended_movies_lstm:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lstm and attention  no weights","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Attention, Flatten\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Feature Weighting\ndf['combined_features'] = (df['overview'] * 6 + ' ' +\n                           df['cast'] * 1 + ' ' + \n                           df['director'] * 4 + ' ' + \n                           df['genre'] * 5 + ' ' + \n                           df['tag'] * 1 + ' ' + \n                           df['reviews'] * 1)\n\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model with Attention mechanism\nembedding_dim = 100\n\ninput_layer = Input(shape=(max_sequence_len,))\nembedding_layer = Embedding(max_words, embedding_dim, input_length=max_sequence_len)(input_layer)\nlstm_layer_1 = LSTM(128, return_sequences=True)(embedding_layer)\ndropout_layer_1 = Dropout(0.2)(lstm_layer_1)\nlstm_layer_2 = LSTM(64, return_sequences=True)(dropout_layer_1)\n\n# Attention mechanism\nattention_layer = Attention()([lstm_layer_2, lstm_layer_2])\nattention_output = Flatten()(attention_layer)  # Flatten the attention output\n\ndense_layer_1 = Dense(32, activation='relu')(attention_output)\ndense_layer_2 = Dense(16, activation='relu')(dense_layer_1)\noutput_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n\nmodel_with_attention = Model(inputs=input_layer, outputs=output_layer)\n\nmodel_with_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_with_attention.summary()\n\n# Train the LSTM model with Attention mechanism\nmodel_with_attention.fit(padded_sequences, np.ones(len(padded_sequences)), epochs=5, batch_size=256)\n\n# Generate embeddings from the trained LSTM model with Attention mechanism\nembedding_model_with_attention = Model(inputs=model_with_attention.input,\n                                       outputs=model_with_attention.layers[3].output)\nmovie_embeddings_with_attention = embedding_model_with_attention.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d_with_attention = np.mean(movie_embeddings_with_attention, axis=1)\n\n# Calculate cosine similarity based on 2D embeddings with Attention mechanism\ncosine_sim_lstm_with_attention = cosine_similarity(movie_embeddings_2d_with_attention, movie_embeddings_2d_with_attention)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings with Attention mechanism\ndef recommend_movies_lstm_with_attention(movie_title, cosine_sim=cosine_sim_lstm_with_attention, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage with LSTM model with Attention mechanism\ntest_movie = 'heat'\nrecommended_movies_lstm_with_attention = recommend_movies_lstm_with_attention(test_movie)\n\nif not recommended_movies_lstm_with_attention:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm_with_attention:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_movie = 'interstellar'\nrecommended_movies_lstm_with_attention = recommend_movies_lstm_with_attention(test_movie)\n\nif not recommended_movies_lstm_with_attention:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm_with_attention:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# eclidian distance","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Attention, Flatten\nfrom scipy.spatial.distance import euclidean\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Feature Weighting\ndf['combined_features'] = (df['overview'] * 7 + ' ' +\n                           df['cast'] * 1 + ' ' + \n                           df['director'] * 4 + ' ' + \n                           df['genre'] * 4 + ' ' + \n                           df['tag'] * 1 + ' ' + \n                           df['reviews'] * 1)\n\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model with Attention mechanism\nembedding_dim = 100\n\ninput_layer = Input(shape=(max_sequence_len,))\nembedding_layer = Embedding(max_words, embedding_dim, input_length=max_sequence_len)(input_layer)\nlstm_layer_1 = LSTM(128, return_sequences=True)(embedding_layer)\ndropout_layer_1 = Dropout(0.2)(lstm_layer_1)\nlstm_layer_2 = LSTM(64, return_sequences=True)(dropout_layer_1)\n\n# Attention mechanism\nattention_layer = Attention()([lstm_layer_2, lstm_layer_2])\nattention_output = Flatten()(attention_layer)  # Flatten the attention output\n\ndense_layer_1 = Dense(32, activation='relu')(attention_output)\ndense_layer_2 = Dense(16, activation='relu')(dense_layer_1)\noutput_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n\nmodel_with_attention = Model(inputs=input_layer, outputs=output_layer)\n\nmodel_with_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_with_attention.summary()\n\n# Train the LSTM model with Attention mechanism\nmodel_with_attention.fit(padded_sequences, np.ones(len(padded_sequences)), epochs=5, batch_size=256)\n\n# Generate embeddings from the trained LSTM model with Attention mechanism\nembedding_model_with_attention = Model(inputs=model_with_attention.input,\n                                       outputs=model_with_attention.layers[3].output)\nmovie_embeddings_with_attention = embedding_model_with_attention.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d_with_attention = np.mean(movie_embeddings_with_attention, axis=1)\n\n# Calculate Euclidean distance based on 2D embeddings with Attention mechanism\ndef calculate_euclidean_similarity(embeddings):\n    num_movies = embeddings.shape[0]\n    euclidean_sim = np.zeros((num_movies, num_movies))\n    for i in range(num_movies):\n        for j in range(num_movies):\n            euclidean_sim[i, j] = euclidean(embeddings[i], embeddings[j])\n    return euclidean_sim\n\neuclidean_sim_lstm_with_attention = calculate_euclidean_similarity(movie_embeddings_2d_with_attention)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings with Attention mechanism using Euclidean distance\ndef recommend_movies_lstm_with_euclidean(movie_title, euclidean_sim=euclidean_sim_lstm_with_attention, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(euclidean_sim[idx]))\n        \n        # Sort the movies based on similarity scores (ascending order for Euclidean distance)\n        sim_scores = sorted(sim_scores, key=lambda x: x[1])\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage with LSTM model with Attention mechanism using Euclidean distance\ntest_movie_heat = 'heat'\nrecommended_movies_lstm_with_euclidean_heat = recommend_movies_lstm_with_euclidean(test_movie_heat)\n\nif not recommended_movies_lstm_with_euclidean_heat:\n    print(f\"'{test_movie_heat}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie_heat}':\")\n    for movie in recommended_movies_lstm_with_euclidean_heat:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n\ntest_movie_interstellar = 'interstellar'\nrecommended_movies_lstm_with_euclidean_interstellar = recommend_movies_lstm_with_euclidean(test_movie_interstellar)\n\nif not recommended_movies_lstm_with_euclidean_interstellar:\n    print(f\"'{test_movie_interstellar}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie_interstellar}':\")\n    for movie in recommended_movies_lstm_with_euclidean_interstellar:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Attention, Flatten\nfrom sklearn.decomposition import PCA\nfrom scipy.spatial.distance import euclidean\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# data cleaning\ndf['overview'] = df['overview'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['cast'] = df['cast'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['director'] = df['director'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['genre'] = df['genre'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['tag'] = df['tag'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['reviews'] = df['reviews'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\n\n# Feature Weighting\ndf['combined_features'] = (df['overview'] * 7 + ' ' +\n                           df['cast'] * 1 + ' ' + \n                           df['director'] * 8 + ' ' + \n                           df['genre'] * 10 + ' ' + \n                           df['tag'] * 1 + ' ' + \n                           df['reviews'] * 1)\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 500\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model with Attention mechanism\nembedding_dim = 100\n\ninput_layer = Input(shape=(max_sequence_len,))\nembedding_layer = Embedding(max_words, embedding_dim, input_length=max_sequence_len)(input_layer)\nlstm_layer_1 = LSTM(128, return_sequences=True)(embedding_layer)\ndropout_layer_1 = Dropout(0.2)(lstm_layer_1)\nlstm_layer_2 = LSTM(64, return_sequences=True)(dropout_layer_1)\n\n# Attention mechanism\nattention_layer = Attention()([lstm_layer_2, lstm_layer_2])\nattention_output = Flatten()(attention_layer)  # Flatten the attention output\n\ndense_layer_1 = Dense(32, activation='relu')(attention_output)\ndense_layer_2 = Dense(16, activation='relu')(dense_layer_1)\noutput_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n\nmodel_with_attention = Model(inputs=input_layer, outputs=output_layer)\n\nmodel_with_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_with_attention.summary()\n\n# Train the LSTM model with Attention mechanism\nmodel_with_attention.fit(padded_sequences, np.ones(len(padded_sequences)), epochs=5, batch_size=256)\n\n# Generate embeddings from the trained LSTM model with Attention mechanism\nembedding_model_with_attention = Model(inputs=model_with_attention.input,\n                                       outputs=model_with_attention.layers[3].output)\nmovie_embeddings_with_attention = embedding_model_with_attention.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d_with_attention = np.mean(movie_embeddings_with_attention, axis=1)\n\n# Reduce dimensionality of embeddings using PCA\npca = PCA(n_components=50)  # Adjust the number of components as needed\nreduced_embeddings = pca.fit_transform(movie_embeddings_2d_with_attention)\n\n# Calculate Euclidean distance based on reduced embeddings\ndef calculate_euclidean_similarity(embeddings):\n    num_movies = embeddings.shape[0]\n    euclidean_sim = np.zeros((num_movies, num_movies))\n    for i in range(num_movies):\n        for j in range(num_movies):\n            euclidean_sim[i, j] = euclidean(embeddings[i], embeddings[j])\n    return euclidean_sim\n\neuclidean_sim_lstm_with_attention_reduced = calculate_euclidean_similarity(reduced_embeddings)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on reduced embeddings with Euclidean distance\ndef recommend_movies_lstm_with_euclidean_reduced(movie_title, euclidean_sim=euclidean_sim_lstm_with_attention_reduced, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(euclidean_sim[idx]))\n        \n        # Sort the movies based on similarity scores (ascending order for Euclidean distance)\n        sim_scores = sorted(sim_scores, key=lambda x: x[1])\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage with reduced embeddings and Euclidean distance\ntest_movie_heat = 'heat'\nrecommended_movies_lstm_with_euclidean_reduced_heat = recommend_movies_lstm_with_euclidean_reduced(test_movie_heat)\n\nif not recommended_movies_lstm_with_euclidean_reduced_heat:\n    print(f\"'{test_movie_heat}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie_heat}':\")\n    for movie in recommended_movies_lstm_with_euclidean_reduced_heat:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n\ntest_movie_interstellar = 'interstellar'\nrecommended_movies_lstm_with_euclidean_reduced_interstellar = recommend_movies_lstm_with_euclidean_reduced(test_movie_interstellar)\n\nif not recommended_movies_lstm_with_euclidean_reduced_interstellar:\n    print(f\"'{test_movie_interstellar}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie_interstellar}':\")\n    for movie in recommended_movies_lstm_with_euclidean_reduced_interstellar:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Attention, Flatten\nfrom sklearn.decomposition import PCA\nfrom scipy.spatial.distance import euclidean\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Clean text data\ndf['overview'] = df['overview'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['cast'] = df['cast'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['director'] = df['director'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['genre'] = df['genre'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['tag'] = df['tag'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['reviews'] = df['reviews'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\n\n# Feature Weighting\ndf['combined_features'] = (df['overview'] * 7 + ' ' +\n                           df['cast'] * 2 + ' ' + \n                           df['director'] * 5 + ' ' + \n                           df['genre'] * 6 + ' ' + \n                           df['tag'] * 1 + ' ' + \n                           df['reviews'] * 1)\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model with Attention mechanism\nembedding_dim = 100\n\ninput_layer = Input(shape=(max_sequence_len,))\nembedding_layer = Embedding(max_words, embedding_dim, input_length=max_sequence_len)(input_layer)\nlstm_layer_1 = LSTM(256, return_sequences=True)(embedding_layer)\ndropout_layer_1 = Dropout(0.3)(lstm_layer_1)\nlstm_layer_2 = LSTM(128, return_sequences=True)(dropout_layer_1)\ndropout_layer_2 = Dropout(0.3)(lstm_layer_2)\nlstm_layer_3 = LSTM(64, return_sequences=True)(dropout_layer_2)\n\n# Attention mechanism\nattention_layer = Attention()([lstm_layer_3, lstm_layer_3])\nattention_output = Flatten()(attention_layer)  # Flatten the attention output\n\ndense_layer_1 = Dense(64, activation='relu')(attention_output)\ndense_layer_2 = Dense(32, activation='relu')(dense_layer_1)\noutput_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n\nmodel_with_attention = Model(inputs=input_layer, outputs=output_layer)\n\nmodel_with_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_with_attention.summary()\n\n# Train the LSTM model with Attention mechanism\nmodel_with_attention.fit(padded_sequences, np.ones(len(padded_sequences)), epochs=5, batch_size=256)\n\n# Generate embeddings from the trained LSTM model with Attention mechanism\nembedding_model_with_attention = Model(inputs=model_with_attention.input,\n                                       outputs=model_with_attention.layers[3].output)\nmovie_embeddings_with_attention = embedding_model_with_attention.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d_with_attention = np.mean(movie_embeddings_with_attention, axis=1)\n\n# Reduce dimensionality of embeddings using PCA\npca = PCA(n_components=50)  # Adjust the number of components as needed\nreduced_embeddings = pca.fit_transform(movie_embeddings_2d_with_attention)\n\n# Calculate Euclidean distance based on reduced embeddings\ndef calculate_euclidean_similarity(embeddings):\n    num_movies = embeddings.shape[0]\n    euclidean_sim = np.zeros((num_movies, num_movies))\n    for i in range(num_movies):\n        for j in range(num_movies):\n            euclidean_sim[i, j] = euclidean(embeddings[i], embeddings[j])\n    return euclidean_sim\n\neuclidean_sim_lstm_with_attention_reduced = calculate_euclidean_similarity(reduced_embeddings)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on reduced embeddings with Euclidean distance\ndef recommend_movies_lstm_with_euclidean_reduced(movie_title, euclidean_sim=euclidean_sim_lstm_with_attention_reduced, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(euclidean_sim[idx]))\n        \n        # Sort the movies based on similarity scores (ascending order for Euclidean distance)\n        sim_scores = sorted(sim_scores, key=lambda x: x[1])\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage with reduced embeddings and Euclidean distance\ntest_movie_heat = 'heat'\nrecommended_movies_lstm_with_euclidean_reduced_heat = recommend_movies_lstm_with_euclidean_reduced(test_movie_heat)\n\nif not recommended_movies_lstm_with_euclidean_reduced_heat:\n    print(f\"'{test_movie_heat}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie_heat}':\")\n    for movie in recommended_movies_lstm_with_euclidean_reduced_heat:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Attention, Flatten\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.decomposition import PCA\nfrom scipy.spatial.distance import euclidean\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Clean text data\ndf['overview'] = df['overview'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['cast'] = df['cast'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['director'] = df['director'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['genre'] = df['genre'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['tag'] = df['tag'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\ndf['reviews'] = df['reviews'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\n\n# Feature Weighting\ndf['combined_features'] = (df['overview'] * 7 + ' ' +\n                           df['cast'] * 2 + ' ' + \n                           df['director'] * 5 + ' ' + \n                           df['genre'] * 6 + ' ' + \n                           df['tag'] * 1 + ' ' + \n                           df['reviews'] * 1)\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model with Attention mechanism\nembedding_dim = 100\n\ninput_layer = Input(shape=(max_sequence_len,))\nembedding_layer = Embedding(max_words, embedding_dim, input_length=max_sequence_len)(input_layer)\nlstm_layer_1 = LSTM(256, return_sequences=True, kernel_regularizer=l2(0.01))(embedding_layer)\ndropout_layer_1 = Dropout(0.5)(lstm_layer_1)\nlstm_layer_2 = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(dropout_layer_1)\ndropout_layer_2 = Dropout(0.5)(lstm_layer_2)\nlstm_layer_3 = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(dropout_layer_2)\n\n# Attention mechanism\nattention_layer = Attention()([lstm_layer_3, lstm_layer_3])\nattention_output = Flatten()(attention_layer)  # Flatten the attention output\n\ndense_layer_1 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(attention_output)\ndense_layer_2 = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(dense_layer_1)\noutput_layer = Dense(1, activation='sigmoid')(dense_layer_2)\n\nmodel_with_attention = Model(inputs=input_layer, outputs=output_layer)\n\nmodel_with_attention.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_with_attention.summary()\n\n# Early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# Train the LSTM model with Attention mechanism\nmodel_with_attention.fit(padded_sequences, np.ones(len(padded_sequences)), epochs=20, batch_size=256, validation_split=0.2, callbacks=[early_stopping])\n\n# Generate embeddings from the trained LSTM model with Attention mechanism\nembedding_model_with_attention = Model(inputs=model_with_attention.input,\n                                       outputs=model_with_attention.layers[3].output)\nmovie_embeddings_with_attention = embedding_model_with_attention.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d_with_attention = np.mean(movie_embeddings_with_attention, axis=1)\n\n# Reduce dimensionality of embeddings using PCA\npca = PCA(n_components=50)  # Adjust the number of components as needed\nreduced_embeddings = pca.fit_transform(movie_embeddings_2d_with_attention)\n\n# Calculate Euclidean distance based on reduced embeddings\ndef calculate_euclidean_similarity(embeddings):\n    num_movies = embeddings.shape[0]\n    euclidean_sim = np.zeros((num_movies, num_movies))\n    for i in range(num_movies):\n        for j in range(num_movies):\n            euclidean_sim[i, j] = euclidean(embeddings[i], embeddings[j])\n    return euclidean_sim\n\neuclidean_sim_lstm_with_attention_reduced = calculate_euclidean_similarity(reduced_embeddings)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on reduced embeddings with Euclidean distance\ndef recommend_movies_lstm_with_euclidean_reduced(movie_title, euclidean_sim=euclidean_sim_lstm_with_attention_reduced, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(euclidean_sim[idx]))\n        \n        # Sort the movies based on similarity scores (ascending order for Euclidean distance)\n        sim_scores = sorted(sim_scores, key=lambda x: x[1])\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage with reduced embeddings and Euclidean distance\ntest_movie_heat = 'heat'\nrecommended_movies_lstm_with_euclidean_reduced_heat = recommend_movies_lstm_with_euclidean_reduced(test_movie_heat)\n\nif not recommended_movies_lstm_with_euclidean_reduced_heat:\n    print(f\"'{test_movie_heat}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie_heat}':\")\n    for movie in recommended_movies_lstm_with_euclidean_reduced_heat:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n\ntest_movie_interstellar = 'interstellar'\nrecommended_movies_lstm_with_euclidean_reduced_interstellar = recommend_movies_lstm_with_euclidean_reduced(test_movie_interstellar)\n\nif not recommended_movies_lstm_with_euclidean_reduced_interstellar:\n    print(f\"'{test_movie_interstellar}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie_interstellar}':\")\n    for movie in recommended_movies_lstm_with_euclidean_reduced_interstellar:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tmdbv3api pandas scikit-learn\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:03:17.307560Z","iopub.execute_input":"2024-09-18T21:03:17.307898Z","iopub.status.idle":"2024-09-18T21:03:31.867523Z","shell.execute_reply.started":"2024-09-18T21:03:17.307858Z","shell.execute_reply":"2024-09-18T21:03:31.866417Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting tmdbv3api\n  Downloading tmdbv3api-1.9.0-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tmdbv3api) (2.32.3)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (2024.7.4)\nDownloading tmdbv3api-1.9.0-py3-none-any.whl (25 kB)\nInstalling collected packages: tmdbv3api\nSuccessfully installed tmdbv3api-1.9.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\n\n# Initialize TMDb with your API key\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Initialize the Movie class\nmovie = Movie()\n\ndef get_movie_data(movie_id):\n    return movie.details(movie_id)\n\ndef get_popular_movies():\n    return movie.popular()\n\ndef get_movie_list():\n    popular_movies = get_popular_movies()\n    movies_data = []\n    for m in popular_movies['results']:\n        movie_id = m['id']\n        details = get_movie_data(movie_id)\n        movies_data.append({\n            'id': movie_id,\n            'title': details.get('title'),\n            'genres': ', '.join([g['name'] for g in details.get('genres', [])]),\n            'rating': details.get('vote_average'),\n            'overview': details.get('overview'),\n        })\n    return pd.DataFrame(movies_data)\n\n# Fetch movie list\nmovies_df = get_movie_list()\nprint(movies_df.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\ndef recommend_movies(title, movies_df):\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['overview'])\n    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n    \n    indices = pd.Series(movies_df.index, index=movies_df['title']).drop_duplicates()\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]  # Top 10 recommendations\n    movie_indices = [i[0] for i in sim_scores]\n    return movies_df['title'].iloc[movie_indices]\n\n# Example usage\nrecommended_movies = recommend_movies('The Shawshank Redemption', movies_df)\nprint(recommended_movies)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Initialize TMDb with your API key\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Initialize the Movie class\nmovie = Movie()\n\ndef get_movie_data(movie_id):\n    \"\"\"Fetch detailed movie information from TMDb.\"\"\"\n    return movie.details(movie_id)\n\ndef get_popular_movies():\n    \"\"\"Fetch popular movies from TMDb.\"\"\"\n    return movie.popular()\n\ndef get_movie_list():\n    \"\"\"Fetch and return a DataFrame of popular movies with their details.\"\"\"\n    popular_movies = get_popular_movies()\n    movies_data = []\n    for m in popular_movies['results']:\n        movie_id = m['id']\n        details = get_movie_data(movie_id)\n        movies_data.append({\n            'id': movie_id,\n            'title': details.get('title'),\n            'genres': ', '.join([g['name'] for g in details.get('genres', [])]),\n            'rating': details.get('vote_average'),\n            'overview': details.get('overview'),\n        })\n    return pd.DataFrame(movies_data)\n\ndef recommend_movies(title, movies_df):\n    \"\"\"Recommend movies based on the given title using a content-based filtering approach.\"\"\"\n    if 'title' not in movies_df.columns:\n        raise ValueError(\"The 'title' column is missing from the DataFrame\")\n    \n    if title.lower() not in movies_df['title'].str.lower().values:\n        raise ValueError(f\"The title '{title}' is not in the DataFrame\")\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['overview'])\n    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n    \n    indices = pd.Series(movies_df.index, index=movies_df['title'].str.lower()).drop_duplicates()\n    \n    if title.lower() not in indices:\n        raise ValueError(f\"The title '{title}' is not found in the index\")\n    \n    idx = indices[title.lower()]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]  # Top 10 recommendations\n    movie_indices = [i[0] for i in sim_scores]\n    \n    return movies_df['title'].iloc[movie_indices]\n\n# Main Workflow\nif __name__ == \"__main__\":\n    # Fetch movie list\n    movies_df = get_movie_list()\n    \n    # Print titles to verify\n    print(\"Available movie titles:\")\n    print(movies_df['title'].head())\n\n    # Example recommendation\n    try:\n        recommended_movies = recommend_movies('The Shawshank Redemption', movies_df)\n        print(\"\\nRecommended Movies:\")\n        print(recommended_movies)\n    except ValueError as e:\n        print(e)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Initialize TMDb with your API key\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Initialize the Movie class\nmovie = Movie()\n\ndef get_movie_data(movie_id):\n    \"\"\"Fetch detailed movie information from TMDb.\"\"\"\n    return movie.details(movie_id)\n\ndef get_popular_movies():\n    \"\"\"Fetch popular movies from TMDb.\"\"\"\n    return movie.popular()\n\ndef search_movies(query):\n    \"\"\"Search for movies by title using TMDb.\"\"\"\n    search = tmdb.Search()\n    results = search.movie(query=query)\n    movies_data = []\n    for result in results['results']:\n        movie_id = result['id']\n        details = get_movie_data(movie_id)\n        movies_data.append({\n            'id': movie_id,\n            'title': details.get('title'),\n            'genres': ', '.join([g['name'] for g in details.get('genres', [])]),\n            'rating': details.get('vote_average'),\n            'overview': details.get('overview'),\n        })\n    return pd.DataFrame(movies_data)\n\ndef recommend_movies(title, movies_df):\n    \"\"\"Recommend movies based on the given title using a content-based filtering approach.\"\"\"\n    if 'title' not in movies_df.columns:\n        raise ValueError(\"The 'title' column is missing from the DataFrame\")\n    \n    if title.lower() not in movies_df['title'].str.lower().values:\n        raise ValueError(f\"The title '{title}' is not in the DataFrame\")\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['overview'])\n    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n    \n    indices = pd.Series(movies_df.index, index=movies_df['title'].str.lower()).drop_duplicates()\n    \n    if title.lower() not in indices:\n        raise ValueError(f\"The title '{title}' is not found in the index\")\n    \n    idx = indices[title.lower()]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]  # Top 10 recommendations\n    movie_indices = [i[0] for i in sim_scores]\n    \n    return movies_df['title'].iloc[movie_indices]\n\n# Main Workflow\nif __name__ == \"__main__\":\n    # Fetch movie list\n    movies_df = get_movie_list()\n    \n    # Print titles to verify\n    print(\"Available movie titles:\")\n    print(movies_df['title'].head())\n\n    # Search for a specific movie\n    searched_movies_df = search_movies('The Shawshank Redemption')\n    print(\"\\nSearched Movies:\")\n    print(searched_movies_df)\n\n    # Example recommendation\n    try:\n        recommended_movies = recommend_movies('The Shawshank Redemption', searched_movies_df)\n        print(\"\\nRecommended Movies:\")\n        print(recommended_movies)\n    except ValueError as e:\n        print(e)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nimport requests\n\n# Initialize TMDb with your API key\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Initialize the Movie class\nmovie = Movie()\n\ndef fetch_genre_names():\n    \"\"\"Fetch the genre names from TMDb and return a dictionary.\"\"\"\n    url = \"https://api.themoviedb.org/3/genre/movie/list\"\n    params = {\"api_key\": tmdb.api_key, \"language\": \"en-US\"}\n    response = requests.get(url, params=params)\n    data = response.json()\n    return {g['id']: g['name'] for g in data['genres']}\n\ndef get_movie_data(movie_id):\n    \"\"\"Fetch detailed movie information from TMDb.\"\"\"\n    return movie.details(movie_id)\n\ndef get_popular_movies():\n    \"\"\"Fetch popular movies from TMDb and convert to DataFrame.\"\"\"\n    popular_movies = movie.popular()\n    genre_names = fetch_genre_names()\n    \n    movies_data = []\n    for result in popular_movies['results']:\n        genres = [genre_names.get(g_id, 'Unknown') for g_id in result.get('genre_ids', [])]\n        movies_data.append({\n            'id': result['id'],\n            'title': result.get('title'),\n            'genres': ', '.join(genres),\n            'rating': result.get('vote_average'),\n            'overview': result.get('overview'),\n        })\n    return pd.DataFrame(movies_data)\n\ndef search_movies(query):\n    \"\"\"Search for movies by title using TMDb.\"\"\"\n    url = \"https://api.themoviedb.org/3/search/movie\"\n    params = {\"api_key\": tmdb.api_key, \"query\": query, \"language\": \"en-US\"}\n    response = requests.get(url, params=params)\n    search_results = response.json()\n    genre_names = fetch_genre_names()\n    \n    movies_data = []\n    for result in search_results['results']:\n        movie_id = result['id']\n        details = get_movie_data(movie_id)\n        genres = [genre_names.get(g_id, 'Unknown') for g_id in details.get('genres', [])]\n        movies_data.append({\n            'id': movie_id,\n            'title': details.get('title'),\n            'genres': ', '.join(genres),\n            'rating': details.get('vote_average'),\n            'overview': details.get('overview'),\n        })\n    return pd.DataFrame(movies_data)\n\ndef recommend_movies(title, movies_df):\n    \"\"\"Recommend movies based on the given title using a content-based filtering approach.\"\"\"\n    if 'title' not in movies_df.columns:\n        raise ValueError(\"The 'title' column is missing from the DataFrame\")\n    \n    if title.lower() not in movies_df['title'].str.lower().values:\n        raise ValueError(f\"The title '{title}' is not in the DataFrame\")\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['overview'])\n    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n    \n    indices = pd.Series(movies_df.index, index=movies_df['title'].str.lower()).drop_duplicates()\n    \n    if title.lower() not in indices:\n        raise ValueError(f\"The title '{title}' is not found in the index\")\n    \n    idx = indices[title.lower()]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]  # Top 10 recommendations\n    movie_indices = [i[0] for i in sim_scores]\n    \n    return movies_df['title'].iloc[movie_indices]\n\n# Main Workflow\nif __name__ == \"__main__\":\n    # Fetch movie list\n    movies_df = get_popular_movies()\n    \n    # Print titles to verify\n    print(\"Available movie titles:\")\n    print(movies_df['title'].head())\n\n    # Search for a specific movie\n    searched_movies_df = search_movies('The Shawshank Redemption')\n    print(\"\\nSearched Movies:\")\n    print(searched_movies_df)\n\n    # Example recommendation\n    try:\n        recommended_movies = recommend_movies('The Shawshank Redemption', searched_movies_df)\n        print(\"\\nRecommended Movies:\")\n        print(recommended_movies)\n    except ValueError as e:\n        print(e)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nimport requests\n\n# Initialize TMDb with your API key\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Initialize the Movie class\nmovie = Movie()\n\n# Fetch genre names once\ndef fetch_genre_names():\n    \"\"\"Fetch the genre names from TMDb and return a dictionary.\"\"\"\n    url = \"https://api.themoviedb.org/3/genre/movie/list\"\n    params = {\"api_key\": tmdb.api_key, \"language\": \"en-US\"}\n    response = requests.get(url, params=params)\n    data = response.json()\n    return {g['id']: g['name'] for g in data['genres']}\n\n# Fetch movie data\ndef get_movie_data(movie_id):\n    \"\"\"Fetch detailed movie information from TMDb.\"\"\"\n    url = f\"https://api.themoviedb.org/3/movie/{movie_id}\"\n    params = {\"api_key\": tmdb.api_key, \"language\": \"en-US\"}\n    response = requests.get(url, params=params)\n    return response.json()\n\ndef get_popular_movies():\n    \"\"\"Fetch popular movies from TMDb and convert to DataFrame.\"\"\"\n    popular_movies = movie.popular()\n    genre_names = fetch_genre_names()\n    \n    movies_data = []\n    for result in popular_movies['results']:\n        genres = [genre_names.get(g_id, 'Unknown') for g_id in result.get('genre_ids', [])]\n        movies_data.append({\n            'id': result['id'],\n            'title': result.get('title'),\n            'genres': ', '.join(genres),\n            'rating': result.get('vote_average'),\n            'overview': result.get('overview'),\n        })\n    return pd.DataFrame(movies_data)\n\ndef search_movies(query):\n    \"\"\"Search for movies by title using TMDb.\"\"\"\n    url = \"https://api.themoviedb.org/3/search/movie\"\n    params = {\"api_key\": tmdb.api_key, \"query\": query, \"language\": \"en-US\"}\n    response = requests.get(url, params=params)\n    search_results = response.json()\n    genre_names = fetch_genre_names()\n    \n    movies_data = []\n    for result in search_results['results']:\n        movie_id = result['id']\n        details = get_movie_data(movie_id)\n        # Ensure genres is a list of dictionaries\n        genres = [genre_names.get(g['id'], 'Unknown') for g in details.get('genres', [])]\n        movies_data.append({\n            'id': movie_id,\n            'title': details.get('title'),\n            'genres': ', '.join(genres),\n            'rating': details.get('vote_average'),\n            'overview': details.get('overview'),\n        })\n    return pd.DataFrame(movies_data)\n\ndef recommend_movies(title, movies_df):\n    \"\"\"Recommend movies based on the given title using a content-based filtering approach.\"\"\"\n    if 'title' not in movies_df.columns:\n        raise ValueError(\"The 'title' column is missing from the DataFrame\")\n    \n    if title.lower() not in movies_df['title'].str.lower().values:\n        raise ValueError(f\"The title '{title}' is not in the DataFrame\")\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['overview'])\n    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n    \n    indices = pd.Series(movies_df.index, index=movies_df['title'].str.lower()).drop_duplicates()\n    \n    if title.lower() not in indices:\n        raise ValueError(f\"The title '{title}' is not found in the index\")\n    \n    idx = indices[title.lower()]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]  # Top 10 recommendations\n    movie_indices = [i[0] for i in sim_scores]\n    \n    return movies_df.iloc[movie_indices]['title']\n\n# Main Workflow\nif __name__ == \"__main__\":\n    # Fetch movie list\n    movies_df = get_popular_movies()\n    \n    # Print titles to verify\n    print(\"Available movie titles:\")\n    print(movies_df['title'].head())\n\n    # Search for a specific movie\n    searched_movies_df = search_movies('The Shawshank Redemption')\n    print(\"\\nSearched Movies:\")\n    print(searched_movies_df)\n\n    # Example recommendation\n    try:\n        recommended_movies = recommend_movies('The Shawshank Redemption', searched_movies_df)\n        print(\"\\nRecommended Movies:\")\n        print(recommended_movies)\n    except ValueError as e:\n        print(e)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imdb import IMDb\n\n# Initialize IMDbPY instance\nia = IMDb()\n\n# Function to search for a movie\ndef get_movie_by_name(movie_name):\n    movies = ia.search_movie(movie_name)\n    if not movies:\n        return None\n    return movies[0]\n\n# Function to fetch movies by genre and year\ndef get_movies_by_genre_and_year(genres, year, keywords):\n    all_movies = []\n    for genre in genres:\n        try:\n            # Search for movies by genre keyword\n            movies = ia.search_movie(genre)\n            # Filter by year (within +/- 10 years)\n            filtered_movies = [m for m in movies if abs(m.get('year', 0) - year) <= 10]\n            all_movies.extend(filtered_movies)\n        except Exception as e:\n            print(f\"Error fetching movies for genre '{genre}': {e}\")\n    return all_movies\n\n# Function to get user preferences for weights\ndef get_user_preferences():\n    print(\"Assign weights to the following features (0-5):\")\n    genre_weight = int(input(\"Genres (0-5): \"))\n    director_weight = int(input(\"Directors (0-5): \"))\n    actor_weight = int(input(\"Actors (0-5): \"))\n    keyword_weight = int(input(\"Keywords (0-5): \"))\n    rating_weight = int(input(\"Ratings (0-5): \"))\n    \n    return {\n        'genre_weight': genre_weight,\n        'director_weight': director_weight,\n        'actor_weight': actor_weight,\n        'keyword_weight': keyword_weight,\n        'rating_weight': rating_weight\n    }\n\n# Function to get similar movies with user-specified weights\ndef recommend_similar_movies(movie, user_weights, num_recommendations=3):\n    ia.update(movie)\n    \n    # Extract features to compare\n    movie_genres = set(movie.get('genres', []))\n    movie_directors = set([director['name'] for director in movie.get('directors', [])])\n    movie_actors = set([actor['name'] for actor in movie.get('cast', [])[:5]])\n    movie_keywords = set(movie.get('keywords', []))\n    movie_rating = movie.get('rating', 0)\n    movie_year = movie.get('year', 0)\n    \n    # Fetch a larger dataset by genre, year, and keywords\n    similar_movies = get_movies_by_genre_and_year(movie_genres, movie_year, movie_keywords)\n    \n    recommendations = []\n    \n    for other_movie in similar_movies:\n        ia.update(other_movie)\n        \n        # Compare genres (user-defined weight)\n        other_genres = set(other_movie.get('genres', []))\n        genre_similarity = len(movie_genres & other_genres) * user_weights['genre_weight']\n        \n        # Compare directors (user-defined weight)\n        other_directors = set([director['name'] for director in other_movie.get('directors', [])])\n        director_similarity = len(movie_directors & other_directors) * user_weights['director_weight']\n        \n        # Compare actors (user-defined weight)\n        other_actors = set([actor['name'] for actor in other_movie.get('cast', [])[:5]])\n        actor_similarity = len(movie_actors & other_actors) * user_weights['actor_weight']\n        \n        # Compare keywords (user-defined weight)\n        other_keywords = set(other_movie.get('keywords', []))\n        keyword_similarity = len(movie_keywords & other_keywords) * user_weights['keyword_weight']\n        \n        # Compare IMDb ratings (user-defined weight)\n        other_rating = other_movie.get('rating', 0)\n        rating_difference = 1 - abs(movie_rating - other_rating) / 10  # Normalized difference\n        rating_similarity = rating_difference * user_weights['rating_weight']\n        \n        # Total score based on user-specified weights\n        score = genre_similarity + director_similarity + actor_similarity + keyword_similarity + rating_similarity\n        \n        if score > 0:\n            recommendations.append((other_movie, score))\n    \n    # Sort by score and return top recommendations\n    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n    return [movie[0] for movie in recommendations[:num_recommendations]]\n\n# Main function\ndef recommend_movies(movie_name):\n    # Get user preferences for weights\n    user_weights = get_user_preferences()\n    \n    # Get the user's movie\n    movie = get_movie_by_name(movie_name)\n    \n    if not movie:\n        return \"Movie not found. Please try a different name.\"\n    \n    print(f\"Movie found: {movie['title']} ({movie['year']})\")\n    \n    # Get 3 recommended movies based on user preferences\n    recommendations = recommend_similar_movies(movie, user_weights)\n    \n    # Output the recommendations\n    if recommendations:\n        print(\"\\nRecommended movies:\")\n        for rec in recommendations:\n            print(f\"- {rec['title']} ({rec['year']}) | Rating: {rec.get('rating', 'N/A')}\")\n    else:\n        print(\"No similar movies found.\")\n\n# Example usage\nmovie_name = input(\"Enter a movie name: \")\nrecommend_movies(movie_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imdb import IMDb\n\n# Initialize IMDbPY instance\nia = IMDb()\n\n# Function to search for a movie\ndef get_movie_by_name(movie_name):\n    movies = ia.search_movie(movie_name)\n    if not movies:\n        return None\n    return movies[0]\n\n# Function to fetch movies by genre and year\ndef get_movies_by_genre_and_year(genres, year, keywords, num_results=100):\n    all_movies = []\n    for genre in genres:\n        try:\n            # Search for movies by genre keyword\n            movies = ia.search_movie(genre)\n            # Filter results to movies only and by year range\n            filtered_movies = [m for m in movies if 'movie' in m['kind'] and abs(m.get('year', 0) - year) <= 10]\n            all_movies.extend(filtered_movies)\n        except Exception as e:\n            print(f\"Error fetching movies for genre '{genre}': {e}\")\n    # Remove duplicates and limit to num_results\n    all_movies = list({m['title']: m for m in all_movies}.values())\n    return all_movies[:num_results]\n\n# Function to get user preferences for weights\ndef get_user_preferences():\n    print(\"Assign weights to the following features (0-5):\")\n    genre_weight = int(input(\"Genres (0-5): \"))\n    director_weight = int(input(\"Directors (0-5): \"))\n    actor_weight = int(input(\"Actors (0-5): \"))\n    keyword_weight = int(input(\"Keywords (0-5): \"))\n    rating_weight = int(input(\"Ratings (0-5): \"))\n    \n    return {\n        'genre_weight': genre_weight,\n        'director_weight': director_weight,\n        'actor_weight': actor_weight,\n        'keyword_weight': keyword_weight,\n        'rating_weight': rating_weight\n    }\n\n# Function to get similar movies with user-specified weights\ndef recommend_similar_movies(movie, user_weights, num_recommendations=3):\n    ia.update(movie)\n    \n    # Extract features to compare\n    movie_genres = set(movie.get('genres', []))\n    movie_directors = set([director['name'] for director in movie.get('directors', [])])\n    movie_actors = set([actor['name'] for actor in movie.get('cast', [])[:5]])\n    movie_keywords = set(movie.get('keywords', []))\n    movie_rating = movie.get('rating', 0)\n    movie_year = movie.get('year', 0)\n    \n    # Fetch a larger dataset by genre, year, and keywords\n    similar_movies = get_movies_by_genre_and_year(movie_genres, movie_year, movie_keywords)\n    \n    recommendations = []\n    \n    for other_movie in similar_movies:\n        ia.update(other_movie)\n        \n        # Filter out TV shows\n        if 'movie' not in other_movie.get('kind', ''):\n            continue\n        \n        # Compare genres (user-defined weight)\n        other_genres = set(other_movie.get('genres', []))\n        genre_similarity = len(movie_genres & other_genres) * user_weights['genre_weight']\n        \n        # Compare directors (user-defined weight)\n        other_directors = set([director['name'] for director in other_movie.get('directors', [])])\n        director_similarity = len(movie_directors & other_directors) * user_weights['director_weight']\n        \n        # Compare actors (user-defined weight)\n        other_actors = set([actor['name'] for actor in other_movie.get('cast', [])[:5]])\n        actor_similarity = len(movie_actors & other_actors) * user_weights['actor_weight']\n        \n        # Compare keywords (user-defined weight)\n        other_keywords = set(other_movie.get('keywords', []))\n        keyword_similarity = len(movie_keywords & other_keywords) * user_weights['keyword_weight']\n        \n        # Compare IMDb ratings (user-defined weight)\n        other_rating = other_movie.get('rating', 0)\n        rating_difference = 1 - abs(movie_rating - other_rating) / 10  # Normalized difference\n        rating_similarity = rating_difference * user_weights['rating_weight']\n        \n        # Total score based on user-specified weights\n        score = genre_similarity + director_similarity + actor_similarity + keyword_similarity + rating_similarity\n        \n        if score > 0:\n            recommendations.append((other_movie, score))\n    \n    # Sort by score and return top recommendations\n    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n    return [movie[0] for movie in recommendations[:num_recommendations]]\n\n# Main function\ndef recommend_movies(movie_name):\n    # Get user preferences for weights\n    user_weights = get_user_preferences()\n    \n    # Get the user's movie\n    movie = get_movie_by_name(movie_name)\n    \n    if not movie:\n        return \"Movie not found. Please try a different name.\"\n    \n    print(f\"Movie found: {movie['title']} ({movie['year']})\")\n    \n    # Get 3 recommended movies based on user preferences\n    recommendations = recommend_similar_movies(movie, user_weights)\n    \n    # Output the recommendations\n    if recommendations:\n        print(\"\\nRecommended movies:\")\n        for rec in recommendations:\n            print(f\"- {rec['title']} ({rec['year']}) | Rating: {rec.get('rating', 'N/A')}\")\n    else:\n        print(\"No similar movies found.\")\n\n# Example usage\nmovie_name = input(\"Enter a movie name: \")\nrecommend_movies(movie_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imdb import IMDb\n\n# Initialize IMDbPY instance\nia = IMDb()\n\n# Function to search for a movie\ndef get_movie_by_name(movie_name):\n    movies = ia.search_movie(movie_name)\n    if not movies:\n        return None\n    return movies[0]\n\n# Function to fetch movies by genre and year with additional attributes\ndef get_movies_by_genre_and_year(genres, year, keywords, num_results=100):\n    all_movies = []\n    for genre in genres:\n        try:\n            # Search for movies by genre keyword\n            movies = ia.search_movie(genre)\n            # Filter results to movies only and by year range\n            filtered_movies = [m for m in movies if 'movie' in m['kind'] and abs(m.get('year', 0) - year) <= 10]\n            all_movies.extend(filtered_movies)\n        except Exception as e:\n            print(f\"Error fetching movies for genre '{genre}': {e}\")\n    # Remove duplicates and limit to num_results\n    all_movies = list({m['title']: m for m in all_movies}.values())\n    return all_movies[:num_results]\n\n# Function to get user preferences for weights\ndef get_user_preferences():\n    print(\"Assign weights to the following features (0-5):\")\n    genre_weight = int(input(\"Genres (0-5): \"))\n    director_weight = int(input(\"Directors (0-5): \"))\n    actor_weight = int(input(\"Actors (0-5): \"))\n    keyword_weight = int(input(\"Keywords (0-5): \"))\n    rating_weight = int(input(\"Ratings (0-5): \"))\n    overview_weight = int(input(\"Overview (0-5): \"))\n    \n    return {\n        'genre_weight': genre_weight,\n        'director_weight': director_weight,\n        'actor_weight': actor_weight,\n        'keyword_weight': keyword_weight,\n        'rating_weight': rating_weight,\n        'overview_weight': overview_weight\n    }\n\n# Function to calculate similarity based on features\ndef calculate_similarity(movie, other_movie, user_weights):\n    # Extract features to compare\n    movie_genres = set(movie.get('genres', []))\n    movie_directors = set([director['name'] for director in movie.get('directors', [])])\n    movie_actors = set([actor['name'] for actor in movie.get('cast', [])[:5]])\n    movie_keywords = set(movie.get('keywords', []))\n    movie_rating = movie.get('rating', 0)\n    movie_overview = movie.get('plot', [''])[0]  # Overview (plot)\n    \n    other_genres = set(other_movie.get('genres', []))\n    other_directors = set([director['name'] for director in other_movie.get('directors', [])])\n    other_actors = set([actor['name'] for actor in other_movie.get('cast', [])[:5]])\n    other_keywords = set(other_movie.get('keywords', []))\n    other_rating = other_movie.get('rating', 0)\n    other_overview = other_movie.get('plot', [''])[0]  # Overview (plot)\n    \n    # Calculate similarity\n    genre_similarity = len(movie_genres & other_genres) * user_weights['genre_weight']\n    director_similarity = len(movie_directors & other_directors) * user_weights['director_weight']\n    actor_similarity = len(movie_actors & other_actors) * user_weights['actor_weight']\n    keyword_similarity = len(movie_keywords & other_keywords) * user_weights['keyword_weight']\n    rating_similarity = (1 - abs(movie_rating - other_rating) / 10) * user_weights['rating_weight']\n    \n    # Simple overview similarity based on common words\n    common_words = len(set(movie_overview.split()) & set(other_overview.split()))\n    overview_similarity = (common_words / max(len(movie_overview.split()), len(other_overview.split()))) * user_weights['overview_weight']\n    \n    # Total score based on user-specified weights\n    return genre_similarity + director_similarity + actor_similarity + keyword_similarity + rating_similarity + overview_similarity\n\n# Function to get similar movies with user-specified weights\ndef recommend_similar_movies(movie, user_weights, num_recommendations=3):\n    ia.update(movie)\n    \n    # Extract features of the input movie\n    movie_genres = set(movie.get('genres', []))\n    movie_year = movie.get('year', 0)\n    movie_keywords = set(movie.get('keywords', []))\n    \n    # Fetch a larger dataset by genre, year, and keywords\n    similar_movies = get_movies_by_genre_and_year(movie_genres, movie_year, movie_keywords)\n    \n    recommendations = []\n    \n    for other_movie in similar_movies:\n        ia.update(other_movie)\n        \n        # Filter out TV shows\n        if 'movie' not in other_movie.get('kind', ''):\n            continue\n        \n        # Calculate similarity score\n        score = calculate_similarity(movie, other_movie, user_weights)\n        \n        if score > 0:\n            recommendations.append((other_movie, score))\n    \n    # Sort by score and return top recommendations\n    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n    return [movie[0] for movie in recommendations[:num_recommendations]]\n\n# Main function\ndef recommend_movies(movie_name):\n    # Get user preferences for weights\n    user_weights = get_user_preferences()\n    \n    # Get the user's movie\n    movie = get_movie_by_name(movie_name)\n    \n    if not movie:\n        return \"Movie not found. Please try a different name.\"\n    \n    print(f\"Movie found: {movie['title']} ({movie['year']})\")\n    \n    # Get 3 recommended movies based on user preferences\n    recommendations = recommend_similar_movies(movie, user_weights)\n    \n    # Output the recommendations\n    if recommendations:\n        print(\"\\nRecommended movies:\")\n        for rec in recommendations:\n            print(f\"- {rec['title']} ({rec['year']}) | Rating: {rec.get('rating', 'N/A')}\")\n    else:\n        print(\"No similar movies found.\")\n\n# Example usage\nmovie_name = input(\"Enter a movie name: \")\nrecommend_movies(movie_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imdb import IMDb\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Initialize IMDbPY instance\nia = IMDb()\n\n# Function to get movie features\ndef get_movie_features(movie):\n    ia.update(movie)\n    return {\n        'title': movie.get('title', ''),\n        'year': movie.get('year', ''),\n        'genres': movie.get('genres', []),\n        'directors': [director['name'] for director in movie.get('directors', [])],\n        'actors': [actor['name'] for actor in movie.get('cast', [])[:5]],\n        'keywords': movie.get('keywords', []),\n        'rating': movie.get('rating', 0),\n        'overview': movie.get('plot', [''])[0]  # Overview (plot)\n    }\n\n# Function to get user preferences for weights\ndef get_user_preferences():\n    print(\"Assign weights to the following features (0-5):\")\n    genre_weight = int(input(\"Genres (0-5): \"))\n    director_weight = int(input(\"Directors (0-5): \"))\n    actor_weight = int(input(\"Actors (0-5): \"))\n    keyword_weight = int(input(\"Keywords (0-5): \"))\n    rating_weight = int(input(\"Ratings (0-5): \"))\n    overview_weight = int(input(\"Overview (0-5): \"))\n    \n    return {\n        'genre_weight': genre_weight,\n        'director_weight': director_weight,\n        'actor_weight': actor_weight,\n        'keyword_weight': keyword_weight,\n        'rating_weight': rating_weight,\n        'overview_weight': overview_weight\n    }\n\n# Function to calculate similarity between two movies\ndef calculate_similarity(movie, other_movie, user_weights):\n    movie_features = get_movie_features(movie)\n    other_features = get_movie_features(other_movie)\n    \n    # Similarity based on genres\n    genre_similarity = len(set(movie_features['genres']) & set(other_features['genres'])) * user_weights['genre_weight']\n    \n    # Similarity based on directors\n    director_similarity = len(set(movie_features['directors']) & set(other_features['directors'])) * user_weights['director_weight']\n    \n    # Similarity based on actors\n    actor_similarity = len(set(movie_features['actors']) & set(other_features['actors'])) * user_weights['actor_weight']\n    \n    # Similarity based on keywords\n    keyword_similarity = len(set(movie_features['keywords']) & set(other_features['keywords'])) * user_weights['keyword_weight']\n    \n    # Similarity based on IMDb ratings\n    rating_similarity = (1 - abs(movie_features['rating'] - other_features['rating']) / 10) * user_weights['rating_weight']\n    \n    # Similarity based on overviews using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform([movie_features['overview'], other_features['overview']])\n    overview_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0] * user_weights['overview_weight']\n    \n    # Total score\n    return genre_similarity + director_similarity + actor_similarity + keyword_similarity + rating_similarity + overview_similarity\n\n# Function to recommend similar movies\ndef recommend_similar_movies(movie, user_weights, num_recommendations=3):\n    ia.update(movie)\n    \n    movie_features = get_movie_features(movie)\n    genres = movie_features['genres']\n    year = movie_features['year']\n    keywords = movie_features['keywords']\n    \n    # Fetch a large dataset\n    similar_movies = get_movies_by_genre_and_year(genres, year, keywords)\n    \n    recommendations = []\n    \n    for other_movie in similar_movies:\n        ia.update(other_movie)\n        \n        # Filter out TV shows\n        if 'movie' not in other_movie.get('kind', ''):\n            continue\n        \n        # Calculate similarity score\n        score = calculate_similarity(movie, other_movie, user_weights)\n        \n        if score > 0:\n            recommendations.append((other_movie, score))\n    \n    # Sort by score and return top recommendations\n    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n    return [movie[0] for movie in recommendations[:num_recommendations]]\n\n# Main function\ndef recommend_movies(movie_name):\n    user_weights = get_user_preferences()\n    movie = get_movie_by_name(movie_name)\n    \n    if not movie:\n        return \"Movie not found. Please try a different name.\"\n    \n    print(f\"Movie found: {movie['title']} ({movie['year']})\")\n    \n    recommendations = recommend_similar_movies(movie, user_weights)\n    \n    if recommendations:\n        print(\"\\nRecommended movies:\")\n        for rec in recommendations:\n            print(f\"- {rec['title']} ({rec['year']}) | Rating: {rec.get('rating', 'N/A')}\")\n    else:\n        print(\"No similar movies found.\")\n\n# Example usage\nmovie_name = input(\"Enter a movie name: \")\nrecommend_movies(movie_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom imdb import IMDb\n\n# Ensure NLTK resources are downloaded\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Initialize IMDb instance and NLTK components\nia = IMDb()\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef get_movie_features(movie):\n    ia.update(movie)\n    return {\n        'title': movie.get('title', ''),\n        'year': movie.get('year', ''),\n        'genres': movie.get('genres', []),\n        'directors': [director['name'] for director in movie.get('directors', [])],\n        'actors': [actor['name'] for actor in movie.get('cast', [])[:5]],\n        'keywords': movie.get('keywords', []),\n        'rating': movie.get('rating', 0),\n        'overview': movie.get('plot', [''])[0],\n        'release_date': movie.get('release_date', ''),\n        'votes': movie.get('votes', 0),\n        'runtime': movie.get('runtime', 0),\n        'language': movie.get('languages', []),\n        'country': movie.get('countries', []),\n        'certification': movie.get('certificates', []),\n        'soundtrack': movie.get('soundtracks', []),\n        'production_companies': [company['name'] for company in movie.get('production companies', [])],\n        'alternate_titles': movie.get('aka', []),\n        'tagline': movie.get('tagline', ''),\n        'box_office': movie.get('box office', {})\n    }\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n    return ' '.join(tokens)\n\ndef calculate_text_similarity(text1, text2):\n    # Placeholder for text similarity implementation\n    return 1 if text1.lower() == text2.lower() else 0\n\ndef calculate_date_similarity(date1, date2):\n    return 1 if date1[:4] == date2[:4] else 0\n\ndef calculate_box_office_similarity(box_office1, box_office2):\n    return 1 - abs(box_office1.get('gross', 0) - box_office2.get('gross', 0)) / max(box_office1.get('gross', 1), box_office2.get('gross', 1))\n\ndef calculate_similarity(movie, other_movie, user_weights):\n    movie_features = get_movie_features(movie)\n    other_features = get_movie_features(other_movie)\n    \n    genre_similarity = len(set(movie_features['genres']) & set(other_features['genres'])) * user_weights['genre_weight']\n    director_similarity = len(set(movie_features['directors']) & set(other_features['directors'])) * user_weights['director_weight']\n    actor_similarity = len(set(movie_features['actors']) & set(other_features['actors'])) * user_weights['actor_weight']\n    keyword_similarity = len(set(movie_features['keywords']) & set(other_features['keywords'])) * user_weights['keyword_weight']\n    rating_similarity = (1 - abs(movie_features['rating'] - other_features['rating']) / 10) * user_weights['rating_weight']\n    overview_similarity = calculate_text_similarity(preprocess_text(movie_features['overview']), preprocess_text(other_features['overview'])) * user_weights['overview_weight']\n    release_date_similarity = calculate_date_similarity(movie_features['release_date'], other_features['release_date']) * user_weights['release_date_weight']\n    votes_similarity = (1 - abs(movie_features['votes'] - other_features['votes']) / max(movie_features['votes'], 1)) * user_weights['votes_weight']\n    runtime_similarity = (1 - abs(movie_features['runtime'] - other_features['runtime']) / max(movie_features['runtime'], 1)) * user_weights['runtime_weight']\n    language_similarity = len(set(movie_features['language']) & set(other_features['language'])) * user_weights['language_weight']\n    country_similarity = len(set(movie_features['country']) & set(other_features['country'])) * user_weights['country_weight']\n    certification_similarity = len(set(movie_features['certification']) & set(other_features['certification'])) * user_weights['certification_weight']\n    soundtrack_similarity = len(set(movie_features['soundtrack']) & set(other_features['soundtrack'])) * user_weights['soundtrack_weight']\n    production_companies_similarity = len(set(movie_features['production_companies']) & set(other_features['production_companies'])) * user_weights['production_companies_weight']\n    alternate_titles_similarity = len(set(movie_features['alternate_titles']) & set(other_features['alternate_titles'])) * user_weights['alternate_titles_weight']\n    tagline_similarity = calculate_text_similarity(movie_features['tagline'], other_features['tagline']) * user_weights['tagline_weight']\n    box_office_similarity = calculate_box_office_similarity(movie_features['box_office'], other_features['box_office']) * user_weights['box_office_weight']\n    \n    return (genre_similarity + director_similarity + actor_similarity + keyword_similarity + rating_similarity +\n            overview_similarity + release_date_similarity + votes_similarity + runtime_similarity +\n            language_similarity + country_similarity + certification_similarity + soundtrack_similarity +\n            production_companies_similarity + alternate_titles_similarity + tagline_similarity +\n            box_office_similarity)\n\ndef get_user_preferences():\n    print(\"Assign weights to the following features (0-5):\")\n    genre_weight = int(input(\"Genres (0-5): \"))\n    director_weight = int(input(\"Directors (0-5): \"))\n    actor_weight = int(input(\"Actors (0-5): \"))\n    keyword_weight = int(input(\"Keywords (0-5): \"))\n    rating_weight = int(input(\"Ratings (0-5): \"))\n    overview_weight = int(input(\"Overview (0-5): \"))\n    release_date_weight = int(input(\"Release Date (0-5): \"))\n    votes_weight = int(input(\"Votes (0-5): \"))\n    runtime_weight = int(input(\"Runtime (0-5): \"))\n    language_weight = int(input(\"Language (0-5): \"))\n    country_weight = int(input(\"Country (0-5): \"))\n    certification_weight = int(input(\"Certification (0-5): \"))\n    soundtrack_weight = int(input(\"Soundtrack (0-5): \"))\n    production_companies_weight = int(input(\"Production Companies (0-5): \"))\n    alternate_titles_weight = int(input(\"Alternate Titles (0-5): \"))\n    tagline_weight = int(input(\"Tagline (0-5): \"))\n    box_office_weight = int(input(\"Box Office (0-5): \"))\n    \n    return {\n        'genre_weight': genre_weight,\n        'director_weight': director_weight,\n        'actor_weight': actor_weight,\n        'keyword_weight': keyword_weight,\n        'rating_weight': rating_weight,\n        'overview_weight': overview_weight,\n        'release_date_weight': release_date_weight,\n        'votes_weight': votes_weight,\n        'runtime_weight': runtime_weight,\n        'language_weight': language_weight,\n        'country_weight': country_weight,\n        'certification_weight': certification_weight,\n        'soundtrack_weight': soundtrack_weight,\n        'production_companies_weight': production_companies_weight,\n        'alternate_titles_weight': alternate_titles_weight,\n        'tagline_weight': tagline_weight,\n        'box_office_weight': box_office_weight\n    }\n\ndef get_movie_by_name(movie_name):\n    search_results = ia.search_movie(movie_name)\n    if search_results:\n        return ia.get_movie(search_results[0].movieID)\n    return None\n\ndef recommend_similar_movies(movie, user_weights, num_recommendations=3):\n    recommendations = []\n    movie_list = ia.get_top250_movies()  # Example: getting a list of top movies. Adjust as needed.\n    for other_movie in movie_list:\n        if other_movie.movieID == movie.movieID:\n            continue\n        \n        score = calculate_similarity(movie, other_movie, user_weights)\n        recommendations.append((other_movie, score))\n    \n    recommendations.sort(key=lambda x: x[1], reverse=True)\n    return recommendations[:num_recommendations]\n\ndef recommend_movies(movie_name):\n    user_weights = get_user_preferences()\n    movie = get_movie_by_name(movie_name)\n    if not movie:\n        return \"Movie not found. Please try a different name.\"\n    \n    print(f\"Movie found: {movie['title']} ({movie['year']})\")\n\n    recommendations = recommend_similar_movies(movie, user_weights)\n    if recommendations:\n        print(\"\\nRecommended movies:\")\n        for rec_movie, score in recommendations:\n            print(f\"- {rec_movie['title']} ({rec_movie['year']}) | Rating: {rec_movie.get('rating', 'N/A')} | Score: {score:.2f}\")\n    else:\n        print(\"No similar movies found.\")\n\n# Example usage\nmovie_name = input(\"Enter a movie name: \")\nrecommend_movies(movie_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T23:57:13.190878Z","iopub.execute_input":"2024-09-17T23:57:13.191457Z","iopub.status.idle":"2024-09-17T23:57:57.143253Z","shell.execute_reply.started":"2024-09-17T23:57:13.191408Z","shell.execute_reply":"2024-09-17T23:57:57.141283Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a movie name:  interstellar\n"},{"name":"stdout","text":"Assign weights to the following features (0-5):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Genres (0-5):  3\nDirectors (0-5):  5\nActors (0-5):  1\nKeywords (0-5):  3\nRatings (0-5):  5\nOverview (0-5):  5\nRelease Date (0-5):  0\nVotes (0-5):  2\nRuntime (0-5):  0\nLanguage (0-5):  0\nCountry (0-5):  0\nCertification (0-5):  3\nSoundtrack (0-5):  3\nProduction Companies (0-5):  0\nAlternate Titles (0-5):  0\nTagline (0-5):  0\nBox Office (0-5):  3\n"},{"name":"stdout","text":"Movie found: Interstellar (2014)\nNo similar movies found.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Load the dataset\nfile_path = '/kaggle/input/movies-updated/movies_with_tags_rating.csv'\ndf = pd.read_csv(file_path)\n\n\n# Fill missing values\ndf['title'].fillna('No title')\ndf['film_rate'].fillna('Not Mentioned')  # Adjust as needed\ndf['genre'].fillna('Not Mentioned')\ndf['cast'].fillna('Not Mentioned')\ndf['reviews'].fillna('Not Mentioned')  # Adjust as needed\ndf['imdbId'].fillna('Not Mentioned')\ndf['director'].fillna('Not Mentioned')\ndf['rating'].fillna(0)  # Assuming rating is numerical\ndf['tag'].fillna('Not Mentioned')\n\n\"\"\"\n# Ensure NLTK resources are downloaded\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\n\"\"\"\n# Initialize NLTK components\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef get_movie_features(movie_title, df):\n    movie = df[df['title'].str.lower() == movie_title.lower()]\n    if movie.empty:\n        return None\n    movie = movie.iloc[0]  # Get the first row\n    \n    # Use .get() method or default to empty strings if NaN or missing\n    return {\n        'title': movie.get('title', ''),\n        'year': movie.get('film_rate', ''),  # Adjust as needed\n        'genres': str(movie.get('genre', '')).split(', '),  # Convert to string and split\n        'directors': [str(movie.get('director', ''))],\n        'actors': str(movie.get('cast', '')).split(', '),  # Convert to string and split\n        'keywords': str(movie.get('tag', '')).split(', '),  # Convert to string and split\n        'rating': movie.get('rating', 0),\n        'overview': str(movie.get('overview', '')),\n        'release_date': '',  # Add or adjust as needed\n        'votes': 0,  # Add or adjust as needed\n        'runtime': 0,  # Add or adjust as needed\n        'language': '',  # Add or adjust as needed\n        'country': '',  # Add or adjust as needed\n        'certification': '',  # Add or adjust as needed\n        'soundtrack': '',  # Add or adjust as needed\n        'production_companies': '',  # Add or adjust as needed\n        'alternate_titles': '',  # Add or adjust as needed\n        'tagline': '',  # Add or adjust as needed\n        'box_office': {}  # Add or adjust as needed\n    }\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n    return ' '.join(tokens)\n\ndef calculate_text_similarity(text1, text2):\n    # Placeholder for text similarity implementation\n    return 1 if text1.lower() == text2.lower() else 0\n\ndef calculate_date_similarity(date1, date2):\n    return 1 if date1[:4] == date2[:4] else 0\n\ndef calculate_box_office_similarity(box_office1, box_office2):\n    return 1 - abs(box_office1.get('gross', 0) - box_office2.get('gross', 0)) / max(box_office1.get('gross', 1), box_office2.get('gross', 1))\n\ndef calculate_similarity(movie, other_movie, user_weights):\n    movie_features = get_movie_features(movie, df)\n    other_features = get_movie_features(other_movie, df)\n    \n    if not movie_features or not other_features:\n        return 0  # Return 0 if either movie is not found\n\n    genre_similarity = len(set(movie_features['genres']) & set(other_features['genres'])) * user_weights['genre_weight']\n    director_similarity = len(set(movie_features['directors']) & set(other_features['directors'])) * user_weights['director_weight']\n    actor_similarity = len(set(movie_features['actors']) & set(other_features['actors'])) * user_weights['actor_weight']\n    keyword_similarity = len(set(movie_features['keywords']) & set(other_features['keywords'])) * user_weights['keyword_weight']\n    rating_similarity = (1 - abs(movie_features['rating'] - other_features['rating']) / 10) * user_weights['rating_weight']\n    overview_similarity = calculate_text_similarity(preprocess_text(movie_features['overview']), preprocess_text(other_features['overview'])) * user_weights['overview_weight']\n    release_date_similarity = calculate_date_similarity(movie_features['release_date'], other_features['release_date']) * user_weights['release_date_weight']\n    votes_similarity = (1 - abs(movie_features['votes'] - other_features['votes']) / max(movie_features['votes'], 1)) * user_weights['votes_weight']\n    runtime_similarity = (1 - abs(movie_features['runtime'] - other_features['runtime']) / max(movie_features['runtime'], 1)) * user_weights['runtime_weight']\n    language_similarity = len(set(movie_features['language']) & set(other_features['language'])) * user_weights['language_weight']\n    country_similarity = len(set(movie_features['country']) & set(other_features['country'])) * user_weights['country_weight']\n    certification_similarity = len(set(movie_features['certification']) & set(other_features['certification'])) * user_weights['certification_weight']\n    soundtrack_similarity = len(set(movie_features['soundtrack']) & set(other_features['soundtrack'])) * user_weights['soundtrack_weight']\n    production_companies_similarity = len(set(movie_features['production_companies']) & set(other_features['production_companies'])) * user_weights['production_companies_weight']\n    alternate_titles_similarity = len(set(movie_features['alternate_titles']) & set(other_features['alternate_titles'])) * user_weights['alternate_titles_weight']\n    tagline_similarity = calculate_text_similarity(movie_features['tagline'], other_features['tagline']) * user_weights['tagline_weight']\n    box_office_similarity = calculate_box_office_similarity(movie_features['box_office'], other_features['box_office']) * user_weights['box_office_weight']\n    \n    return (genre_similarity + director_similarity + actor_similarity + keyword_similarity + rating_similarity +\n            overview_similarity + release_date_similarity + votes_similarity + runtime_similarity +\n            language_similarity + country_similarity + certification_similarity + soundtrack_similarity +\n            production_companies_similarity + alternate_titles_similarity + tagline_similarity +\n            box_office_similarity)\n\ndef get_user_preferences():\n    print(\"Assign weights to the following features (0-5):\")\n    genre_weight = int(input(\"Genres (0-5): \"))\n    director_weight = int(input(\"Directors (0-5): \"))\n    actor_weight = int(input(\"Actors (0-5): \"))\n    keyword_weight = int(input(\"Keywords (0-5): \"))\n    rating_weight = int(input(\"Ratings (0-5): \"))\n    overview_weight = int(input(\"Overview (0-5): \"))\n    release_date_weight = int(input(\"Release Date (0-5): \"))\n    votes_weight = int(input(\"Votes (0-5): \"))\n    runtime_weight = int(input(\"Runtime (0-5): \"))\n    language_weight = int(input(\"Language (0-5): \"))\n    country_weight = int(input(\"Country (0-5): \"))\n    certification_weight = int(input(\"Certification (0-5): \"))\n    soundtrack_weight = int(input(\"Soundtrack (0-5): \"))\n    production_companies_weight = int(input(\"Production Companies (0-5): \"))\n    alternate_titles_weight = int(input(\"Alternate Titles (0-5): \"))\n    tagline_weight = int(input(\"Tagline (0-5): \"))\n    box_office_weight = int(input(\"Box Office (0-5): \"))\n    \n    return {\n        'genre_weight': genre_weight,\n        'director_weight': director_weight,\n        'actor_weight': actor_weight,\n        'keyword_weight': keyword_weight,\n        'rating_weight': rating_weight,\n        'overview_weight': overview_weight,\n        'release_date_weight': release_date_weight,\n        'votes_weight': votes_weight,\n        'runtime_weight': runtime_weight,\n        'language_weight': language_weight,\n        'country_weight': country_weight,\n        'certification_weight': certification_weight,\n        'soundtrack_weight': soundtrack_weight,\n        'production_companies_weight': production_companies_weight,\n        'alternate_titles_weight': alternate_titles_weight,\n        'tagline_weight': tagline_weight,\n        'box_office_weight': box_office_weight\n    }\n\ndef recommend_similar_movies(movie, user_weights, df):\n    # Get all movies\n    all_movies = df['title'].tolist()\n    \n    # Calculate similarity for all movies\n    similarities = []\n    for other_movie in all_movies:\n        if other_movie == movie:\n            continue\n        other_movie_features = get_movie_features(other_movie, df)\n        if other_movie_features:\n            score = calculate_similarity(movie, other_movie_features, user_weights)\n            similarities.append((other_movie_features, score))\n    \n    # Sort movies by similarity score\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    \n    return similarities[:10]  # Return top 10 recommendations\n\ndef recommend_movies(movie_name):\n    user_weights = get_user_preferences()\n    movie = get_movie_features(movie_name, df)\n    if not movie:\n        print(\"Movie not found. Please try a different name.\")\n        return\n\n    print(f\"Movie found: {movie['title']} ({movie['year']})\")\n\n    recommendations = recommend_similar_movies(movie, user_weights, df)\n    if recommendations:\n        print(\"\\nRecommended movies:\")\n        for rec_movie, score in recommendations:\n            print(f\"- {rec_movie['title']} ({rec_movie['year']}) | Rating: {rec_movie.get('rating', 'N/A')} | Score: {score:.2f}\")\n    else:\n        print(\"No similar movies found.\")\n\n# Example usage\nmovie_name = input(\"Enter a movie name: \")\nrecommend_movies(movie_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T00:14:33.736276Z","iopub.execute_input":"2024-09-18T00:14:33.737756Z","iopub.status.idle":"2024-09-18T00:15:00.117504Z","shell.execute_reply.started":"2024-09-18T00:14:33.737688Z","shell.execute_reply":"2024-09-18T00:15:00.115271Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Assign weights to the following features (0-5):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Genres (0-5):  3\nDirectors (0-5):  3\nActors (0-5):  3\nKeywords (0-5):  3\nRatings (0-5):  5\nOverview (0-5):  5\nRelease Date (0-5):  0\nVotes (0-5):  3\nRuntime (0-5):  0\nLanguage (0-5):  0\nCountry (0-5):  0\nCertification (0-5):  0\nSoundtrack (0-5):  0\nProduction Companies (0-5):  0\nAlternate Titles (0-5):  0\nTagline (0-5):  0\nBox Office (0-5):  0\n"},{"name":"stdout","text":"Movie found: Heat (8.3)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 186\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    185\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m \u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[6], line 176\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmovie[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmovie[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_similar_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recommendations:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRecommended movies:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[6], line 159\u001b[0m, in \u001b[0;36mrecommend_similar_movies\u001b[0;34m(movie, user_weights, df)\u001b[0m\n\u001b[1;32m    157\u001b[0m     other_movie_features \u001b[38;5;241m=\u001b[39m get_movie_features(other_movie, df)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other_movie_features:\n\u001b[0;32m--> 159\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_movie_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m         similarities\u001b[38;5;241m.\u001b[39mappend((other_movie_features, score))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Sort movies by similarity score\u001b[39;00m\n","Cell \u001b[0;32mIn[6], line 78\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[0;34m(movie, other_movie, user_weights)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_similarity\u001b[39m(movie, other_movie, user_weights):\n\u001b[0;32m---> 78\u001b[0m     movie_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_movie_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     other_features \u001b[38;5;241m=\u001b[39m get_movie_features(other_movie, df)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m movie_features \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m other_features:\n","Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36mget_movie_features\u001b[0;34m(movie_title, df)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_movie_features\u001b[39m(movie_title, df):\n\u001b[0;32m---> 34\u001b[0m     movie \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[43mmovie_title\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m movie\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'lower'"],"ename":"AttributeError","evalue":"'dict' object has no attribute 'lower'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Load the dataset\nfile_path = '/kaggle/input/movies-updated/movies_with_tags_rating.csv'\ndf = pd.read_csv(file_path)\n\n# Fill missing values\ndf['title'].fillna('No title', inplace=True)\ndf['film_rate'].fillna('Not Mentioned', inplace=True)  # Adjust as needed\ndf['genre'].fillna('Not Mentioned', inplace=True)\ndf['cast'].fillna('Not Mentioned', inplace=True)\ndf['reviews'].fillna('Not Mentioned', inplace=True)  # Adjust as needed\ndf['imdbId'].fillna('Not Mentioned', inplace=True)\ndf['director'].fillna('Not Mentioned', inplace=True)\ndf['rating'].fillna(0, inplace=True)  # Assuming rating is numerical\ndf['tag'].fillna('Not Mentioned', inplace=True)\n\n# Ensure NLTK resources are downloaded\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\n\n# Initialize NLTK components\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef get_movie_features(movie_title, df):\n    movie = df[df['title'].str.lower() == movie_title.lower()]\n    if movie.empty:\n        return None\n    movie = movie.iloc[0]  # Get the first row\n    \n    return {\n        'title': movie.get('title', 'Not Mentioned'),\n        'year': movie.get('film_rate', 'Not Mentioned'),  # Adjust as needed\n        'genres': str(movie.get('genre', 'Not Mentioned')).split(', '),\n        'directors': [str(movie.get('director', 'Not Mentioned'))],\n        'actors': str(movie.get('cast', 'Not Mentioned')).split(', '),\n        'keywords': str(movie.get('tag', 'Not Mentioned')).split(', '),\n        'rating': movie.get('rating', 0),\n        'overview': str(movie.get('overview', 'Not Mentioned')),\n        'release_date': '',  # Add or adjust as needed\n        'votes': 0,  # Add or adjust as needed\n        'runtime': 0,  # Add or adjust as needed\n        'language': '',  # Add or adjust as needed\n        'country': '',  # Add or adjust as needed\n        'certification': '',  # Add or adjust as needed\n        'soundtrack': '',  # Add or adjust as needed\n        'production_companies': '',  # Add or adjust as needed\n        'alternate_titles': '',  # Add or adjust as needed\n        'tagline': '',  # Add or adjust as needed\n        'box_office': {}  # Add or adjust as needed\n    }\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n    return ' '.join(tokens)\n\ndef calculate_text_similarity(text1, text2):\n    # Placeholder for text similarity implementation\n    return 1 if text1.lower() == text2.lower() else 0\n\ndef calculate_date_similarity(date1, date2):\n    return 1 if date1[:4] == date2[:4] else 0\n\ndef calculate_box_office_similarity(box_office1, box_office2):\n    return 1 - abs(box_office1.get('gross', 0) - box_office2.get('gross', 0)) / max(box_office1.get('gross', 1), box_office2.get('gross', 1))\n\ndef calculate_similarity(movie, other_movie, user_weights):\n    movie_features = get_movie_features(movie, df)\n    other_features = get_movie_features(other_movie, df)\n    \n    if not movie_features or not other_features:\n        return 0  # Return 0 if either movie is not found\n\n    genre_similarity = len(set(movie_features['genres']) & set(other_features['genres'])) * user_weights['genre_weight']\n    director_similarity = len(set(movie_features['directors']) & set(other_features['directors'])) * user_weights['director_weight']\n    actor_similarity = len(set(movie_features['actors']) & set(other_features['actors'])) * user_weights['actor_weight']\n    keyword_similarity = len(set(movie_features['keywords']) & set(other_features['keywords'])) * user_weights['keyword_weight']\n    rating_similarity = (1 - abs(movie_features['rating'] - other_features['rating']) / 10) * user_weights['rating_weight']\n    overview_similarity = calculate_text_similarity(preprocess_text(movie_features['overview']), preprocess_text(other_features['overview'])) * user_weights['overview_weight']\n    release_date_similarity = calculate_date_similarity(movie_features['release_date'], other_features['release_date']) * user_weights['release_date_weight']\n    votes_similarity = (1 - abs(movie_features['votes'] - other_features['votes']) / max(movie_features['votes'], 1)) * user_weights['votes_weight']\n    runtime_similarity = (1 - abs(movie_features['runtime'] - other_features['runtime']) / max(movie_features['runtime'], 1)) * user_weights['runtime_weight']\n    language_similarity = len(set(movie_features['language']) & set(other_features['language'])) * user_weights['language_weight']\n    country_similarity = len(set(movie_features['country']) & set(other_features['country'])) * user_weights['country_weight']\n    certification_similarity = len(set(movie_features['certification']) & set(other_features['certification'])) * user_weights['certification_weight']\n    soundtrack_similarity = len(set(movie_features['soundtrack']) & set(other_features['soundtrack'])) * user_weights['soundtrack_weight']\n    production_companies_similarity = len(set(movie_features['production_companies']) & set(other_features['production_companies'])) * user_weights['production_companies_weight']\n    alternate_titles_similarity = len(set(movie_features['alternate_titles']) & set(other_features['alternate_titles'])) * user_weights['alternate_titles_weight']\n    tagline_similarity = calculate_text_similarity(movie_features['tagline'], other_features['tagline']) * user_weights['tagline_weight']\n    box_office_similarity = calculate_box_office_similarity(movie_features['box_office'], other_features['box_office']) * user_weights['box_office_weight']\n    \n    return (genre_similarity + director_similarity + actor_similarity + keyword_similarity + rating_similarity +\n            overview_similarity + release_date_similarity + votes_similarity + runtime_similarity +\n            language_similarity + country_similarity + certification_similarity + soundtrack_similarity +\n            production_companies_similarity + alternate_titles_similarity + tagline_similarity +\n            box_office_similarity)\n\ndef get_user_preferences():\n    print(\"Assign weights to the following features (0-5):\")\n    genre_weight = int(input(\"Genres (0-5): \"))\n    director_weight = int(input(\"Directors (0-5): \"))\n    actor_weight = int(input(\"Actors (0-5): \"))\n    keyword_weight = int(input(\"Keywords (0-5): \"))\n    rating_weight = int(input(\"Ratings (0-5): \"))\n    overview_weight = int(input(\"Overview (0-5): \"))\n    release_date_weight = int(input(\"Release Date (0-5): \"))\n    votes_weight = int(input(\"Votes (0-5): \"))\n    runtime_weight = int(input(\"Runtime (0-5): \"))\n    language_weight = int(input(\"Language (0-5): \"))\n    country_weight = int(input(\"Country (0-5): \"))\n    certification_weight = int(input(\"Certification (0-5): \"))\n    soundtrack_weight = int(input(\"Soundtrack (0-5): \"))\n    production_companies_weight = int(input(\"Production Companies (0-5): \"))\n    alternate_titles_weight = int(input(\"Alternate Titles (0-5): \"))\n    tagline_weight = int(input(\"Tagline (0-5): \"))\n    box_office_weight = int(input(\"Box Office (0-5): \"))\n    \n    return {\n        'genre_weight': genre_weight,\n        'director_weight': director_weight,\n        'actor_weight': actor_weight,\n        'keyword_weight': keyword_weight,\n        'rating_weight': rating_weight,\n        'overview_weight': overview_weight,\n        'release_date_weight': release_date_weight,\n        'votes_weight': votes_weight,\n        'runtime_weight': runtime_weight,\n        'language_weight': language_weight,\n        'country_weight': country_weight,\n        'certification_weight': certification_weight,\n        'soundtrack_weight': soundtrack_weight,\n        'production_companies_weight': production_companies_weight,\n        'alternate_titles_weight': alternate_titles_weight,\n        'tagline_weight': tagline_weight,\n        'box_office_weight': box_office_weight\n    }\n\ndef recommend_similar_movies(movie, user_weights, df):\n    # Get all movies\n    all_movies = df['title'].tolist()\n    \n    # Calculate similarity for all movies\n    similarities = []\n    for other_movie in all_movies:\n        if other_movie == movie:\n            continue\n        other_movie_features = get_movie_features(other_movie, df)\n        if other_movie_features:\n            score = calculate_similarity(movie, other_movie, user_weights)\n            similarities.append((other_movie_features, score))\n    \n    # Sort movies by similarity score\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    \n    return similarities[:10]  # Return top 10 recommendations\n\ndef recommend_movies(movie_name):\n    user_weights = get_user_preferences()\n    movie = get_movie_features(movie_name, df)\n    if not movie:\n        print(\"Movie not found. Please try a different name.\")\n        return\n\n    print(f\"Movie found: {movie['title']} ({movie['year']})\")\n\n    recommendations = recommend_similar_movies(movie['title'], user_weights, df)\n    if recommendations:\n        print(\"\\nRecommended movies:\")\n        for rec_movie, score in recommendations:\n            print(f\"- {rec_movie['title']} ({rec_movie['year']}) | Rating: {rec_movie.get('rating', 'N/A')} | Score: {score:.2f}\")\n    else:\n        print(\"No similar movies found.\")\n\n# Example usage\nmovie_name = input(\"Enter a movie name: \")\nrecommend_movies(movie_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T00:16:34.452646Z","iopub.execute_input":"2024-09-18T00:16:34.453197Z","iopub.status.idle":"2024-09-18T00:17:00.587133Z","shell.execute_reply.started":"2024-09-18T00:16:34.453148Z","shell.execute_reply":"2024-09-18T00:17:00.584605Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_158/1561368314.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['title'].fillna('No title', inplace=True)\n/tmp/ipykernel_158/1561368314.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['film_rate'].fillna('Not Mentioned', inplace=True)  # Adjust as needed\n/tmp/ipykernel_158/1561368314.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['genre'].fillna('Not Mentioned', inplace=True)\n/tmp/ipykernel_158/1561368314.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['cast'].fillna('Not Mentioned', inplace=True)\n/tmp/ipykernel_158/1561368314.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['reviews'].fillna('Not Mentioned', inplace=True)  # Adjust as needed\n/tmp/ipykernel_158/1561368314.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['imdbId'].fillna('Not Mentioned', inplace=True)\n/tmp/ipykernel_158/1561368314.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['director'].fillna('Not Mentioned', inplace=True)\n/tmp/ipykernel_158/1561368314.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['rating'].fillna(0, inplace=True)  # Assuming rating is numerical\n/tmp/ipykernel_158/1561368314.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['tag'].fillna('Not Mentioned', inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Assign weights to the following features (0-5):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Genres (0-5):  3\nDirectors (0-5):  5\nActors (0-5):  1\nKeywords (0-5):  5\nRatings (0-5):  5\nOverview (0-5):  5\nRelease Date (0-5):  0\nVotes (0-5):  0\nRuntime (0-5):  0\nLanguage (0-5):  0\nCountry (0-5):  0\nCertification (0-5):  0\nSoundtrack (0-5):  0\nProduction Companies (0-5):  0\nAlternate Titles (0-5):  0\nTagline (0-5):  0\nBox Office (0-5):  0\n"},{"name":"stdout","text":"Movie found: Heat (8.3)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 183\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    182\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 173\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmovie[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmovie[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 173\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_similar_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recommendations:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRecommended movies:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[7], line 156\u001b[0m, in \u001b[0;36mrecommend_similar_movies\u001b[0;34m(movie, user_weights, df)\u001b[0m\n\u001b[1;32m    154\u001b[0m     other_movie_features \u001b[38;5;241m=\u001b[39m get_movie_features(other_movie, df)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other_movie_features:\n\u001b[0;32m--> 156\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_movie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         similarities\u001b[38;5;241m.\u001b[39mappend((other_movie_features, score))\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Sort movies by similarity score\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 86\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[0;34m(movie, other_movie, user_weights)\u001b[0m\n\u001b[1;32m     84\u001b[0m keyword_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(movie_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(other_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;241m*\u001b[39m user_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     85\u001b[0m rating_similarity \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mabs\u001b[39m(movie_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m other_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m*\u001b[39m user_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 86\u001b[0m overview_similarity \u001b[38;5;241m=\u001b[39m calculate_text_similarity(\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, preprocess_text(other_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverview\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;241m*\u001b[39m user_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverview_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     87\u001b[0m release_date_similarity \u001b[38;5;241m=\u001b[39m calculate_date_similarity(movie_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelease_date\u001b[39m\u001b[38;5;124m'\u001b[39m], other_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelease_date\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m user_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelease_date_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     88\u001b[0m votes_similarity \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mabs\u001b[39m(movie_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvotes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m other_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvotes\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(movie_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvotes\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m user_weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvotes_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","Cell \u001b[0;32mIn[7], line 61\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m     60\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m---> 61\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n","Cell \u001b[0;32mIn[7], line 61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m     60\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m---> 61\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"],"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************","output_type":"error"}]},{"cell_type":"code","source":"pip install tmdbv3api\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:33:54.820633Z","iopub.execute_input":"2024-09-18T12:33:54.821119Z","iopub.status.idle":"2024-09-18T12:34:13.705478Z","shell.execute_reply.started":"2024-09-18T12:33:54.821070Z","shell.execute_reply":"2024-09-18T12:34:13.703954Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting tmdbv3api\n  Downloading tmdbv3api-1.9.0-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tmdbv3api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (2024.7.4)\nDownloading tmdbv3api-1.9.0-py3-none-any.whl (25 kB)\nInstalling collected packages: tmdbv3api\nSuccessfully installed tmdbv3api-1.9.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\nuser_movie_ratings = pd.read_csv('/kaggle/input/rating-imdbid/ratings.csv')\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Check the first few rows of each DataFrame to ensure proper loading\nprint(user_movie_ratings.head())\nprint(movies_with_tags.head())\n\n# Merge datasets on 'imdbId'\nuser_movie_ratings = user_movie_ratings.merge(movies_with_tags[['imdbId', 'title', 'director', 'genre', 'rating']], on='imdbId', how='left')\n\n# Now user_movie_ratings contains additional movie details from movies_with_tags\nprint(user_movie_ratings.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T13:11:51.807460Z","iopub.execute_input":"2024-09-18T13:11:51.808064Z","iopub.status.idle":"2024-09-18T13:11:53.525448Z","shell.execute_reply.started":"2024-09-18T13:11:51.808010Z","shell.execute_reply":"2024-09-18T13:11:53.524178Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"   userId  imdbId  rating   timestamp\n0       1       1     4.0  1225734739\n1       1     110     4.0  1225865086\n2       1     158     4.0  1225733503\n3       1     260     4.5  1225735204\n4       1     356     5.0  1225735119\n   index                          title  film_rate  \\\n0      0               Before and After        6.1   \n1      1          Anne Frank Remembered        8.1   \n2      2  The Young Poisoner's Handbook        7.0   \n3      3                   If Lucy Fell        4.9   \n4      4         Steal Big Steal Little        4.7   \n\n                                            overview  \\\n0  ['Two parents deal with the effects when their...   \n1  [\"Tells the story of the Frank family and pain...   \n2  [\"This film is based on a true story about a B...   \n3  [\"Two NYC roommates have a pact to jump off Br...   \n4  [\"Ruben and Robby are twin brothers, adopted b...   \n\n                                           cover_url  \\\n0  https://m.media-amazon.com/images/M/MV5BOWJmOD...   \n1  https://m.media-amazon.com/images/M/MV5BNGZjNz...   \n2  https://m.media-amazon.com/images/M/MV5BMTg0MT...   \n3  https://m.media-amazon.com/images/M/MV5BMmFjYW...   \n4  https://m.media-amazon.com/images/M/MV5BNjVhMz...   \n\n                                                cast  \\\n0          Meryl Streep, Liam Neeson, Edward Furlong   \n1           Isa Baschwitz, Mary Bos, Kenneth Branagh   \n2        Tobias Arnold, Ruth Sheen, Roger Lloyd Pack   \n3  Sarah Jessica Parker, Eric Schaeffer, Ben Stiller   \n4            Andy Garcia, Alan Arkin, Rachel Ticotin   \n\n                                             reviews  imdbId  \\\n0  While not a terrible movie it has a few seriou...  115645   \n1  The Diary of Anne Frank is the second best-sel...  112373   \n2  Director Benjamin Ross has done a terrific job...  115033   \n3  ...to Eric Schaeffer, but he's not the leading...  116606   \n4  During the time this was being shot I was spen...  114536   \n\n           director                        genre  rating  tag  \n0  Barbet Schroeder        Crime, Drama, Mystery    3.05  NaN  \n1         Jon Blair  Documentary, Biography, War    4.05  NaN  \n2     Benjamin Ross         Comedy, Crime, Drama    3.50  NaN  \n3    Eric Schaeffer              Comedy, Romance    4.00  NaN  \n4      Andrew Davis                       Comedy    2.35  NaN  \n   userId  imdbId  rating_x   timestamp title director genre  rating_y\n0       1       1       4.0  1225734739   NaN      NaN   NaN       NaN\n1       1     110       4.0  1225865086   NaN      NaN   NaN       NaN\n2       1     158       4.0  1225733503   NaN      NaN   NaN       NaN\n3       1     260       4.5  1225735204   NaN      NaN   NaN       NaN\n4       1     356       5.0  1225735119   NaN      NaN   NaN       NaN\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the ratings dataset with IMDb IDs\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\n# Load the movie titles dataset (with IMDb IDs)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Merge ratings with movie details\nuser_movie_ratings = user_movie_ratings.merge(movies_with_tags[['imdbId', 'title', 'director', 'genre']], on='imdbId', how='left')\n\n# Function to fetch genres and director for a movie using TMDb (searching by TMDb ID)\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details including credits\n    movie_details = movie.details(tmdb_id, append_to_response='credits')\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.genres]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.credits['crew']:\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n            \n    return genres, director\n\n# Collaborative Filtering (using matrix factorization)\ndef collaborative_filtering(user_movie_ratings):\n    # User-Item Matrix (using movie titles instead of movie IDs)\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Matrix Factorization using Truncated SVD\n    svd = TruncatedSVD(n_components=300)  # Increase latent factors to 300\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n\n    # Cosine Similarity between movies\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    return cosine_sim_matrix, user_movie_matrix.columns\n\ndef recommend_movies(movie_name, user_movie_ratings):\n    # Search for the movie by name\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    # Get the first search result's TMDb ID\n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    # Fetch movie details (genres, director) using TMDb ID\n    genres, director = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}\")\n    \n    # Check if IMDb ID exists in the ratings dataset\n    imdb_id = search_results[0].id\n    if imdb_id not in user_movie_ratings['imdbId'].unique():\n        return \"Movie not found in the ratings dataset.\"\n\n    # Get the user-item matrix and perform collaborative filtering\n    cosine_sim_matrix, movie_titles = collaborative_filtering(user_movie_ratings)\n    \n    # Find the movie index in the cosine similarity matrix (title to index mapping)\n    try:\n        movie_idx = list(movie_titles).index(search_results[0].title)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    # Find similar movies using collaborative filtering\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n\n    # Sort by similarity score\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    # Fetch details for top similar movies\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = movie_titles[movie_idx]\n        \n        # Filter by content (same genres, same director)\n        similar_movie_id = movies_with_tags[movies_with_tags['title'] == similar_movie_title]['imdbId'].values[0]\n        rec_genres, rec_director = get_movie_details_by_tmdb(similar_movie_id)\n        \n        if any(g in genres for g in rec_genres) or rec_director == director:\n            recommended_movies.append(f\"{i+1}. {similar_movie_title} (Genres: {rec_genres})\")\n            if len(recommended_movies) == 3:\n                break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name, user_movie_ratings))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T13:17:55.797820Z","iopub.execute_input":"2024-09-18T13:17:55.798340Z","iopub.status.idle":"2024-09-18T13:18:00.424551Z","shell.execute_reply.started":"2024-09-18T13:17:55.798292Z","shell.execute_reply":"2024-09-18T13:18:00.422723Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: Michael Mann\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    105\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_movie_ratings\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[15], line 74\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name, user_movie_ratings)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie not found in the ratings dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Get the user-item matrix and perform collaborative filtering\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m cosine_sim_matrix, movie_titles \u001b[38;5;241m=\u001b[39m \u001b[43mcollaborative_filtering\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_movie_ratings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Find the movie index in the cosine similarity matrix (title to index mapping)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mcollaborative_filtering\u001b[0;34m(user_movie_ratings)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollaborative_filtering\u001b[39m(user_movie_ratings):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# User-Item Matrix (using movie titles instead of movie IDs)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     user_movie_matrix \u001b[38;5;241m=\u001b[39m \u001b[43muser_movie_ratings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muserId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Matrix Factorization using Truncated SVD\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     svd \u001b[38;5;241m=\u001b[39m TruncatedSVD(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)  \u001b[38;5;66;03m# Increase latent factors to 300\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:9339\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[0;34m(self, columns, index, values)\u001b[0m\n\u001b[1;32m   9332\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9333\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   9334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\n\u001b[1;32m   9335\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, columns, index\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default, values\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m   9336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9337\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[0;32m-> 9339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:570\u001b[0m, in \u001b[0;36mpivot\u001b[0;34m(data, columns, index, values)\u001b[0m\n\u001b[1;32m    566\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mindexed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns_listlike\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    572\u001b[0m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m    573\u001b[0m ]\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4615\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[0;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4571\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[1;32m   4572\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4611\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[1;32m   4612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4613\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[0;32m-> 4615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:517\u001b[0m, in \u001b[0;36munstack\u001b[0;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m--> 517\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[1;32m    521\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    522\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:154\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[0;34m(self, index, level, constructor, sort)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[1;32m    147\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    150\u001b[0m         PerformanceWarning,\n\u001b[1;32m    151\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:210\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n","\u001b[0;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"],"ename":"ValueError","evalue":"Index contains duplicate entries, cannot reshape","output_type":"error"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the ratings dataset with IMDb IDs\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\n# Load the movie titles dataset (with IMDb IDs)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Merge ratings with movie details\nuser_movie_ratings = user_movie_ratings.merge(movies_with_tags[['imdbId', 'title', 'director', 'genre']], on='imdbId', how='left')\n\n# Function to fetch genres and director for a movie using TMDb (searching by TMDb ID)\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details including credits\n    movie_details = movie.details(tmdb_id, append_to_response='credits')\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.genres]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.credits['crew']:\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n            \n    return genres, director\n\n# Collaborative Filtering (using matrix factorization)\ndef collaborative_filtering(user_movie_ratings):\n    # Remove duplicates by averaging ratings for user-movie pairs\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title']).rating.mean().reset_index()\n\n    # User-Item Matrix (using movie titles instead of movie IDs)\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Matrix Factorization using Truncated SVD\n    svd = TruncatedSVD(n_components=300)  # Increase latent factors to 300\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n\n    # Cosine Similarity between movies\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    return cosine_sim_matrix, user_movie_matrix.columns\n\ndef recommend_movies(movie_name, user_movie_ratings):\n    # Search for the movie by name\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    # Get the first search result's TMDb ID\n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    # Fetch movie details (genres, director) using TMDb ID\n    genres, director = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}\")\n    \n    # Check if IMDb ID exists in the ratings dataset\n    imdb_id = search_results[0].id\n    if imdb_id not in user_movie_ratings['imdbId'].unique():\n        return \"Movie not found in the ratings dataset.\"\n\n    # Get the user-item matrix and perform collaborative filtering\n    cosine_sim_matrix, movie_titles = collaborative_filtering(user_movie_ratings)\n    \n    # Find the movie index in the cosine similarity matrix (title to index mapping)\n    try:\n        movie_idx = list(movie_titles).index(search_results[0].title)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    # Find similar movies using collaborative filtering\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n\n    # Sort by similarity score\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    # Fetch details for top similar movies\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = movie_titles[movie_idx]\n        \n        # Filter by content (same genres, same director)\n        similar_movie_id = movies_with_tags[movies_with_tags['title'] == similar_movie_title]['imdbId'].values[0]\n        rec_genres, rec_director = get_movie_details_by_tmdb(similar_movie_id)\n        \n        if any(g in genres for g in rec_genres) or rec_director == director:\n            recommended_movies.append(f\"{i+1}. {similar_movie_title} (Genres: {rec_genres})\")\n            if len(recommended_movies) == 3:\n                break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name, user_movie_ratings))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T13:20:33.676124Z","iopub.execute_input":"2024-09-18T13:20:33.677205Z","iopub.status.idle":"2024-09-18T13:20:38.975367Z","shell.execute_reply.started":"2024-09-18T13:20:33.677143Z","shell.execute_reply":"2024-09-18T13:20:38.973763Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: Michael Mann\nMovie not found in the ratings dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the ratings dataset with IMDb IDs\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\n# Load the movie titles dataset (with IMDb IDs)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Merge ratings with movie details\nuser_movie_ratings = user_movie_ratings.merge(movies_with_tags[['imdbId', 'title', 'director', 'genre', 'overview']], on='imdbId', how='left')\n\n# Function to fetch genres and director for a movie using TMDb (searching by TMDb ID)\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details including credits\n    movie_details = movie.details(tmdb_id, append_to_response='credits')\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.genres]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.credits['crew']:\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n            \n    return genres, director\n\n# Collaborative Filtering (using matrix factorization)\ndef collaborative_filtering(user_movie_ratings):\n    # Aggregate duplicate entries by taking the mean rating\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    # User-Item Matrix (using movie titles instead of movie IDs)\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Matrix Factorization using Truncated SVD\n    svd = TruncatedSVD(n_components=300)  # Increase latent factors to 300\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n\n    # Cosine Similarity between movies\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    return cosine_sim_matrix, user_movie_matrix.columns\n\n\ndef recommend_movies_by_overview(movie_name):\n    # Get the movie details\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    # Get the first search result's TMDb ID and details\n    tmdb_id = search_results[0].id\n    movie_details = movie.details(tmdb_id)\n    movie_overview = movie_details.overview\n    \n    # Use TF-IDF to vectorize overviews and compute similarity\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_with_tags['overview'].fillna(''))\n    movie_tfidf = tfidf_vectorizer.transform([movie_overview])\n    \n    # Compute cosine similarity between the input movie and all movies in the dataset\n    cosine_similarities = cosine_similarity(movie_tfidf, tfidf_matrix).flatten()\n    \n    # Get the indices of the top similar movies\n    similar_indices = cosine_similarities.argsort()[-10:][::-1]\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    \n    recommended_movies = []\n    for idx in similar_movies.index:\n        recommended_movies.append(f\"{idx+1}. {similar_movies.iloc[idx]['title']} (Overview: {similar_movies.iloc[idx]['overview'][:100]}...)\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies(movie_name, user_movie_ratings):\n    # Search for the movie by name\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    # Get the first search result's TMDb ID\n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    # Fetch movie details (genres, director) using TMDb ID\n    genres, director = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}\")\n    \n    # Check if IMDb ID exists in the ratings dataset\n    imdb_id = search_results[0].id\n    if imdb_id not in user_movie_ratings['imdbId'].unique():\n        # Recommend based on overview if the movie is not found in the dataset\n        return recommend_movies_by_overview(movie_name)\n\n    # Get the user-item matrix and perform collaborative filtering\n    cosine_sim_matrix, movie_titles = collaborative_filtering(user_movie_ratings)\n    \n    # Find the movie index in the cosine similarity matrix (title to index mapping)\n    try:\n        movie_idx = list(movie_titles).index(search_results[0].title)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    # Find similar movies using collaborative filtering\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n\n    # Sort by similarity score\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    # Fetch details for top similar movies\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = movie_titles[movie_idx]\n        \n        # Filter by content (same genres, same director)\n        similar_movie_id = movies_with_tags[movies_with_tags['title'] == similar_movie_title]['imdbId'].values[0]\n        rec_genres, rec_director = get_movie_details_by_tmdb(similar_movie_id)\n        \n        if any(g in genres for g in rec_genres) or rec_director == director:\n            recommended_movies.append(f\"{i+1}. {similar_movie_title} (Genres: {rec_genres})\")\n            if len(recommended_movies) == 3:\n                break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name, user_movie_ratings))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:25:01.325560Z","iopub.execute_input":"2024-09-18T14:25:01.326014Z","iopub.status.idle":"2024-09-18T14:25:10.132198Z","shell.execute_reply.started":"2024-09-18T14:25:01.325975Z","shell.execute_reply":"2024-09-18T14:25:10.130869Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: Michael Mann\nMovie not found in the ratings dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install tmdbv3api","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:24:38.560434Z","iopub.execute_input":"2024-09-18T14:24:38.560850Z","iopub.status.idle":"2024-09-18T14:24:53.364640Z","shell.execute_reply.started":"2024-09-18T14:24:38.560813Z","shell.execute_reply":"2024-09-18T14:24:53.363279Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tmdbv3api\n  Downloading tmdbv3api-1.9.0-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tmdbv3api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tmdbv3api) (2024.7.4)\nDownloading tmdbv3api-1.9.0-py3-none-any.whl (25 kB)\nInstalling collected packages: tmdbv3api\nSuccessfully installed tmdbv3api-1.9.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the ratings dataset with IMDb IDs\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\n# Load the movie titles dataset (with IMDb IDs)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Merge ratings with movie details\nuser_movie_ratings = user_movie_ratings.merge(movies_with_tags[['imdbId', 'title', 'director', 'genre', 'overview']], on='imdbId', how='left')\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n            \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef collaborative_filtering(user_movie_ratings):\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    svd = TruncatedSVD(n_components=300)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    return cosine_sim_matrix, user_movie_matrix.columns\n\ndef recommend_movies_by_overview(movie_name):\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    movie_details = movie.details(tmdb_id)\n    movie_overview = movie_details.get('overview', '')\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_with_tags['overview'].fillna(''))\n    movie_tfidf = tfidf_vectorizer.transform([movie_overview])\n    \n    cosine_similarities = cosine_similarity(movie_tfidf, tfidf_matrix).flatten()\n    \n    similar_indices = cosine_similarities.argsort()[-10:][::-1]\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    \n    recommended_movies = []\n    for idx in similar_movies.index:\n        recommended_movies.append(f\"{idx+1}. {similar_movies.iloc[idx]['title']} (Overview: {similar_movies.iloc[idx]['overview'][:100]}...)\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies(movie_name, user_movie_ratings):\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    \n    if imdb_id not in user_movie_ratings['imdbId'].astype(str).unique():\n        return recommend_movies_by_overview(movie_name)\n\n    cosine_sim_matrix, movie_titles = collaborative_filtering(user_movie_ratings)\n    \n    try:\n        movie_idx = list(movie_titles).index(search_results[0].title)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = movie_titles[movie_idx]\n        \n        similar_movie_id = movies_with_tags[movies_with_tags['title'] == similar_movie_title]['imdbId'].values[0]\n        rec_genres, rec_director, _ = get_movie_details_by_tmdb(similar_movie_id)\n        \n        if any(g in genres for g in rec_genres) or rec_director == director:\n            recommended_movies.append(f\"{i+1}. {similar_movie_title} (Genres: {rec_genres})\")\n            if len(recommended_movies) == 3:\n                break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name, user_movie_ratings))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:31:21.676701Z","iopub.execute_input":"2024-09-18T14:31:21.677166Z","iopub.status.idle":"2024-09-18T14:31:29.007483Z","shell.execute_reply.started":"2024-09-18T14:31:21.677113Z","shell.execute_reply":"2024-09-18T14:31:29.005657Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_movie_ratings\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[7], line 95\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name, user_movie_ratings)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenres: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Director: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirector\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, IMDb ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m imdb_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m user_movie_ratings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimdbId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecommend_movies_by_overview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m cosine_sim_matrix, movie_titles \u001b[38;5;241m=\u001b[39m collaborative_filtering(user_movie_ratings)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mrecommend_movies_by_overview\u001b[0;34m(movie_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m recommended_movies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m similar_movies\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m---> 77\u001b[0m     recommended_movies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msimilar_movies\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Overview: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilar_movies\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverview\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(recommended_movies) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"],"ename":"IndexError","evalue":"single positional indexer is out-of-bounds","output_type":"error"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the ratings dataset with IMDb IDs\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\n# Load the movie titles dataset (with IMDb IDs)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Merge ratings with movie details\nuser_movie_ratings = user_movie_ratings.merge(movies_with_tags[['imdbId', 'title', 'director', 'genre', 'overview']], on='imdbId', how='left')\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef collaborative_filtering(user_movie_ratings):\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    svd = TruncatedSVD(n_components=300)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    return cosine_sim_matrix, user_movie_matrix.columns\n\ndef recommend_movies_by_overview(movie_name):\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    movie_details = movie.details(tmdb_id)\n    movie_overview = movie_details.get('overview', '')\n    \n    # Check if overview is empty\n    if not movie_overview:\n        return \"No overview found for the movie.\"\n\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_with_tags['overview'].fillna(''))\n    movie_tfidf = tfidf_vectorizer.transform([movie_overview])\n    \n    cosine_similarities = cosine_similarity(movie_tfidf, tfidf_matrix).flatten()\n    \n    # Ensure we have similarities calculated\n    if not cosine_similarities.any():\n        return \"Similarity calculation failed.\"\n\n    # Get indices of the most similar movies\n    similar_indices = cosine_similarities.argsort()[-10:][::-1]\n    \n    # Handle case where similar_indices may exceed DataFrame length\n    similar_indices = [idx for idx in similar_indices if idx < len(movies_with_tags)]\n    \n    if not similar_indices:\n        return \"No similar movies found.\"\n\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    \n    # Debug: print similar_movies to inspect\n    print(\"Similar Movies DataFrame:\")\n    print(similar_movies)\n    \n    recommended_movies = []\n    for idx in similar_indices:\n        if idx >= len(similar_movies) or idx < 0:\n            continue  # Skip indices that are out-of-bounds\n        movie_title = similar_movies.iloc[idx]['title']\n        recommended_movies.append(f\"{movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return \"\\n\".join(recommended_movies)\n\n\n\ndef recommend_movies(movie_name, user_movie_ratings):\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    \n    if imdb_id not in user_movie_ratings['imdbId'].astype(str).unique():\n        return recommend_movies_by_overview(movie_name)\n\n    cosine_sim_matrix, movie_titles = collaborative_filtering(user_movie_ratings)\n    \n    try:\n        movie_idx = list(movie_titles).index(search_results[0].title)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = movie_titles[movie_idx]\n        \n        similar_movie_id = movies_with_tags[movies_with_tags['title'] == similar_movie_title]['imdbId'].values[0]\n        rec_genres, rec_director, _ = get_movie_details_by_tmdb(similar_movie_id)\n        \n        if any(g in genres for g in rec_genres) or rec_director == director:\n            recommended_movies.append(f\"{i+1}. {similar_movie_title} (Genres: {rec_genres})\")\n            if len(recommended_movies) == 3:\n                break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name, user_movie_ratings))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:37:13.866803Z","iopub.execute_input":"2024-09-18T14:37:13.867286Z","iopub.status.idle":"2024-09-18T14:37:19.877497Z","shell.execute_reply.started":"2024-09-18T14:37:13.867247Z","shell.execute_reply":"2024-09-18T14:37:19.876253Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\nSimilar Movies DataFrame:\n      index            title  film_rate  \\\n114     114             Heat        8.3   \n7834   7834       The Reader        7.6   \n8468   8468            Hanna        6.7   \n6175   6175       Collateral        7.5   \n4724   4724  City by the Sea        6.1   \n9395   9395      St. Vincent        7.2   \n1531   1531        Mousehunt        6.5   \n4538   4538         Time Out        7.3   \n6080   6080      The Hitcher        5.6   \n6081   6081      The Hitcher        5.6   \n\n                                               overview  \\\n114   ['A group of high-end professional thieves sta...   \n7834  ['Post-WWII Germany: Nearly a decade after his...   \n8468  ['A sixteen-year-old girl who was raised by he...   \n6175  ['A cab driver finds himself the hostage of an...   \n4724  ['Vincent Lamarca, whose father was executed f...   \n9395  ['A young boy whose parents have just divorced...   \n1531  ['Two stumblebum inheritors are determined to ...   \n4538  ['An unemployed man finds his life sinking mor...   \n6080  ['A couple from college get caught in a danger...   \n6081  ['A couple from college get caught in a danger...   \n\n                                              cover_url  \\\n114   https://m.media-amazon.com/images/M/MV5BYjZjNT...   \n7834  https://m.media-amazon.com/images/M/MV5BMTM0ND...   \n8468  https://m.media-amazon.com/images/M/MV5BNTAzMT...   \n6175  https://m.media-amazon.com/images/M/MV5BMjE3Nj...   \n4724  https://m.media-amazon.com/images/M/MV5BMTIwMj...   \n9395  https://m.media-amazon.com/images/M/MV5BMTk5Nz...   \n1531  https://m.media-amazon.com/images/M/MV5BMzE0NT...   \n4538  https://m.media-amazon.com/images/M/MV5BZTgyOW...   \n6080  https://m.media-amazon.com/images/M/MV5BMTIwOT...   \n6081  https://m.media-amazon.com/images/M/MV5BMTIwOT...   \n\n                                                 cast  \\\n114             Al Pacino, Robert De Niro, Val Kilmer   \n7834        Ralph Fiennes, Jeanette Hain, David Kross   \n8468           Saoirse Ronan, Eric Bana, Vicky Krieps   \n6175       Tom Cruise, Jamie Foxx, Jada Pinkett Smith   \n4724  Robert De Niro, Frances McDormand, James Franco   \n9395       Bill Murray, Melissa McCarthy, Naomi Watts   \n1531              Nathan Lane, Lee Evans, Vicki Lewis   \n4538   Aurlien Recoing, Karin Viard, Serge Livrozet   \n6080         Sean Bean, Sophia Bush, Zachary Knighton   \n6081         Sean Bean, Sophia Bush, Zachary Knighton   \n\n                                                reviews   imdbId  \\\n114   I have very little interest in most action fil...   113277   \n7834  Kate Winslet is just outstanding in this very ...   976051   \n8468  I really wanted to watch this film but I didn'...   993842   \n6175  I definitely enjoyed this movie. It keeps you ...   369339   \n4724  This is a very good movie. I do not understand...   269095   \n9395  This film is recommended.The old curmudgeon ha...  2170593   \n1531  'Mousehunt' was one of my favorites movies as ...   119715   \n4538  Vincent has recently been fired from his job a...   279065   \n6080  To be honest, I wasn't expecting much from thi...    91209   \n6081  To be honest, I wasn't expecting much from thi...   455960   \n\n                 director                               genre  rating  tag  \n114          Michael Mann                Action, Crime, Drama    4.15  NaN  \n7834       Stephen Daldry             Drama, Mystery, Romance    3.80  NaN  \n8468           Joe Wright  Action, Adventure, Drama, Thriller    3.35  NaN  \n6175         Michael Mann      Action, Crime, Drama, Thriller    3.75  NaN  \n4724  Michael Caton-Jones     Crime, Drama, Mystery, Thriller    3.05  NaN  \n9395       Theodore Melfi                       Comedy, Drama    3.60  NaN  \n1531       Gore Verbinski                              Comedy    3.25  NaN  \n4538       Laurent Cantet                               Drama    3.65  NaN  \n6080        Robert Harmon           Action, Mystery, Thriller    2.80  NaN  \n6081          Dave Meyers                     Crime, Thriller    2.80  NaN  \nNo recommendations found.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the ratings dataset with IMDb IDs\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\n# Load the movie titles dataset (with IMDb IDs)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Normalize the movie titles by removing year and converting to lowercase\ndef normalize_title(title):\n    if isinstance(title, str):\n        title = title.lower().strip()\n        title = re.sub(r'\\(\\d{4}\\)', '', title).strip()  # Remove year in parentheses\n        return title\n    return ''\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef collaborative_filtering(user_movie_ratings):\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    svd = TruncatedSVD(n_components=300)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    return cosine_sim_matrix, user_movie_matrix.columns\n\ndef recommend_movies_by_overview(movie_name):\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    movie_details = movie.details(tmdb_id)\n    movie_overview = movie_details.get('overview', '')\n    \n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_with_tags['overview'].fillna(''))\n    movie_tfidf = tfidf_vectorizer.transform([movie_overview])\n    \n    cosine_similarities = cosine_similarity(movie_tfidf, tfidf_matrix).flatten()\n    \n    # Get indices of the most similar movies\n    similar_indices = cosine_similarities.argsort()[-10:][::-1]\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    \n    recommended_movies = []\n    for idx in similar_indices:\n        if idx >= len(similar_movies) or idx < 0:\n            continue  # Skip indices that are out-of-bounds\n        movie_title = similar_movies.iloc[idx]['title']\n        recommended_movies.append(f\"{movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return \"\\n\".join(recommended_movies)\n\ndef recommend_movies(movie_name, user_movie_ratings):\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    movie_title = search_results[0].title\n    normalized_title = normalize_title(movie_title)\n    print(f\"Found movie: {movie_title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    \n    if imdb_id not in user_movie_ratings['imdbId'].astype(str).unique():\n        return recommend_movies_by_overview(movie_name)\n\n    cosine_sim_matrix, movie_titles = collaborative_filtering(user_movie_ratings)\n    \n    normalized_movie_titles = [normalize_title(title) for title in movie_titles]\n    \n    try:\n        movie_idx = normalized_movie_titles.index(normalized_title)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = movie_titles[movie_idx]\n        \n        similar_movie_id = movies_with_tags[movies_with_tags['title'] == similar_movie_title]['imdbId'].values[0]\n        rec_genres, rec_director, _ = get_movie_details_by_tmdb(similar_movie_id)\n        \n        if any(g in genres for g in rec_genres) or rec_director == director:\n            recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n            if len(recommended_movies) == 3:\n                break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name, user_movie_ratings))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:40:53.074653Z","iopub.execute_input":"2024-09-18T14:40:53.075071Z","iopub.status.idle":"2024-09-18T14:40:58.828278Z","shell.execute_reply.started":"2024-09-18T14:40:53.075032Z","shell.execute_reply":"2024-09-18T14:40:58.827118Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\nNo recommendations found.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef recommend_movies(movie_name):\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        # Collaborative filtering\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            # Content-based filtering\n            return recommend_movies_by_content(genres, director, movie_name)\n    else:\n        # Fallback content-based filtering if IMDb ID is not available\n        return recommend_movies_by_content(genres, director, movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    # Vectorize the movie overviews\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(movies_with_tags['overview'].fillna(''))\n    \n    # Create a query vector for the input movie\n    movie_overview = movie.search(movie_name)[0].overview if movie.search(movie_name) else ''\n    query_vector = tfidf_vectorizer.transform([movie_overview])\n    \n    # Compute cosine similarities\n    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n    \n    # Get indices of the most similar movies\n    similar_indices = cosine_similarities.argsort()[-10:][::-1]\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    \n    recommended_movies = []\n    for idx in similar_indices:\n        movie_title = similar_movies.iloc[idx]['title']\n        movie_genres = similar_movies.iloc[idx]['genre'].split(',') if not pd.isna(similar_movies.iloc[idx]['genre']) else []\n        movie_director = similar_movies.iloc[idx]['director'] if not pd.isna(similar_movies.iloc[idx]['director']) else None\n        \n        # Check if the movie has at least one matching genre or matching director\n        if (genres and any(g in movie_genres for g in genres)) or director == movie_director:\n            recommended_movies.append(movie_title)\n            if len(recommended_movies) == 3:\n                break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:43:36.701819Z","iopub.execute_input":"2024-09-18T14:43:36.702353Z","iopub.status.idle":"2024-09-18T14:43:42.695232Z","shell.execute_reply.started":"2024-09-18T14:43:36.702285Z","shell.execute_reply":"2024-09-18T14:43:42.693334Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    137\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[17], line 69\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m recommend_movies_collaborative(imdb_id)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# Content-based filtering\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecommend_movies_by_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Fallback content-based filtering if IMDb ID is not available\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recommend_movies_by_content(genres, director, movie_name)\n","Cell \u001b[0;32mIn[17], line 124\u001b[0m, in \u001b[0;36mrecommend_movies_by_content\u001b[0;34m(genres, director, movie_name)\u001b[0m\n\u001b[1;32m    122\u001b[0m recommended_movies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m similar_indices:\n\u001b[0;32m--> 124\u001b[0m     movie_title \u001b[38;5;241m=\u001b[39m \u001b[43msimilar_movies\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    125\u001b[0m     movie_genres \u001b[38;5;241m=\u001b[39m similar_movies\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(similar_movies\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    126\u001b[0m     movie_director \u001b[38;5;241m=\u001b[39m similar_movies\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(similar_movies\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirector\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"],"ename":"IndexError","evalue":"single positional indexer is out-of-bounds","output_type":"error"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\n\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\n# Add normalized titles to the dataset\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    # Here, we should map the normalized title back to the original titles.\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef recommend_movies(movie_name):\n    # Normalize the input movie name\n    normalized_movie_name = normalize_title(movie_name)\n    \n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        # Collaborative filtering\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            # Content-based filtering\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        # Fallback content-based filtering if IMDb ID is not available\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    # Normalize the movie name for matching\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Ensure director is a string\n    if director is None:\n        director = ''\n    \n    # Filter movies that match genres and director\n    similar_movies = movies_with_tags[\n        movies_with_tags['genre'].str.contains('|'.join(genres), na=False) &\n        movies_with_tags['director'].str.contains(director, na=False)\n    ]\n    \n    # Check if there are any movies to recommend\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    # Drop the movie already entered from the recommendations\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    # Check if there are any movies left to recommend\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    # Select top recommendations based on some criterion\n    recommended_movies = similar_movies.head(3)['title'].tolist()\n\n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return recommended_movies\n\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:51:20.532037Z","iopub.execute_input":"2024-09-18T14:51:20.532500Z","iopub.status.idle":"2024-09-18T14:51:25.880009Z","shell.execute_reply.started":"2024-09-18T14:51:20.532458Z","shell.execute_reply":"2024-09-18T14:51:25.878831Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  interstellar\n"},{"name":"stdout","text":"Found movie: Interstellar (TMDb ID: 157336)\nGenres: ['Adventure', 'Drama', 'Science Fiction'], Director: None, IMDb ID: tt0816692\n['Before and After', \"The Young Poisoner's Handbook\", 'The Boys of St. Vincent']\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\n# Add normalized titles to the dataset\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    # Map the normalized title back to the original titles.\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef recommend_movies(movie_name):\n    # Normalize the input movie name\n    normalized_movie_name = normalize_title(movie_name)\n    \n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        # Collaborative filtering\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            # Content-based filtering\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        # Fallback content-based filtering if IMDb ID is not available\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    # Normalize the movie name for matching\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Ensure director is a string\n    if director is None:\n        director = ''\n    \n    # Convert genres to a string for TF-IDF\n    genres_str = ' '.join(genres)\n    \n    # Combine overview, genres, and director for content-based filtering\n    movies_with_tags['combined_features'] = (\n        movies_with_tags['overview'].fillna('') + ' ' +\n        movies_with_tags['genre'].fillna('') + ' ' +\n        movies_with_tags['director'].fillna('')\n    )\n\n    # TF-IDF Vectorization\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(movies_with_tags['combined_features'])\n    \n    # TF-IDF Vectorization for the input movie\n    input_movie_vector = vectorizer.transform([movie_name])\n    \n    # Compute cosine similarity between the input movie and dataset\n    cosine_similarities = cosine_similarity(input_movie_vector, tfidf_matrix).flatten()\n    \n    # Get indices of the top 10 most similar movies\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    \n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n    \n    # Check if there are any movies left to recommend\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    recommended_movies = similar_movies['title'].tolist()\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return recommended_movies\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:53:13.470120Z","iopub.execute_input":"2024-09-18T14:53:13.470982Z","iopub.status.idle":"2024-09-18T14:53:22.184501Z","shell.execute_reply.started":"2024-09-18T14:53:13.470934Z","shell.execute_reply":"2024-09-18T14:53:22.183336Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  me before you\n"},{"name":"stdout","text":"Found movie: Me Before You (TMDb ID: 296096)\nGenres: ['Drama', 'Romance'], Director: None, IMDb ID: tt2674426\n['Hardcore Henry', 'Austin Powers in Goldmember', 'Defending Your Life', 'Poker Night', \"'night, Mother\", \"What's Up, Tiger Lily?\", 'Dog Soldiers', \"It's My Party\", 'Nymphomaniac: Vol. II', 'The Dilemma']\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\n# Add normalized titles to the dataset\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    # Map the normalized title back to the original titles.\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef recommend_movies(movie_name):\n    # Normalize the input movie name\n    normalized_movie_name = normalize_title(movie_name)\n    \n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        # Collaborative filtering\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            # Content-based filtering\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        # Fallback content-based filtering if IMDb ID is not available\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    # Normalize the movie name for matching\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Ensure director is a string\n    if director is None:\n        director = ''\n    \n    # Combine overview, genres, and director for content-based filtering\n    movies_with_tags['combined_features'] = (\n        movies_with_tags['overview'].fillna('') + ' ' +\n        movies_with_tags['genre'].fillna('') + ' ' +\n        movies_with_tags['director'].fillna('')\n    )\n    \n    # TF-IDF Vectorization for movie features\n    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    tfidf_matrix = vectorizer.fit_transform(movies_with_tags['combined_features'])\n    \n    # TF-IDF Vectorization for the input movie\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = vectorizer.transform([input_movie_features])\n    \n    # Compute cosine similarity between the input movie and dataset\n    cosine_similarities = cosine_similarity(input_movie_vector, tfidf_matrix).flatten()\n    \n    # Get indices of the top 10 most similar movies\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    \n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n    \n    # Check if there are any movies left to recommend\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    recommended_movies = similar_movies['title'].tolist()\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return recommended_movies\n\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:55:14.636727Z","iopub.execute_input":"2024-09-18T14:55:14.637196Z","iopub.status.idle":"2024-09-18T14:55:27.589854Z","shell.execute_reply.started":"2024-09-18T14:55:14.637155Z","shell.execute_reply":"2024-09-18T14:55:27.588638Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\n['Digging Up the Marrow', 'Pusher II', 'Batman: The Killing Joke', 'Swelter', 'A Chorus Line', 'Mojave', 'White Sands', 'Batman: The Dark Knight Returns, Part 1', 'One Tough Cop', 'Undisputed 2: Last Man Standing']\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\n# Add normalized titles to the dataset\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    # Map the normalized title back to the original titles.\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef recommend_movies(movie_name):\n    # Normalize the input movie name\n    normalized_movie_name = normalize_title(movie_name)\n    \n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        # Collaborative filtering\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            # Content-based filtering\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        # Fallback content-based filtering if IMDb ID is not available\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    # Normalize the movie name for matching\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Ensure director is a string\n    if director is None:\n        director = ''\n    \n    # Combine overview, genres, and director for content-based filtering\n    movies_with_tags['combined_features'] = (\n        movies_with_tags['overview'].fillna('') + ' ' +\n        movies_with_tags['genre'].fillna('') + ' ' +\n        movies_with_tags['director'].fillna('')\n    )\n    \n    # TF-IDF Vectorization for movie features\n    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    tfidf_matrix = vectorizer.fit_transform(movies_with_tags['combined_features'])\n    \n    # TF-IDF Vectorization for the input movie\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = vectorizer.transform([input_movie_features])\n    \n    # Compute cosine similarity between the input movie and dataset\n    cosine_similarities = cosine_similarity(input_movie_vector, tfidf_matrix).flatten()\n    \n    # Get indices of the top 10 most similar movies\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    \n    # Debugging output\n    print(\"Cosine similarities:\", cosine_similarities)\n    print(\"Similar indices:\", similar_indices)\n    \n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n    \n    # Check if there are any movies left to recommend\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    recommended_movies = similar_movies['title'].tolist()\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return recommended_movies\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:58:02.906745Z","iopub.execute_input":"2024-09-18T14:58:02.907292Z","iopub.status.idle":"2024-09-18T14:58:15.103942Z","shell.execute_reply.started":"2024-09-18T14:58:02.907248Z","shell.execute_reply":"2024-09-18T14:58:15.102742Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\nCosine similarities: [0.01728618 0.         0.01614282 ... 0.00125946 0.00115076 0.00135964]\nSimilar indices: [ 9545  6874 10181 10020  5153 10002  3508  8905  2213  8146]\n['Digging Up the Marrow', 'Pusher II', 'Batman: The Killing Joke', 'Swelter', 'A Chorus Line', 'Mojave', 'White Sands', 'Batman: The Dark Knight Returns, Part 1', 'One Tough Cop', 'Undisputed 2: Last Man Standing']\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# BERT tokenizer and model initialization\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ndef bert_embedding(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\n# Extract features\nmovies_with_tags['overview_bert'] = movies_with_tags['overview'].apply(lambda x: bert_embedding(x))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create TF-IDF for genres and director\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\ngenre_director_tfidf = tfidf_vectorizer.fit_transform(movies_with_tags['genre'] + ' ' + movies_with_tags['director'])\n\n# Combine BERT embeddings with TF-IDF vectors\ncombined_features = torch.cat((torch.tensor(movies_with_tags['overview_bert'].tolist()), \n                                torch.tensor(genre_director_tfidf.toarray())), dim=1).numpy()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef recommend_movies_by_content(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    input_movie_features = movies_with_tags[movies_with_tags['normalized_title'] == normalized_movie_name]\n    \n    if input_movie_features.empty:\n        return \"No recommendations found.\"\n\n    input_movie_vector = torch.tensor(bert_embedding(input_movie_features.iloc[0]['overview'])).unsqueeze(0).numpy()\n    combined_features_matrix = combined_features\n\n    cosine_similarities = cosine_similarity(input_movie_vector, combined_features_matrix).flatten()\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    return similar_movies['title'].tolist()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            collaborative_recommendations = recommend_movies_collaborative(imdb_id)\n            content_recommendations = recommend_movies_by_content(normalized_movie_name)\n            \n            # Combine recommendations (you can use weighted combination)\n            combined_recommendations = set(collaborative_recommendations + content_recommendations)\n            return list(combined_recommendations)\n        else:\n            return recommend_movies_by_content(normalized_movie_name)\n    else:\n        return recommend_movies_by_content(normalized_movie_name)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# new chapter","metadata":{}},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom transformers import BertTokenizer, BertModel\nimport torch\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\n# Add normalized titles to the dataset\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    # Map the normalized title back to the original titles.\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef bert_embedding(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    # Normalize the movie name for matching\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Ensure director is a string\n    if director is None:\n        director = ''\n    \n    # Preprocess overviews and create BERT embeddings\n    movies_with_tags['overview_bert'] = movies_with_tags['overview'].fillna('').apply(lambda x: bert_embedding(x))\n    \n    # Combine overview embeddings with TF-IDF for genres and director\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    genre_director_tfidf = tfidf_vectorizer.fit_transform(movies_with_tags['genre'].fillna('') + ' ' + movies_with_tags['director'].fillna(''))\n    \n    # Combine BERT embeddings and TF-IDF vectors\n    combined_features = torch.cat((torch.tensor(movies_with_tags['overview_bert'].tolist()), \n                                   torch.tensor(genre_director_tfidf.toarray())), dim=1).numpy()\n    \n    # Vectorize the input movie's features\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = torch.cat((torch.tensor(bert_embedding('')), \n                                    tfidf_vectorizer.transform([input_movie_features]).toarray()), dim=1).numpy()\n    \n    # Compute cosine similarity between the input movie and dataset\n    cosine_similarities = cosine_similarity(input_movie_vector, combined_features).flatten()\n    \n    # Get indices of the top 10 most similar movies\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    \n    # Debugging output\n    print(\"Cosine similarities:\", cosine_similarities)\n    print(\"Similar indices:\", similar_indices)\n    \n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n    \n    # Check if there are any movies left to recommend\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    return similar_movies['title'].tolist()\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies(movie_name):\n    # Normalize the input movie name\n    normalized_movie_name = normalize_title(movie_name)\n    \n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        # Collaborative filtering\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            collaborative_recommendations = recommend_movies_collaborative(imdb_id)\n            content_recommendations = recommend_movies_by_content(genres, director, normalized_movie_name)\n            \n            # Combine recommendations (you can use weighted combination)\n            combined_recommendations = set(collaborative_recommendations.split('\\n') + content_recommendations)\n            return list(combined_recommendations)\n        else:\n            # Content-based filtering\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        # Fallback content-based filtering if IMDb ID is not available\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T15:03:54.672237Z","iopub.execute_input":"2024-09-18T15:03:54.672801Z","iopub.status.idle":"2024-09-18T16:38:29.649748Z","shell.execute_reply.started":"2024-09-18T15:03:54.672752Z","shell.execute_reply":"2024-09-18T16:38:29.648172Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05585d4631274633a5a74afcd3156da8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73853490589f4e438b1fae9d7f342e19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3712e20f789b4575bd1f5c66547b85da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df881eee5594b6099c13765018dfe49"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9c3602a9bf4b9084b2f864159757a0"}},"metadata":{}},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4235241570.py:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  combined_features = torch.cat((torch.tensor(movies_with_tags['overview_bert'].tolist()),\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 180\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[35], line 173\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(combined_recommendations)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;66;03m# Content-based filtering\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecommend_movies_by_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_movie_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# Fallback content-based filtering if IMDb ID is not available\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recommend_movies_by_content(genres, director, normalized_movie_name)\n","Cell \u001b[0;32mIn[35], line 93\u001b[0m, in \u001b[0;36mrecommend_movies_by_content\u001b[0;34m(genres, director, movie_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Vectorize the input movie's features\u001b[39;00m\n\u001b[1;32m     92\u001b[0m input_movie_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenre: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(genres)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirector: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirector\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 93\u001b[0m input_movie_vector \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_movie_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Compute cosine similarity between the input movie and dataset\u001b[39;00m\n\u001b[1;32m     97\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(input_movie_vector, combined_features)\u001b[38;5;241m.\u001b[39mflatten()\n","\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got numpy.ndarray"],"ename":"TypeError","evalue":"expected Tensor as element 1 in argument 0, but got numpy.ndarray","output_type":"error"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom transformers import BertTokenizer, BertModel\nimport torch\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\n# Add normalized titles to the dataset\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    # Map the normalized title back to the original titles.\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    # Fetch movie details\n    movie_details = movie.details(tmdb_id)\n    \n    # Fetch external IDs\n    external_ids = movie.external_ids(tmdb_id)\n    \n    # Extract genres\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    \n    # Extract the director from the credits\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    \n    # Retrieve IMDb ID\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    \n    return genres, director, imdb_id\n\ndef bert_embedding(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    # Normalize the movie name for matching\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Ensure director is a string\n    if director is None:\n        director = ''\n    \n    # Preprocess overviews and create BERT embeddings\n    movies_with_tags['overview_bert'] = movies_with_tags['overview'].fillna('').apply(lambda x: bert_embedding(x))\n    \n    # Combine overview embeddings with TF-IDF for genres and director\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    genre_director_tfidf = tfidf_vectorizer.fit_transform(movies_with_tags['genre'].fillna('') + ' ' + movies_with_tags['director'].fillna(''))\n    \n    # Convert BERT embeddings to NumPy arrays\n    overview_bert_array = np.array(movies_with_tags['overview_bert'].tolist())\n    \n    # Combine BERT embeddings and TF-IDF vectors\n    combined_features = np.hstack((overview_bert_array, genre_director_tfidf.toarray()))\n    \n    # Vectorize the input movie's features\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = np.hstack((bert_embedding(''), tfidf_vectorizer.transform([input_movie_features]).toarray()))\n    \n    # Compute cosine similarity between the input movie and dataset\n    cosine_similarities = cosine_similarity(input_movie_vector.reshape(1, -1), combined_features).flatten()\n    \n    # Get indices of the top 10 most similar movies\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    \n    # Debugging output\n    print(\"Cosine similarities:\", cosine_similarities)\n    print(\"Similar indices:\", similar_indices)\n    \n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n    \n    # Check if there are any movies left to recommend\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    return similar_movies['title'].tolist()\n\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies(movie_name):\n    # Normalize the input movie name\n    normalized_movie_name = normalize_title(movie_name)\n    \n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        # Collaborative filtering\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            collaborative_recommendations = recommend_movies_collaborative(imdb_id)\n            content_recommendations = recommend_movies_by_content(genres, director, normalized_movie_name)\n            \n            # Combine recommendations (you can use weighted combination)\n            combined_recommendations = set(collaborative_recommendations.split('\\n') + content_recommendations)\n            return list(combined_recommendations)\n        else:\n            # Content-based filtering\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        # Fallback content-based filtering if IMDb ID is not available\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T16:44:35.039361Z","iopub.execute_input":"2024-09-18T16:44:35.039888Z","iopub.status.idle":"2024-09-18T18:18:49.483980Z","shell.execute_reply.started":"2024-09-18T16:44:35.039793Z","shell.execute_reply":"2024-09-18T18:18:49.482271Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nGenres: ['Action', 'Crime', 'Drama'], Director: None, IMDb ID: tt0113277\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 182\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    181\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[36], line 175\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(combined_recommendations)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;66;03m# Content-based filtering\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecommend_movies_by_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_movie_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Fallback content-based filtering if IMDb ID is not available\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recommend_movies_by_content(genres, director, normalized_movie_name)\n","Cell \u001b[0;32mIn[36], line 95\u001b[0m, in \u001b[0;36mrecommend_movies_by_content\u001b[0;34m(genres, director, movie_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Vectorize the input movie's features\u001b[39;00m\n\u001b[1;32m     94\u001b[0m input_movie_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenre: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(genres)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirector: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirector\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 95\u001b[0m input_movie_vector \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_movie_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Compute cosine similarity between the input movie and dataset\u001b[39;00m\n\u001b[1;32m     98\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(input_movie_vector\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), combined_features)\u001b[38;5;241m.\u001b[39mflatten()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:357\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arrs \u001b[38;5;129;01mand\u001b[39;00m arrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n","\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"],"ename":"ValueError","evalue":"all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)","output_type":"error"}]},{"cell_type":"code","source":"movie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie.external_ids(tmdb_id)\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    return genres, director, imdb_id\n\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    if director is None:\n        director = ''\n    \n    # Extract BERT embeddings for the overview\n    movie_overviews = movies_with_tags['overview'].fillna('')\n    overview_embeddings = get_bert_embeddings(movie_overviews.tolist())\n\n    # TF-IDF Vectorization for genres and director\n    tfidf_vectorizer_genre = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    genre_tfidf_matrix = tfidf_vectorizer_genre.fit_transform(movies_with_tags['genre'].fillna(''))\n    \n    tfidf_vectorizer_director = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    director_tfidf_matrix = tfidf_vectorizer_director.fit_transform(movies_with_tags['director'].fillna(''))\n\n    input_movie_features = {\n        'genre': ' '.join([f\"genre: {', '.join(genres)}\"]),\n        'director': f\"director: {director}\"\n    }\n\n    # TF-IDF Vectorization for the input movie genres and director\n    input_genre_vector = tfidf_vectorizer_genre.transform([input_movie_features['genre']])\n    input_director_vector = tfidf_vectorizer_director.transform([input_movie_features['director']])\n    \n    # Compute cosine similarity for genre and director\n    genre_cosine_similarities = cosine_similarity(input_genre_vector, genre_tfidf_matrix).flatten()\n    director_cosine_similarities = cosine_similarity(input_director_vector, director_tfidf_matrix).flatten()\n    \n    # Compute final similarity scores\n    # Normalize to avoid bias\n    genre_sim = (genre_cosine_similarities - genre_cosine_similarities.min()) / (genre_cosine_similarities.max() - genre_cosine_similarities.min())\n    director_sim = (director_cosine_similarities - director_cosine_similarities.min()) / (director_cosine_similarities.max() - director_cosine_similarities.min())\n\n    combined_similarities = genre_sim * 0.5 + director_sim * 0.5\n\n    # Get indices of the top 10 most similar movies\n    similar_indices = combined_similarities.argsort()[-11:-1][::-1]\n\n    # Debugging output\n    print(\"Cosine similarities (genre):\", genre_cosine_similarities)\n    print(\"Cosine similarities (director):\", director_cosine_similarities)\n    print(\"Combined similarities:\", combined_similarities)\n    print(\"Similar indices:\", similar_indices)\n\n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    recommended_movies = similar_movies['title'].tolist()\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return recommended_movies\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie.external_ids(tmdb_id)\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    return genres, director, imdb_id\n\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_filtered = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings_filtered.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings_filtered = user_movie_ratings_filtered.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings_filtered.pivot(index='userId', columns='title', values='rating').fillna(0)\n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    if director is None:\n        director = ''\n    \n    # Extract BERT embeddings for the overview\n    movie_overviews = movies_with_tags['overview'].fillna('')\n    overview_embeddings = get_bert_embeddings(movie_overviews.tolist())\n\n    # TF-IDF Vectorization for genres and director\n    tfidf_vectorizer_genre = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    genre_tfidf_matrix = tfidf_vectorizer_genre.fit_transform(movies_with_tags['genre'].fillna(''))\n    \n    tfidf_vectorizer_director = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    director_tfidf_matrix = tfidf_vectorizer_director.fit_transform(movies_with_tags['director'].fillna(''))\n\n    input_movie_features = {\n        'genre': ' '.join([f\"genre: {', '.join(genres)}\"]),\n        'director': f\"director: {director}\"\n    }\n\n    # TF-IDF Vectorization for the input movie genres and director\n    input_genre_vector = tfidf_vectorizer_genre.transform([input_movie_features['genre']])\n    input_director_vector = tfidf_vectorizer_director.transform([input_movie_features['director']])\n    \n    # Compute cosine similarity for genre and director\n    genre_cosine_similarities = cosine_similarity(input_genre_vector, genre_tfidf_matrix).flatten()\n    director_cosine_similarities = cosine_similarity(input_director_vector, director_tfidf_matrix).flatten()\n    \n    # Compute final similarity scores\n    # Normalize to avoid bias\n    genre_sim = (genre_cosine_similarities - genre_cosine_similarities.min()) / (genre_cosine_similarities.max() - genre_cosine_similarities.min())\n    director_sim = (director_cosine_similarities - director_cosine_similarities.min()) / (director_cosine_similarities.max() - director_cosine_similarities.min())\n\n    combined_similarities = genre_sim * 0.5 + director_sim * 0.5\n\n    # Get indices of the top 10 most similar movies\n    similar_indices = combined_similarities.argsort()[-11:-1][::-1]\n\n    # Debugging output\n    print(\"Cosine similarities (genre):\", genre_cosine_similarities)\n    print(\"Cosine similarities (director):\", director_cosine_similarities)\n    print(\"Combined similarities:\", combined_similarities)\n    print(\"Similar indices:\", similar_indices)\n\n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    recommended_movies = similar_movies['title'].tolist()\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return recommended_movies\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Normalize movie titles by removing years and converting to lowercase\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\n# Add normalized titles to the dataset\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie.external_ids(tmdb_id)\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    return genres, director, imdb_id\n\ndef get_bert_embeddings(texts, batch_size=32):\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i + batch_size]\n        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n        with torch.no_grad():\n            outputs = bert_model(**inputs)\n        batch_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n        embeddings.append(batch_embeddings)\n    return np.vstack(embeddings)\n\ndef compute_tfidf_features(texts, ngram_range=(1, 2)):\n    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=ngram_range)\n    return vectorizer.fit_transform(texts)\n\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_filtered = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    \n    if user_movie_ratings_filtered.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    \n    user_movie_ratings_filtered = user_movie_ratings_filtered.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    \n    user_movie_matrix = user_movie_ratings_filtered.pivot(index='userId', columns='title', values='rating').fillna(0)\n    \n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    \n    if director is None:\n        director = ''\n    \n    # Generate BERT embeddings for overviews\n    movie_overviews = movies_with_tags['overview'].fillna('')\n    overview_embeddings = get_bert_embeddings(movie_overviews.tolist())\n    \n    # Compute TF-IDF features for genres and directors\n    genre_tfidf_matrix = compute_tfidf_features(movies_with_tags['genre'].fillna(''))\n    director_tfidf_matrix = compute_tfidf_features(movies_with_tags['director'].fillna(''))\n    \n    # Combine BERT embeddings and TF-IDF vectors\n    combined_features = np.hstack((overview_embeddings, genre_tfidf_matrix.toarray(), director_tfidf_matrix.toarray()))\n    \n    # Compute BERT embedding for the input movie\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = get_bert_embeddings([input_movie_features]).flatten()\n    \n    # Compute cosine similarity\n    cosine_similarities = cosine_similarity(input_movie_vector.reshape(1, -1), combined_features).flatten()\n    \n    # Get indices of the top 10 most similar movies\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    \n    print(\"Cosine similarities:\", cosine_similarities)\n    print(\"Similar indices:\", similar_indices)\n    \n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n    \n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    recommended_movies = similar_movies['title'].tolist()\n    \n    return recommended_movies if recommended_movies else \"No recommendations found.\"\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# more features","metadata":{}},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie_details.get('external_ids', {})\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    cast = movie_details.get('credits', {}).get('cast', [])\n    crew = movie_details.get('credits', {}).get('crew', [])       \n    imdb_id = external_ids.get('imdb_id') if external_ids else None\n    synopsis = movie_details.get('overview', '')\n    ratings = movie_details.get('vote_average', 0)\n    reviews = movie_details.get('reviews', {}).get('results', [])\n    actors = [a['name'] for a in movie_details.get('credits', {}).get('cast', [])[:5]]  # Top 5 actors\n    keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])]\n\n    return genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords\n\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    print(f\"Synopsis: {synopsis}\")\n    print(f\"Ratings: {ratings}\")\n    print(f\"Reviews: {reviews}\")\n    print(f\"Actors: {actors}\")\n    print(f\"Keywords: {keywords}\")\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    if director is None:\n        director = ''\n    \n    # Extract BERT embeddings for the synopsis\n    movie_synopses = movies_with_tags['overview'].fillna('')\n    synopsis_embeddings = get_bert_embeddings(movie_synopses.tolist())\n\n    # TF-IDF Vectorization for genres, director, actors, and keywords\n    tfidf_vectorizer_genre = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    genre_tfidf_matrix = tfidf_vectorizer_genre.fit_transform(movies_with_tags['genre'].fillna(''))\n    \n    tfidf_vectorizer_director = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    director_tfidf_matrix = tfidf_vectorizer_director.fit_transform(movies_with_tags['director'].fillna(''))\n    \n    tfidf_vectorizer_actor = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    actor_tfidf_matrix = tfidf_vectorizer_actor.fit_transform(movies_with_tags['actors'].fillna(''))\n    \n    tfidf_vectorizer_keywords = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    keywords_tfidf_matrix = tfidf_vectorizer_keywords.fit_transform(movies_with_tags['keywords'].fillna(''))\n\n    input_movie_features = {\n        'genre': ' '.join([f\"genre: {', '.join(genres)}\"]),\n        'director': f\"director: {director}\",\n        'actors': ' '.join([f\"actor: {', '.join(actors)}\"]),\n        'keywords': ' '.join([f\"keyword: {', '.join(keywords)}\"])\n    }\n\n    # TF-IDF Vectorization for the input movie features\n    input_genre_vector = tfidf_vectorizer_genre.transform([input_movie_features['genre']])\n    input_director_vector = tfidf_vectorizer_director.transform([input_movie_features['director']])\n    input_actor_vector = tfidf_vectorizer_actor.transform([input_movie_features['actors']])\n    input_keywords_vector = tfidf_vectorizer_keywords.transform([input_movie_features['keywords']])\n    \n    # Compute cosine similarity for genre, director, actors, and keywords\n    genre_cosine_similarities = cosine_similarity(input_genre_vector, genre_tfidf_matrix).flatten()\n    director_cosine_similarities = cosine_similarity(input_director_vector, director_tfidf_matrix).flatten()\n    actor_cosine_similarities = cosine_similarity(input_actor_vector, actor_tfidf_matrix).flatten()\n    keywords_cosine_similarities = cosine_similarity(input_keywords_vector, keywords_tfidf_matrix).flatten()\n    \n    # Compute final similarity scores\n    # Normalize to avoid bias\n    genre_sim = (genre_cosine_similarities - genre_cosine_similarities.min()) / (genre_cosine_similarities.max() - genre_cosine_similarities.min())\n    director_sim = (director_cosine_similarities - director_cosine_similarities.min()) / (director_cosine_similarities.max() - director_cosine_similarities.min())\n    actor_sim = (actor_cosine_similarities - actor_cosine_similarities.min()) / (actor_cosine_similarities.max() - actor_cosine_similarities.min())\n       # Continue with similarity normalization and combination\n    keywords_sim = (keywords_cosine_similarities - keywords_cosine_similarities.min()) / (keywords_cosine_similarities.max() - keywords_cosine_similarities.min())\n\n    # Combine the similarity scores\n    combined_similarities = (genre_sim * 0.25 +\n                             director_sim * 0.25 +\n                             actor_sim * 0.25 +\n                             keywords_sim * 0.25)\n\n    # Get indices of the top 10 most similar movies\n    similar_indices = combined_similarities.argsort()[-11:-1][::-1]\n\n    # Debugging output\n    print(\"Cosine similarities (genre):\", genre_cosine_similarities)\n    print(\"Cosine similarities (director):\", director_cosine_similarities)\n    print(\"Cosine similarities (actors):\", actor_cosine_similarities)\n    print(\"Cosine similarities (keywords):\", keywords_cosine_similarities)\n    print(\"Combined similarities:\", combined_similarities)\n    print(\"Similar indices:\", similar_indices)\n\n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n\n    recommended_movies = similar_movies['title'].tolist()\n    \n    if not recommended_movies:\n        return \"No recommendations found.\"\n    \n    return recommended_movies\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie_details.get('external_ids', {})\n    genres = [g['name'] for g in movie_details.get('genres', [])]\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    cast = movie_details.get('credits', {}).get('cast', [])[:5]  # Top 5 actors\n    keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])]\n    imdb_id = external_ids.get('imdb_id')\n    synopsis = movie_details.get('overview', '')\n    ratings = movie_details.get('vote_average', 0)\n    reviews = movie_details.get('reviews', {}).get('results', [])\n\n    return genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords\n\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    print(f\"Synopsis: {synopsis}\")\n    print(f\"Ratings: {ratings}\")\n    print(f\"Reviews: {reviews}\")\n    print(f\"Actors: {actors}\")\n    print(f\"Keywords: {keywords}\")\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_subset = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings_subset.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Apply SVD for collaborative filtering\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n    \n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    if director is None:\n        director = ''\n    \n    # Extract BERT embeddings for the synopsis\n    movie_synopses = movies_with_tags['overview'].fillna('')\n    synopsis_embeddings = get_bert_embeddings(movie_synopses.tolist())\n\n    # TF-IDF Vectorization for genres, director, actors, and keywords\n    tfidf_vectorizer_genre = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    genre_tfidf_matrix = tfidf_vectorizer_genre.fit_transform(movies_with_tags['genre'].fillna(''))\n    \n    tfidf_vectorizer_director = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    director_tfidf_matrix = tfidf_vectorizer_director.fit_transform(movies_with_tags['director'].fillna(''))\n    \n    tfidf_vectorizer_actor = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    actor_tfidf_matrix = tfidf_vectorizer_actor.fit_transform(movies_with_tags['cast'].fillna(''))\n    \n    tfidf_vectorizer_keywords = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    keywords_tfidf_matrix = tfidf_vectorizer_keywords.fit_transform(movies_with_tags['tag'].fillna(''))\n\n    input_movie_features = {\n        'genre': ' '.join([f\"genre: {', '.join(genres)}\"]),\n        'director': f\"director: {director}\",\n        'actors': ' '.join([f\"actor: {', '.join([actor['name'] for actor in actors])}\"]),\n        'keywords': ' '.join([f\"keyword: {', '.join(keywords)}\"])\n    }\n\n    # TF-IDF Vectorization for the input movie features\n    input_genre_vector = tfidf_vectorizer_genre.transform([input_movie_features['genre']])\n    input_director_vector = tfidf_vectorizer_director.transform([input_movie_features['director']])\n    input_actor_vector = tfidf_vectorizer_actor.transform([input_movie_features['actors']])\n    input_keywords_vector = tfidf_vectorizer_keywords.transform([input_movie_features['keywords']])\n    \n    # Compute cosine similarity for genre, director, actors, and keywords\n    genre_cosine_similarities = cosine_similarity(input_genre_vector, genre_tfidf_matrix).flatten()\n    director_cosine_similarities = cosine_similarity(input_director_vector, director_tfidf_matrix).flatten()\n    actor_cosine_similarities = cosine_similarity(input_actor_vector, actor_tfidf_matrix).flatten()\n    keywords_cosine_similarities = cosine_similarity(input_keywords_vector, keywords_tfidf_matrix).flatten()\n    \n    # Normalize to avoid bias and compute final similarity scores\n    genre_sim = (genre_cosine_similarities - genre_cosine_similarities.min()) / (genre_cosine_similarities.max() - genre_cosine_similarities.min())\n    director_sim = (director_cosine_similarities - director_cosine_similarities.min()) / (director_cosine_similarities.max() - director_cosine_similarities.min())\n    actor_sim = (actor_cosine_similarities - actor_cosine_similarities.min()) / (actor_cosine_similarities.max() - actor_cosine_similarities.min())\n    keywords_sim = (keywords_cosine_similarities - keywords_cosine_similarities.min()) / (keywords_cosine_similarities.max() - keywords_cosine_similarities.min())\n\n    # Combine the similarity scores\n    combined_similarities = (genre_sim * 0.25 + director_sim * 0.25 + actor_sim * 0.25 + keywords_sim * 0.25)\n    similar_movies_idx = np.argsort(combined_similarities)[::-1][:10]\n\n    recommended_movies = []\n    for idx in similar_movies_idx[:3]:\n        similar_movie_title = movies_with_tags.iloc[idx]['title']\n        if normalize_title(similar_movie_title) != normalized_movie_name:\n            recommended_movies.append(f\"Recommendation: {denormalize_title(similar_movie_title)}\")\n\n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Example usage\nrecommended_movies = recommend_movies('Heat')\nprint(recommended_movies)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Handle NA values\nmovies_with_tags.fillna({'genre': 'unknown', 'director': 'unknown', 'overview': 'No overview available', \n                         'cast': 'unknown', 'tag': 'no keywords'}, inplace=True)\n\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie_details.get('external_ids', {})\n    genres = [g['name'] for g in movie_details.get('genres', [])] or ['unknown']\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    director = director or 'unknown'\n    cast = movie_details.get('credits', {}).get('cast', [])[:5] or ['unknown']\n    keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])] or ['no keywords']\n    imdb_id = external_ids.get('imdb_id')\n    synopsis = movie_details.get('overview', 'No overview available')\n    ratings = movie_details.get('vote_average', 0)\n    reviews = movie_details.get('reviews', {}).get('results', [])\n\n    return genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords\n\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    print(f\"Synopsis: {synopsis}\")\n    print(f\"Ratings: {ratings}\")\n    print(f\"Reviews: {reviews}\")\n    print(f\"Actors: {actors}\")\n    print(f\"Keywords: {keywords}\")\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_subset = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings_subset.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Apply SVD for collaborative filtering\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n    \n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Extract BERT embeddings for the synopsis\n    movie_synopses = movies_with_tags['overview'].fillna('')\n    synopsis_embeddings = get_bert_embeddings(movie_synopses.tolist())\n\n    # TF-IDF Vectorization for genres, director, actors, and keywords\n    tfidf_vectorizer_genre = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    genre_tfidf_matrix = tfidf_vectorizer_genre.fit_transform(movies_with_tags['genre'])\n    \n    tfidf_vectorizer_director = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    director_tfidf_matrix = tfidf_vectorizer_director.fit_transform(movies_with_tags['director'])\n    \n    tfidf_vectorizer_actor = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    actor_tfidf_matrix = tfidf_vectorizer_actor.fit_transform(movies_with_tags['cast'])\n    \n    tfidf_vectorizer_keywords = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    keywords_tfidf_matrix = tfidf_vectorizer_keywords.fit_transform(movies_with_tags['tag'])\n\n    input_movie_features = {\n        'genre': ' '.join([f\"genre: {', '.join(genres)}\"]),\n        'director': f\"director: {director}\",\n        'actors': ' '.join([f\"actor: {', '.join([actor['name'] for actor in actors])}\"]),\n        'keywords': ' '.join([f\"keyword: {', '.join(keywords)}\"])\n    }\n\n    # TF-IDF Vectorization for the input movie features\n    input_genre_vector = tfidf_vectorizer_genre.transform([input_movie_features['genre']])\n    input_director_vector = tfidf_vectorizer_director.transform([input_movie_features['director']])\n    input_actor_vector = tfidf_vectorizer_actor.transform([input_movie_features['actors']])\n    input_keywords_vector = tfidf_vectorizer_keywords.transform([input_movie_features['keywords']])\n    \n    # Compute cosine similarity for genre, director, actors, and keywords\n    genre_cosine_similarities = cosine_similarity(input_genre_vector, genre_tfidf_matrix).flatten()\n    director_cosine_similarities = cosine_similarity(input_director_vector, director_tfidf_matrix).flatten()\n    actor_cosine_similarities = cosine_similarity(input_actor_vector, actor_tfidf_matrix).flatten()\n    keywords_cosine_similarities = cosine_similarity(input_keywords_vector, keywords_tfidf_matrix).flatten()\n    \n    # Normalize to avoid bias and compute final similarity scores\n    genre_sim = (genre_cosine_similarities - genre_cosine_similarities.min()) / (genre_cosine_similarities.max() - genre_cosine_similarities.min())\n    director_sim = (director_cosine_similarities - director_cosine_similarities.min()) / (director_cosine_similarities.max() - director_cosine_similarities.min())\n    actor_sim = (actor_cosine_similarities - actor_cosine_similarities.min()) / (actor_cosine_similarities.max() - actor_cosine_similarities.min())\n    keywords_sim = (keywords_cosine_similarities - keywords_cosine_similarities.min()) / (keywords_cosine_similarities.max() - keywords_cosine_similarities.min())\n    \n    # Weighted sum of similarities\n    combined_similarity = genre_sim * 0.3 + director_sim * 0.2 + actor_sim * 0.25 + keywords_sim * 0.25\n    top_similar_movies_idx = combined_similarity.argsort()[::-1][:3]\n    \n    recommended_movies = []\n    for idx in top_similar_movies_idx:\n        recommended_movie = denormalize_title(movies_with_tags.iloc[idx]['normalized_title'])\n        if recommended_movie and recommended_movie != normalized_movie_name:\n            recommended_movies.append(recommended_movie)\n    \n    return recommended_movies if recommended_movies else \"No recommendations found.\"\n\n# Example usage\nprint(recommend_movies('Interstellar'))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Handle NA values\nmovies_with_tags.fillna({'genre': 'unknown', 'director': 'unknown', 'overview': 'No overview available', \n                         'cast': 'unknown', 'tag': 'no keywords'}, inplace=True)\n\n# Normalize movie titles\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\n# Denormalize movie titles to original form\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\n# Fetch movie details using TMDb\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie_details.get('external_ids', {})\n    genres = [g['name'] for g in movie_details.get('genres', [])] or ['unknown']\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    director = director or 'unknown'\n    cast = movie_details.get('credits', {}).get('cast', [])[:5] or ['unknown']\n    keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])] or ['no keywords']\n    imdb_id = external_ids.get('imdb_id')\n    synopsis = movie_details.get('overview', 'No overview available')\n    ratings = movie_details.get('vote_average', 0)\n    reviews = movie_details.get('reviews', {}).get('results', [])\n\n    return genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords\n\n# BERT embeddings for synopsis\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\n# Main recommendation function\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n    \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = get_movie_details_by_tmdb(tmdb_id)\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    print(f\"Synopsis: {synopsis}\")\n    print(f\"Ratings: {ratings}\")\n    print(f\"Reviews: {reviews}\")\n    print(f\"Actors: {actors}\")\n    print(f\"Keywords: {keywords}\")\n\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\n# Collaborative filtering recommendation\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_subset = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings_subset.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Apply SVD for collaborative filtering\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n    \n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Content-based recommendation\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    \n    # Extract BERT embeddings for the synopsis\n    movie_synopses = movies_with_tags['overview'].fillna('')\n    synopsis_embeddings = get_bert_embeddings(movie_synopses.tolist())\n\n    # TF-IDF Vectorization for genres, director, actors, and keywords\n    tfidf_vectorizer_genre = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    genre_tfidf_matrix = tfidf_vectorizer_genre.fit_transform(movies_with_tags['genre'])\n    \n    tfidf_vectorizer_director = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    director_tfidf_matrix = tfidf_vectorizer_director.fit_transform(movies_with_tags['director'])\n    \n    tfidf_vectorizer_actor = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    actor_tfidf_matrix = tfidf_vectorizer_actor.fit_transform(movies_with_tags['cast'])\n    \n    tfidf_vectorizer_keywords = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n    keywords_tfidf_matrix = tfidf_vectorizer_keywords.fit_transform(movies_with_tags['tag'])\n\n    input_movie_features = {\n        'genre': ' '.join([f\"genre: {', '.join(genres)}\"]),\n        'director': f\"director: {director}\",\n        'actors': ' '.join([f\"actor: {', '.join([actor['name'] for actor in actors])}\"]),\n        'keywords': ' '.join([f\"keyword: {', '.join(keywords)}\"])\n    }\n\n    # TF-IDF Vectorization for the input movie features\n    input_genre_vector = tfidf_vectorizer_genre.transform([input_movie_features['genre']])\n    input_director_vector = tfidf_vectorizer_director.transform([input_movie_features['director']])\n    input_actor_vector = tfidf_vectorizer_actor.transform([input_movie_features['actors']])\n    input_keywords_vector = tfidf_vectorizer_keywords.transform([input_movie_features['keywords']])\n    \n    # Compute cosine similarity for genre, director, actors, and keywords\n    genre_cosine_similarities = cosine_similarity(input_genre_vector, genre_tfidf_matrix).flatten()\n    director_cosine_similarities = cosine_similarity(input_director_vector, director_tfidf_matrix).flatten()\n    actor_cosine_similarities = cosine_similarity(input_actor_vector, actor_tfidf_matrix).flatten()\n    keywords_cosine_similarities = cosine_similarity(input_keywords_vector, keywords_tfidf_matrix).flatten()\n    \n    # Normalize to avoid bias and compute final similarity scores\n    genre_sim = (genre_cosine_similarities - genre_cosine_similarities.min()) / (genre_cosine_similarities.max() - genre_cosine_similarities.min())\n    director_sim = (director_cosine_similarities - director_cosine_similarities.min()) / (director_cosine_similarities.max() - director_cosine_similarities.min())\n    actor_sim = (actor_cosine_similarities - actor_cosine_similarities.min()) / (actor_cosine_similarities.max() - actor_cosine_similarities.min())\n    keywords_sim = (keywords_cosine_similarities - keywords_cosine_similarities.min()) / (keywords_cosine_similarities.max() - keywords_cosine_similarities.min())\n\n    # Combine all similarity scores (tune weights if needed)\n    combined_similarities = 0.3 * genre_sim + 0.3 * director_sim + 0.2 * actor_sim + 0.2 * keywords_sim\n    \n    # Exclude the input movie from the recommendations\n    indices = movies_with_tags[movies_with_tags['normalized_title'] != normalized_movie_name].index\n    best_matches = indices[combined_similarities.argsort()[-3:][::-1]]\n\n    recommended_movies = []\n    for i, idx in enumerate(best_matches):\n        recommended_movies.append(f\"{i+1}. {movies_with_tags.loc[idx, 'title']}\")\n\n    return \"\\n\".join(recommended_movies)\n\n# Test the function with a sample movie name\nmovie_name = \"Heat\"\nrecommendations = recommend_movies(movie_name)\nprint(f\"Recommendations for {movie_name}: \\n{recommendations}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# final trial","metadata":{}},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Handle NA values\nmovies_with_tags.fillna({'genre': 'unknown', 'director': 'unknown', 'overview': 'No overview available', \n                         'cast': 'unknown', 'tag': 'no keywords'}, inplace=True)\n\n# Normalize movie titles\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\n# Denormalize movie titles to original form\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\n# Fetch movie details using TMDb\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie_details.get('external_ids', {})\n    genres = [g['name'] for g in movie_details.get('genres', [])] or ['unknown']\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    director = director or 'unknown'\n    cast = movie_details.get('credits', {}).get('cast', [])[:5] or ['unknown']\n    keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])] or ['no keywords']\n    imdb_id = external_ids.get('imdb_id')\n    synopsis = movie_details.get('overview', 'No overview available')\n    ratings = movie_details.get('vote_average', 0)\n    reviews = movie_details.get('reviews', {}).get('results', [])\n\n    return genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords\n\n# BERT embeddings for synopsis\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\n# Main recommendation function\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n   \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = get_movie_details_by_tmdb(tmdb_id)\n    \"\"\"\n    print(f\"Genres: {genres}, Director: {director}, IMDb ID: {imdb_id}\")\n    print(f\"Synopsis: {synopsis}\")\n    print(f\"Ratings: {ratings}\")\n    print(f\"Reviews: {reviews}\")\n    print(f\"Actors: {actors}\")\n    print(f\"Keywords: {keywords}\")\n    \"\"\"\n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\n# Collaborative filtering recommendation\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_subset = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings_subset.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Apply SVD for collaborative filtering\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n    # Content-based recommendation\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name):\n    # Preprocess overviews and create BERT embeddings\n    movies_with_tags['overview_bert'] = movies_with_tags['overview'].apply(get_bert_embeddings)\n\n    # Combine BERT embeddings with TF-IDF for genres and director\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    genre_director_tfidf = tfidf_vectorizer.fit_transform(movies_with_tags['genre'] + ' ' + movies_with_tags['director'])\n\n    # Convert BERT embeddings to NumPy arrays\n    overview_bert_array = np.array(movies_with_tags['overview_bert'].tolist())\n\n    # Reshape overview_bert_array from (10548, 1, 768) to (10548, 768)\n    overview_bert_array = overview_bert_array.reshape(overview_bert_array.shape[0], -1)\n\n    # Check the shapes\n    num_bert_features = overview_bert_array.shape[1]\n    num_tfidf_features = genre_director_tfidf.shape[1]\n\n    print(\"Shape of overview_bert_array:\", overview_bert_array.shape)\n    print(\"Shape of genre_director_tfidf.toarray():\", genre_director_tfidf.toarray().shape)\n\n    if num_bert_features > num_tfidf_features:\n        # Reduce the larger array (overview_bert_array) to match the smaller one (genre_director_tfidf)\n        overview_bert_array = overview_bert_array[:, :num_tfidf_features]\n    elif num_tfidf_features > num_bert_features:\n        # Reduce the larger array (genre_director_tfidf) to match the smaller one (overview_bert_array)\n        genre_director_tfidf = genre_director_tfidf[:, :num_bert_features]\n\n    # Ensure both arrays are 2D and concatenate them\n    combined_features = np.hstack((overview_bert_array, genre_director_tfidf.toarray()))\n\n    # Vectorize the input movie's features\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = np.hstack((get_bert_embeddings(synopsis).reshape(1, -1), tfidf_vectorizer.transform([input_movie_features]).toarray()))\n\n    # Compute cosine similarity\n    cosine_similarities = cosine_similarity(input_movie_vector, combined_features).flatten()\n\n    # Get top similar movies\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n\n    # Filter out the input movie itself\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n    \n    if similar_movies.empty:\n        return \"No recommendations found.\"\n    \n    return similar_movies['title'].tolist()\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:49:08.303779Z","iopub.execute_input":"2024-09-18T22:49:08.304171Z","iopub.status.idle":"2024-09-18T23:44:00.912124Z","shell.execute_reply.started":"2024-09-18T22:49:08.304132Z","shell.execute_reply":"2024-09-18T23:44:00.910727Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a movie name:  heat\n"},{"name":"stdout","text":"Found movie: Heat (TMDb ID: 949)\nShape of overview_bert_array: (10548, 768)\nShape of genre_director_tfidf.toarray(): (10548, 5186)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m movie_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a movie name: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrecommend_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_name\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[0;32mIn[4], line 100\u001b[0m, in \u001b[0;36mrecommend_movies\u001b[0;34m(movie_name)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecommend_movies_by_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynopsis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreviews\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_movie_name\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 169\u001b[0m, in \u001b[0;36mrecommend_movies_by_content\u001b[0;34m(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\u001b[0m\n\u001b[1;32m    166\u001b[0m input_movie_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((get_bert_embeddings(synopsis)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform([input_movie_features])\u001b[38;5;241m.\u001b[39mtoarray()))\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Compute cosine similarity\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_movie_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Get top similar movies\u001b[39;00m\n\u001b[1;32m    172\u001b[0m similar_indices \u001b[38;5;241m=\u001b[39m cosine_similarities\u001b[38;5;241m.\u001b[39margsort()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1393\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;124;03m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1393\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:180\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    178\u001b[0m         )\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n","\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 5954 while Y.shape[1] == 1536"],"ename":"ValueError","evalue":"Incompatible dimension for X and Y matrices: X.shape[1] == 5954 while Y.shape[1] == 1536","output_type":"error"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Handle NA values\nmovies_with_tags.fillna({'genre': 'unknown', 'director': 'unknown', 'overview': 'No overview available', \n                         'cast': 'unknown', 'tag': 'no keywords'}, inplace=True)\n\n# Normalize movie titles\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\n# Denormalize movie titles to original form\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\n# Fetch movie details using TMDb\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie_details.get('external_ids', {})\n    genres = [g['name'] for g in movie_details.get('genres', [])] or ['unknown']\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    director = director or 'unknown'\n    cast = movie_details.get('credits', {}).get('cast', [])[:5] or ['unknown']\n    keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])] or ['no keywords']\n    imdb_id = external_ids.get('imdb_id')\n    synopsis = movie_details.get('overview', 'No overview available')\n    ratings = movie_details.get('vote_average', 0)\n    reviews = movie_details.get('reviews', {}).get('results', [])\n\n    return genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords\n\n# BERT embeddings for synopsis\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)  # Added padding/truncation\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\n# Main recommendation function\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n   \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = get_movie_details_by_tmdb(tmdb_id)\n    \n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\n# Collaborative filtering recommendation\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_subset = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings_subset.empty:\n        return \"No ratings data available for collaborative filtering.\"\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Apply SVD for collaborative filtering\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Content-based recommendation\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name):\n    # Preprocess overviews and create BERT embeddings\n    movies_with_tags['overview_bert'] = movies_with_tags['overview'].apply(get_bert_embeddings)\n\n    # Combine BERT embeddings with TF-IDF for genres and director\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    genre_director_tfidf = tfidf_vectorizer.fit_transform(movies_with_tags['genre'] + ' ' + movies_with_tags['director'])\n\n    # Convert BERT embeddings to NumPy arrays\n    overview_bert_array = np.array(movies_with_tags['overview_bert'].tolist())\n\n    # Reshape overview_bert_array from (10548, 1, 768) to (10548, 768)\n    overview_bert_array = overview_bert_array.reshape(overview_bert_array.shape[0], -1)\n\n    # Check the shapes of BERT and TF-IDF feature arrays\n    num_bert_features = overview_bert_array.shape[1]\n    num_tfidf_features = genre_director_tfidf.shape[1]\n\n    print(\"Shape of overview_bert_array:\", overview_bert_array.shape)\n    print(\"Shape of genre_director_tfidf.toarray():\", genre_director_tfidf.toarray().shape)\n\n    # Adjust dimensions by padding or trimming to ensure both have the same size\n    if num_bert_features > num_tfidf_features:\n        # If BERT features are larger, trim BERT array to match TF-IDF\n        overview_bert_array = overview_bert_array[:, :num_tfidf_features]\n    elif num_tfidf_features > num_bert_features:\n        # If TF-IDF features are larger, pad TF-IDF array to match BERT\n        padding_width = num_tfidf_features - num_bert_features\n        padding = np.zeros((overview_bert_array.shape[0], padding_width))\n        overview_bert_array = np.hstack((overview_bert_array, padding))\n\n    # Combine the adjusted arrays\n    combined_features = np.hstack((overview_bert_array, genre_director_tfidf.toarray()))\n\n    # Vectorize the input movie's features (for new movie content)\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = np.hstack((get_bert_embeddings(synopsis).reshape(1, -1), tfidf_vectorizer.transform([input_movie_features]).toarray()))\n\n    # Ensure the input vector also matches the combined feature size\n    if input_movie_vector.shape[1] < combined_features.shape[1]:\n        padding_width = combined_features.shape[1] - input_movie_vector.shape[1]\n        input_movie_vector = np.hstack((input_movie_vector, np.zeros((1, padding_width))))\n    elif input_movie_vector.shape[1] > combined_features.shape[1]:\n        input_movie_vector = input_movie_vector[:, :combined_features.shape[1]]\n\n    # Compute cosine similarity\n    cosine_similarities = cosine_similarity(input_movie_vector, combined_features).flatten()\n\n    # Get top similar movies and filter out the input movie itself\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    # Return recommendations\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n    else:\n        return similar_movies['title'].tolist()\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T23:54:06.193128Z","iopub.execute_input":"2024-09-18T23:54:06.193485Z","iopub.status.idle":"2024-09-19T00:50:40.384923Z","shell.execute_reply.started":"2024-09-18T23:54:06.193452Z","shell.execute_reply":"2024-09-19T00:50:40.383671Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a movie name:  me before you\n"},{"name":"stdout","text":"Found movie: Me Before You (TMDb ID: 296096)\nShape of overview_bert_array: (10548, 768)\nShape of genre_director_tfidf.toarray(): (10548, 5186)\n['The Kid', 'The Kid', 'I Capture the Castle', 'Stanley & Iris', 'Bella', 'The Girl Next Door', 'The Girl Next Door', 'Me and You and Everyone We Know', \"Breakfast at Tiffany's\", 'Man on the Train']\n","output_type":"stream"}]},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\n# Handle NA values\nmovies_with_tags.fillna({'genre': 'unknown', 'director': 'unknown', 'overview': 'No overview available', \n                         'cast': 'unknown', 'tag': 'no keywords'}, inplace=True)\n\n# Normalize movie titles\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\n# Denormalize movie titles to original form\ndef denormalize_title(normalized_title):\n    for original_title in movies_with_tags['title']:\n        if normalize_title(original_title) == normalized_title:\n            return original_title\n    return None\n\n# Fetch movie details using TMDb\ndef get_movie_details_by_tmdb(tmdb_id):\n    movie_details = movie.details(tmdb_id)\n    external_ids = movie_details.get('external_ids', {})\n    genres = [g['name'] for g in movie_details.get('genres', [])] or ['unknown']\n    director = None\n    for crew_member in movie_details.get('credits', {}).get('crew', []):\n        if crew_member['job'] == 'Director':\n            director = crew_member['name']\n            break\n    director = director or 'unknown'\n    cast = movie_details.get('credits', {}).get('cast', [])[:5] or ['unknown']\n    keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])] or ['no keywords']\n    imdb_id = external_ids.get('imdb_id')\n    synopsis = movie_details.get('overview', 'No overview available')\n    ratings = movie_details.get('vote_average', 0)\n    reviews = movie_details.get('reviews', {}).get('results', [])\n\n    return genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords\n\n# BERT embeddings for synopsis\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\n# Main recommendation function\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n   \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = get_movie_details_by_tmdb(tmdb_id)\n    \n    if imdb_id:\n        if imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n            return recommend_movies_collaborative(imdb_id)\n        else:\n            return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\n# Collaborative filtering recommendation\ndef recommend_movies_collaborative(imdb_id):\n    user_movie_ratings_subset = user_movie_ratings[user_movie_ratings['imdbId'] == imdb_id]\n    if user_movie_ratings_subset.empty:\n        return \"No ratings data available for collaborative filtering.\"\n\n    user_movie_ratings = user_movie_ratings.groupby(['userId', 'title'], as_index=False).agg({'rating': 'mean'})\n    user_movie_matrix = user_movie_ratings.pivot(index='userId', columns='title', values='rating').fillna(0)\n\n    # Apply SVD for collaborative filtering\n    svd = TruncatedSVD(n_components=20)\n    matrix_factorized = svd.fit_transform(user_movie_matrix)\n\n    cosine_sim_matrix = cosine_similarity(matrix_factorized.T)\n    \n    try:\n        movie_idx = list(user_movie_matrix.columns).index(imdb_id)\n    except ValueError:\n        return \"Movie not found in the ratings dataset.\"\n\n    similar_movies = list(enumerate(cosine_sim_matrix[movie_idx]))\n    similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n\n    recommended_movies = []\n    for i, (movie_idx, score) in enumerate(similar_movies[:10]):\n        similar_movie_title = user_movie_matrix.columns[movie_idx]\n        recommended_movies.append(f\"{i+1}. {similar_movie_title}\")\n        if len(recommended_movies) == 3:\n            break\n    \n    return \"\\n\".join(recommended_movies) if recommended_movies else \"No recommendations found.\"\n\n# Content-based recommendation\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name):\n    # Preprocess overviews and create BERT embeddings\n    movies_with_tags['overview_bert'] = movies_with_tags['overview'].apply(lambda x: get_bert_embeddings([x])[0])\n\n    # Combine BERT embeddings with TF-IDF for genres and director\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n    genre_director_tfidf = tfidf_vectorizer.fit_transform(movies_with_tags['genre'] + ' ' + movies_with_tags['director'])\n\n    # Convert BERT embeddings to NumPy arrays\n    overview_bert_array = np.array(movies_with_tags['overview_bert'].tolist())\n\n    # Reshape overview_bert_array from (10548, 1, 768) to (10548, 768)\n    overview_bert_array = overview_bert_array.reshape(overview_bert_array.shape[0], -1)\n\n    # Check the shapes of BERT and TF-IDF feature arrays\n    num_bert_features = overview_bert_array.shape[1]\n    num_tfidf_features = genre_director_tfidf.shape[1]\n\n    print(\"Shape of overview_bert_array:\", overview_bert_array.shape)\n    print(\"Shape of genre_director_tfidf.toarray():\", genre_director_tfidf.toarray().shape)\n\n    # Adjust dimensions by padding or trimming to ensure both have the same size\n    if num_bert_features > num_tfidf_features:\n        overview_bert_array = overview_bert_array[:, :num_tfidf_features]\n    elif num_tfidf_features > num_bert_features:\n        padding_width = num_tfidf_features - num_bert_features\n        padding = np.zeros((overview_bert_array.shape[0], padding_width))\n        overview_bert_array = np.hstack((overview_bert_array, padding))\n\n    # Combine the adjusted arrays\n    combined_features = np.hstack((overview_bert_array, genre_director_tfidf.toarray()))\n\n    # Vectorize the input movie's features (for new movie content)\n    input_movie_features = ' '.join([f\"genre: {', '.join(genres)}\", f\"director: {director}\"])\n    input_movie_vector = np.hstack((get_bert_embeddings([synopsis])[0].reshape(1, -1), tfidf_vectorizer.transform([input_movie_features]).toarray()))\n\n    # Ensure the input vector also matches the combined feature size\n    if input_movie_vector.shape[1] < combined_features.shape[1]:\n        padding_width = combined_features.shape[1] - input_movie_vector.shape[1]\n        input_movie_vector = np.hstack((input_movie_vector, np.zeros((1, padding_width))))\n    elif input_movie_vector.shape[1] > combined_features.shape[1]:\n        input_movie_vector = input_movie_vector[:, :combined_features.shape[1]]\n\n    # Compute cosine similarity\n    cosine_similarities = cosine_similarity(input_movie_vector, combined_features).flatten()\n\n    # Get top similar movies and filter out the input movie itself\n    similar_indices = cosine_similarities.argsort()[-11:-1][::-1]\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    # Return recommendations\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n    else:\n        return \"\\n\".join([f\"{i+1}. {row['title']}: {row['overview']}\" for i, row in similar_movies.head(3).iterrows()])\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deployment version of the model ","metadata":{}},{"cell_type":"code","source":"from tmdbv3api import TMDb, Movie\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\nfrom joblib import Parallel, delayed\nimport joblib\nimport os\n\n# Initialize TMDb\ntmdb = TMDb()\ntmdb.api_key = 'ddad317e776c8ec2f92ec52efe9d34f5'\n\n# Set up Movie object\nmovie = Movie()\n\n# Load the datasets\nratings_path = '/kaggle/input/rating-imdbid/ratings.csv'\nuser_movie_ratings = pd.read_csv(ratings_path)\nmovies_with_tags = pd.read_csv('/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Initialize DistilBERT\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ndistilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n\n# Handle NA values\nmovies_with_tags.fillna({\n    'genre': 'unknown',\n    'director': 'unknown',\n    'overview': 'No overview available',\n    'cast': 'unknown',\n    'tag': 'no keywords'\n}, inplace=True)\n\n# Normalize movie titles\ndef normalize_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower().strip()\n    title = re.sub(r'\\(\\d{4}\\)', '', title).strip()\n    return title\n\nmovies_with_tags['normalized_title'] = movies_with_tags['title'].apply(normalize_title)\n\n# Cache for TMDb API responses\ntmdb_cache = {}\n\n# Precompute movie embeddings and save them\ndef cache_tmdb_movie_details(tmdb_id):\n    if tmdb_id not in tmdb_cache:\n        movie_details = movie.details(tmdb_id)\n        genres = [g['name'] for g in movie_details.get('genres', [])] or ['unknown']\n        director = 'unknown'\n        for crew_member in movie_details.get('credits', {}).get('crew', []):\n            if crew_member['job'] == 'Director':\n                director = crew_member['name']\n                break\n        cast = movie_details.get('credits', {}).get('cast', [])[:5] or ['unknown']\n        keywords = [k['name'] for k in movie_details.get('keywords', {}).get('keywords', [])] or ['no keywords']\n        synopsis = movie_details.get('overview', 'No overview available')\n        ratings = movie_details.get('vote_average', 0)\n        reviews = movie_details.get('reviews', {}).get('results', [])\n        imdb_id = movie_details.get('external_ids', {}).get('imdb_id')\n\n        tmdb_cache[tmdb_id] = (genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords)\n        return genres, director, imdb_id, synopsis, ratings, reviews, cast, keywords\n    return tmdb_cache[tmdb_id]\n\n# BERT embeddings for synopsis\ndef get_bert_embeddings(texts):\n    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = distilbert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\n# Precompute and store embeddings for movie overviews\ndef precompute_embeddings_and_similarity():\n    # Precompute BERT embeddings\n    if not os.path.exists(\"overview_bert_embeddings.npy\"):\n        print(\"Precomputing BERT embeddings for movie overviews...\")\n        bert_embeddings = Parallel(n_jobs=-1)(delayed(get_bert_embeddings)(overview) for overview in movies_with_tags['overview'])\n        bert_embeddings = np.vstack(bert_embeddings)\n        np.save(\"overview_bert_embeddings.npy\", bert_embeddings)\n    else:\n        print(\"Loading precomputed BERT embeddings...\")\n        bert_embeddings = np.load(\"overview_bert_embeddings.npy\")\n\n    # Precompute TF-IDF vectors for genres and director\n    if not os.path.exists(\"tfidf_features.npy\"):\n        print(\"Precomputing TF-IDF features for genres and directors...\")\n        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n        genre_director_tfidf = tfidf_vectorizer.fit_transform(movies_with_tags['genre'] + ' ' + movies_with_tags['director'])\n        joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n        np.save(\"tfidf_features.npy\", genre_director_tfidf.toarray())\n    else:\n        print(\"Loading precomputed TF-IDF features...\")\n        genre_director_tfidf = np.load(\"tfidf_features.npy\")\n    \n    # Reduce dimensions for faster similarity computations\n    print(\"Reducing dimensions with TruncatedSVD...\")\n    svd = TruncatedSVD(n_components=100)\n    bert_embeddings_reduced = svd.fit_transform(bert_embeddings)\n    genre_director_tfidf_reduced = svd.fit_transform(genre_director_tfidf)\n\n    # Compute cosine similarities between all movies and store them\n    combined_features = np.hstack((bert_embeddings_reduced, genre_director_tfidf_reduced))\n    similarity_matrix = cosine_similarity(combined_features)\n    np.save(\"similarity_matrix.npy\", similarity_matrix)\n\n# Run precomputation process (will skip if already done)\nprecompute_embeddings_and_similarity()\n\n# Main recommendation function using precomputed data\ndef recommend_movies(movie_name):\n    normalized_movie_name = normalize_title(movie_name)\n    search_results = movie.search(movie_name)\n    \n    if not search_results:\n        return f\"No movie found for '{movie_name}'\"\n    \n    tmdb_id = search_results[0].id\n    print(f\"Found movie: {search_results[0].title} (TMDb ID: {tmdb_id})\")\n   \n    genres, director, imdb_id, synopsis, ratings, reviews, actors, keywords = cache_tmdb_movie_details(tmdb_id)\n    \n    if imdb_id and imdb_id in user_movie_ratings['imdbId'].astype(str).unique():\n        return recommend_movies_collaborative(imdb_id)\n    else:\n        return recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name)\n\n# Content-based recommendation using precomputed similarity matrix\ndef recommend_movies_by_content(genres, director, synopsis, ratings, reviews, actors, keywords, normalized_movie_name):\n    similarity_matrix = np.load(\"similarity_matrix.npy\")\n    movie_idx = movies_with_tags.index[movies_with_tags['normalized_title'] == normalized_movie_name].tolist()\n\n    if not movie_idx:\n        return f\"No recommendations found for '{normalized_movie_name}'\"\n    \n    movie_idx = movie_idx[0]\n    similar_indices = similarity_matrix[movie_idx].argsort()[-11:-1][::-1]\n\n    similar_movies = movies_with_tags.iloc[similar_indices]\n    similar_movies = similar_movies[similar_movies['normalized_title'] != normalized_movie_name]\n\n    if similar_movies.empty:\n        return \"No recommendations found.\"\n    \n    return similar_movies['title'].tolist()\n\n# Example usage:\nmovie_name = input(\"Enter a movie name: \")\nprint(recommend_movies(movie_name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example saving files\nnp.save(\"/kaggle/working/overview_bert_embeddings.npy\", bert_embeddings)\nnp.save(\"/kaggle/working/tfidf_features.npy\", genre_director_tfidf)\nnp.save(\"/kaggle/working/similarity_matrix.npy\", similarity_matrix)","metadata":{},"execution_count":null,"outputs":[]}]}