{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9369105,"sourceType":"datasetVersion","datasetId":5681965},{"sourceId":9376607,"sourceType":"datasetVersion","datasetId":5687713},{"sourceId":9376615,"sourceType":"datasetVersion","datasetId":5687719},{"sourceId":9394033,"sourceType":"datasetVersion","datasetId":5701159}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install imdbPY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\nimport csv\nimport pandas as pd\nfrom imdb import IMDb, IMDbDataAccessError","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ia = imdb.IMDb()\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(r'/kaggle/input/movielens/movies.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#old verison \nimport imdb\nimport csv\nimport random\nimport pandas as pd\nimport concurrent.futures\n\n# Load the movie data\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        data = {\n            'film_name': movie.get('title', ''),\n            'film_rate': movie.get('rating', 0.0),  # default to 0.0 if no rating\n            'plot': movie.get('plot', ''),\n            'cover_url': movie.get('cover url', ''),\n            'cast': ', '.join([person['name'] for person in movie.get('cast', [])[:3]]) if movie.get('cast') else '',  # return empty string if cast is not available\n            'reviews': movie.get('reviews', [{}])[0].get('content', '')  # default to empty string if no reviews\n        }\n        return data\n    except IMDbDataAccessError as e:\n        return {'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': ''}  # return default values\n    except urllib.error.HTTPError as e:\n        return {'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': ''}  # return default values\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\ncsv_file='/kaggle/input/imdbdata/imdb_data.csv'\n# Save the processed data to a CSV file\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['film_name', 'film_rate', 'plot', 'cover_url', 'cast', 'reviews'])\n    for data in data_list:\n        writer.writerow(list(data.values()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#updated version with more features \nimport imdb\nimport csv\nimport random\nimport pandas as pd\nimport concurrent.futures\n\n# Load the movie data\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        ia.update(movie, 'directors')\n        ia.update(movie, 'writers')\n\n        data = {\n            'imdbId': movie_id,\n            'film_name': movie.get('title', ''),\n            'film_rate': movie.get('rating', 0.0),  # default to 0.0 if no rating\n            'plot': movie.get('plot', [''])[0],  # take the first plot synopsis\n            'cover_url': movie.get('cover url', ''),\n            'cast': ', '.join([person['name'] for person in movie.get('cast', [])[:3]]) if movie.get('cast') else '',  # top 3 cast\n            'reviews': movie.get('reviews', [{}])[0].get('content', ''),  # first review content\n            'director': ', '.join([person['name'] for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'writer': ', '.join([person['name'] for person in movie.get('writers', [])]) if movie.get('writers') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''  # join genre list\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        return {'imdbId': movie_id, 'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': '', 'director': '', 'writer': '', 'genre': ''}\n    except urllib.error.HTTPError as e:\n        return {'imdbId': movie_id, 'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': '', 'director': '', 'writer': '', 'genre': ''}\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Specify the path to save the CSV\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'\n\n# Save the processed data to a CSV file\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'film_rate', 'plot', 'cover_url', 'cast', 'reviews', 'director', 'writer', 'genre'])\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['film_rate'], data['plot'], data['cover_url'], data['cast'], data['reviews'], data['director'], data['writer'], data['genre']])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\nimport pandas as pd\nimport concurrent.futures\n\n# Extract the list of IMDb IDs\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        ia.update(movie, 'directors')\n        ia.update(movie, 'writers')\n\n        # Return the new data, using `.get()` to safely handle missing fields\n        data = {\n            'imdbId': movie_id,\n            'director': ', '.join([person.get('name', '') for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'writer': ', '.join([person.get('name', '') for person in movie.get('writers', [])]) if movie.get('writers') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''  # join genre list\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        return {'imdbId': movie_id, 'director': '', 'writer': '', 'genre': ''}\n    \n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Convert the collected data into a DataFrame\nnew_data_df = pd.DataFrame(data_list)\n\n# Merge the new data with the existing dataset\nupdated_data = pd.merge(data, new_data_df, on='imdbId', how='left')\n\n# Save the updated data back to the CSV file\nupdated_data.to_csv(csv_file, index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Create a Pandas DataFrame from the data_list\ndf = pd.DataFrame(data_list)\n\ndf.to_csv('imdb_data.csv', index=False)\nfrom IPython.display import FileLink\n\n# This will create a download link for the file\nFileLink('imdb_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosication","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n\n# Load the dataset in chunks\nchunksize = 1000\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize)\n\n# Concatenate all chunks into a single DataFrame\nratings = pd.concat(chunks, ignore_index=True)\n\n# Filter only ratings of 4.0 and above for \"liked\" movies\nratings_filtered = ratings[ratings['rating'] >= 4.0]\n\n# Create a user-item matrix\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(user_movie_matrix, min_support=0.02, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Display the rules\nprint(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosiation model ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n\n# Load the dataset in chunks\nchunksize = 1000\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize)\n\n# Concatenate all chunks into a single DataFrame\nratings = pd.concat(chunks, ignore_index=True)\n\n# Filter only ratings of 4.0 and above for \"liked\" movies\nratings_filtered = ratings[ratings['rating'] >= 4.0]\n\n# Create a user-item matrix, where 1 represents a \"liked\" movie (rating 4.0 or above)\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(user_movie_matrix, min_support=0.02, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset to map movie IDs to movie titles\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    \"\"\"\n    movie_id = movies[movies['title'].str.contains(movie_title, case=False, na=False)]['movieId']\n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value (descending)\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get the top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n# Example usage:\nmovie_name = 'Interstellar'  # Change this to the movie title you want recommendations for\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # If the function returns an error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosiation model low resources","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom scipy.sparse import csr_matrix\n\n# Load the dataset in chunks to avoid memory overload\nchunksize = 10**6  # Increase the chunksize to process more data at once but keep it manageable\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize, usecols=['userId', 'movieId', 'rating'])\n\n# Initialize an empty list to store filtered data\nfiltered_chunks = []\n\n# Process each chunk individually\nfor chunk in chunks:\n    # Filter ratings of 4.0 and above for \"liked\" movies\n    filtered_chunk = chunk[chunk['rating'] >= 4.0]\n    filtered_chunks.append(filtered_chunk)\n\n# Concatenate all filtered chunks into a single DataFrame\nratings_filtered = pd.concat(filtered_chunks, ignore_index=True)\n\n# Create a user-item matrix in sparse format to save memory\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\nuser_movie_matrix_sparse = csr_matrix(user_movie_matrix.values)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(pd.DataFrame(user_movie_matrix_sparse.todense(), columns=user_movie_matrix.columns), min_support=0.02, use_colnames=True)\n\n# Generate association rules from the frequent item sets\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift to find the most interesting ones\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset to map movie IDs to movie titles\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    \"\"\"\n    movie_id = movies[movies['title'].str.contains(movie_title, case=False, na=False)]['movieId']\n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value (descending)\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get the top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n# Example usage:\nmovie_name = 'Interstellar'  # Change this to the movie title you want recommendations for\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # If the function returns an error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# final try ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom scipy.sparse import csr_matrix\nimport re\n\n# Function to clean movie titles by removing special characters and lowercasing\ndef clean_movie_title(title):\n    # Remove anything in parentheses (like year)\n    title = re.sub(r'\\(.*?\\)', '', title)\n    # Remove extra spaces and lowercase the title\n    title = re.sub(r'\\s+', ' ', title).strip().lower()\n    return title\n\n# Load the dataset in chunks\nchunksize = 10**6\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize, usecols=['userId', 'movieId', 'rating'])\n\n# Initialize an empty list to store filtered data\nfiltered_chunks = []\n\n# Process each chunk individually\nfor chunk in chunks:\n    # Filter ratings of 4.0 and above for \"liked\" movies\n    filtered_chunk = chunk[chunk['rating'] >= 4.0]\n    filtered_chunks.append(filtered_chunk)\n\n# Concatenate filtered chunks\nratings_filtered = pd.concat(filtered_chunks, ignore_index=True)\n\n# Create a user-item matrix in sparse format\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\nuser_movie_matrix_sparse = csr_matrix(user_movie_matrix.values)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(pd.DataFrame(user_movie_matrix_sparse.todense(), columns=user_movie_matrix.columns), min_support=0.05, use_colnames=True)\n\n# Generate association rules from frequent item sets\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Apply the clean_movie_title function to standardize movie titles\nmovies['clean_title'] = movies['title'].apply(clean_movie_title)\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    Clean the input movie title for consistency.\n    \"\"\"\n    cleaned_title = clean_movie_title(movie_title)\n    movie_id = movies[movies['clean_title'] == cleaned_title]['movieId']\n    \n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:38:15.545099Z","iopub.execute_input":"2024-09-14T17:38:15.545500Z","iopub.status.idle":"2024-09-14T17:38:24.360752Z","shell.execute_reply.started":"2024-09-14T17:38:15.545463Z","shell.execute_reply":"2024-09-14T17:38:24.359833Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Movie 'Shawshank Redemption' not found in the dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example usage:\nmovie_name = 'Heat (1995)'  # You can try different variations like 'interstellar'\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # Error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:43:38.009153Z","iopub.execute_input":"2024-09-14T17:43:38.009531Z","iopub.status.idle":"2024-09-14T17:43:38.302323Z","shell.execute_reply.started":"2024-09-14T17:43:38.009495Z","shell.execute_reply":"2024-09-14T17:43:38.301382Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Movies recommended based on 'Heat (1995)':\n1. Silence of the Lambs, The (1991)\n2. Saving Private Ryan (1998)\n","output_type":"stream"}]}]}