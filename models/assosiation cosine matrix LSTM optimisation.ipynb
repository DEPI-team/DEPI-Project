{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9369105,"sourceType":"datasetVersion","datasetId":5681965},{"sourceId":9376607,"sourceType":"datasetVersion","datasetId":5687713},{"sourceId":9376615,"sourceType":"datasetVersion","datasetId":5687719},{"sourceId":9394033,"sourceType":"datasetVersion","datasetId":5701159},{"sourceId":9415106,"sourceType":"datasetVersion","datasetId":5717842}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install imdbPY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imdb\nimport csv\nimport pandas as pd\nfrom imdb import IMDb, IMDbDataAccessError","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ia = imdb.IMDb()\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(r'/kaggle/input/movielens/movies.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#old verison \nimport imdb\nimport csv\nimport random\nimport pandas as pd\nimport concurrent.futures\n\n# Load the movie data\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        data = {\n            'film_name': movie.get('title', ''),\n            'film_rate': movie.get('rating', 0.0),  # default to 0.0 if no rating\n            'plot': movie.get('plot', ''),\n            'cover_url': movie.get('cover url', ''),\n            'cast': ', '.join([person['name'] for person in movie.get('cast', [])[:3]]) if movie.get('cast') else '',  # return empty string if cast is not available\n            'reviews': movie.get('reviews', [{}])[0].get('content', '')  # default to empty string if no reviews\n        }\n        return data\n    except IMDbDataAccessError as e:\n        return {'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': ''}  # return default values\n    except urllib.error.HTTPError as e:\n        return {'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': ''}  # return default values\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\ncsv_file='/kaggle/input/imdbdata/imdb_data.csv'\n# Save the processed data to a CSV file\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['film_name', 'film_rate', 'plot', 'cover_url', 'cast', 'reviews'])\n    for data in data_list:\n        writer.writerow(list(data.values()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# updated version with more features ","metadata":{}},{"cell_type":"code","source":"import imdb\nimport csv\nimport random\nimport pandas as pd\nimport concurrent.futures\nimport urllib.error\n\n\n# Load the movie data\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        ia.update(movie, 'directors')\n        ia.update(movie, 'writers')\n\n        data = {\n            'imdbId': movie_id,\n            'film_name': movie.get('title', ''),\n            'film_rate': movie.get('rating', 0.0),  # default to 0.0 if no rating\n            'plot': movie.get('plot', [''])[0],  # take the first plot synopsis\n            'cover_url': movie.get('cover url', ''),\n            'cast': ', '.join([person['name'] for person in movie.get('cast', [])[:3]]) if movie.get('cast') else '',  # top 3 cast\n            'reviews': movie.get('reviews', [{}])[0].get('content', ''),  # first review content\n            'director': ', '.join([person['name'] for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'writer': ', '.join([person.get('name', '') for person in movie.get('writers', [])]) if movie.get('writers') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''  # join genre list\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        return {'imdbId': movie_id, 'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': '', 'director': '', 'writer': '', 'genre': ''}\n    except urllib.error.HTTPError as e:\n        return {'imdbId': movie_id, 'film_name': '', 'film_rate': 0.0, 'plot': '', 'cover_url': '', 'cast': '', 'reviews': '', 'director': '', 'writer': '', 'genre': ''}\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Specify the path to save the CSV\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'\n\n# Save the processed data to a CSV file\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'film_rate', 'plot', 'cover_url', 'cast', 'reviews', 'director', 'writer', 'genre'])\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['film_rate'], data['plot'], data['cover_url'], data['cast'], data['reviews'], data['director'], data['writer'], data['genre']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# missing features only ","metadata":{}},{"cell_type":"code","source":"import imdb\nimport csv\nimport pandas as pd\nimport concurrent.futures\nimport urllib.error\n\nimdbs = data['imdbId']\n\nia = imdb.IMDb()\n\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, info=['directors', 'genres'])  # Only updating required fields\n\n        data = {\n            'imdbId': movie_id,\n            'film_name': movie.get('title', ''),\n            'director': ', '.join([person['name'] for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        # Handle IMDb Data Access Error\n        return {'imdbId': movie_id, 'film_name': '', 'director': '', 'genre': ''}\n    except urllib.error.HTTPError as e:\n        # Handle HTTP Error\n        return {'imdbId': movie_id, 'film_name': '', 'director': '', 'genre': ''}\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Specify the path to save the CSV\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'\n\n# Save the processed data to a CSV file with only imdbId, film_name, director, and genre\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'director', 'genre'])  # Header row\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['director'], data['genre']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the path to save the CSV in the writable /kaggle/working directory\ncsv_file = '/kaggle/working/imdb_data.csv'  # Change the directory to /kaggle/working\n\n# Save the processed data to a CSV file with only imdbId, film_name, director, and genre\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'director', 'genre'])  # Header row\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['director'], data['genre']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# handling data more effectivly","metadata":{}},{"cell_type":"code","source":"import imdb\nimport csv\nimport pandas as pd\nimport concurrent.futures\nimport urllib.error\nimport time\nfrom tqdm import tqdm  # For progress tracking\n\n# Load the movie IDs (Assuming 'data' is a DataFrame that contains imdbId)\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data with retry logic\ndef get_movie_data(movie_id, retries=3):\n    for _ in range(retries):\n        try:\n            movie = ia.get_movie(movie_id)\n\n            # Fetch the required data without explicitly updating 'directors' or 'genres'\n            data = {\n                'imdbId': movie_id,\n                'film_name': movie.get('title', ''),\n                'director': ', '.join([person['name'] for person in movie.get('directors', [])]) if movie.get('directors') else '',\n                'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''\n            }\n            return data\n        except (imdb.IMDbDataAccessError, urllib.error.HTTPError) as e:\n            time.sleep(1)  # Wait for 1 second before retrying\n    # If retries are exhausted, return empty data\n    return {'imdbId': movie_id, 'film_name': '', 'director': '', 'genre': ''}\n\n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel and use tqdm for progress tracking\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results with progress tracking\n    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n        data_list.extend(future.result())\n\n# Specify the path to save the CSV\ncsv_file = '/kaggle/input/imdbdata/imdb_data.csv'\n\n# Save the processed data to a CSV file with only imdbId, film_name, director, and genre\nwith open(csv_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['imdbId', 'film_name', 'director', 'genre'])  # Header row\n    for data in data_list:\n        writer.writerow([data['imdbId'], data['film_name'], data['director'], data['genre']])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# updated needed features to current file","metadata":{}},{"cell_type":"code","source":"import imdb\nimport pandas as pd\nimport concurrent.futures\n\n# Extract the list of IMDb IDs\nimdbs = data['imdbId']\n\n# Set up the IMDb API\nia = imdb.IMDb()\n\n# Define the batch size for processing\nbatch_size = 100\n\n# Define the function to get movie data\ndef get_movie_data(movie_id):\n    try:\n        movie = ia.get_movie(movie_id)\n        ia.update(movie, 'reviews')\n        ia.update(movie, 'cast')\n        ia.update(movie, 'directors')\n        ia.update(movie, 'writers')\n\n        # Return the new data, using `.get()` to safely handle missing fields\n        data = {\n            'imdbId': movie_id,\n            'director': ', '.join([person.get('name', '') for person in movie.get('directors', [])]) if movie.get('directors') else '',\n            'writer': ', '.join([person.get('name', '') for person in movie.get('writers', [])]) if movie.get('writers') else '',\n            'genre': ', '.join(movie.get('genres', [])) if movie.get('genres') else ''  # join genre list\n        }\n        return data\n    except imdb.IMDbDataAccessError as e:\n        return {'imdbId': movie_id, 'director': '', 'writer': '', 'genre': ''}\n    \n# Define the function to process a batch of movie IDs\ndef process_batch(movie_ids):\n    data_list = []\n    for movie_id in movie_ids:\n        data_list.append(get_movie_data(movie_id))\n    return data_list\n\n# Create a list to store the processed data\ndata_list = []\n\n# Create a ThreadPoolExecutor to parallelize the API calls\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n    # Split the movie IDs into batches\n    batches = [imdbs[i:i + batch_size] for i in range(0, len(imdbs), batch_size)]\n    \n    # Process each batch in parallel\n    futures = [executor.submit(process_batch, batch) for batch in batches]\n    \n    # Collect the results\n    for future in concurrent.futures.as_completed(futures):\n        data_list.extend(future.result())\n\n# Convert the collected data into a DataFrame\nnew_data_df = pd.DataFrame(data_list)\n\n# Merge the new data with the existing dataset\nupdated_data = pd.merge(data, new_data_df, on='imdbId', how='left')\n\n# Save the updated data back to the CSV file\nupdated_data.to_csv(csv_file, index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# download the csv file","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Create a Pandas DataFrame from the data_list\ndf = pd.DataFrame(data_list)\n\ndf.to_csv('imdb_data.csv', index=False)\nfrom IPython.display import FileLink\n\n# This will create a download link for the file\nFileLink('imdb_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosication","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n\n# Load the dataset in chunks\nchunksize = 1000\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize)\n\n# Concatenate all chunks into a single DataFrame\nratings = pd.concat(chunks, ignore_index=True)\n\n# Filter only ratings of 4.0 and above for \"liked\" movies\nratings_filtered = ratings[ratings['rating'] >= 4.0]\n\n# Create a user-item matrix\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(user_movie_matrix, min_support=0.02, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Display the rules\nprint(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosiation model ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n\n# Load the dataset in chunks\nchunksize = 1000\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize)\n\n# Concatenate all chunks into a single DataFrame\nratings = pd.concat(chunks, ignore_index=True)\n\n# Filter only ratings of 4.0 and above for \"liked\" movies\nratings_filtered = ratings[ratings['rating'] >= 4.0]\n\n# Create a user-item matrix, where 1 represents a \"liked\" movie (rating 4.0 or above)\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(user_movie_matrix, min_support=0.02, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset to map movie IDs to movie titles\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    \"\"\"\n    movie_id = movies[movies['title'].str.contains(movie_title, case=False, na=False)]['movieId']\n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value (descending)\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get the top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n# Example usage:\nmovie_name = 'Interstellar'  # Change this to the movie title you want recommendations for\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # If the function returns an error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# assosiation model low resources","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom scipy.sparse import csr_matrix\n\n# Load the dataset in chunks to avoid memory overload\nchunksize = 10**6  # Increase the chunksize to process more data at once but keep it manageable\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize, usecols=['userId', 'movieId', 'rating'])\n\n# Initialize an empty list to store filtered data\nfiltered_chunks = []\n\n# Process each chunk individually\nfor chunk in chunks:\n    # Filter ratings of 4.0 and above for \"liked\" movies\n    filtered_chunk = chunk[chunk['rating'] >= 4.0]\n    filtered_chunks.append(filtered_chunk)\n\n# Concatenate all filtered chunks into a single DataFrame\nratings_filtered = pd.concat(filtered_chunks, ignore_index=True)\n\n# Create a user-item matrix in sparse format to save memory\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\nuser_movie_matrix_sparse = csr_matrix(user_movie_matrix.values)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(pd.DataFrame(user_movie_matrix_sparse.todense(), columns=user_movie_matrix.columns), min_support=0.02, use_colnames=True)\n\n# Generate association rules from the frequent item sets\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort the rules by lift to find the most interesting ones\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset to map movie IDs to movie titles\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    \"\"\"\n    movie_id = movies[movies['title'].str.contains(movie_title, case=False, na=False)]['movieId']\n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value (descending)\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get the top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n# Example usage:\nmovie_name = 'Interstellar'  # Change this to the movie title you want recommendations for\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # If the function returns an error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# final try ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\nfrom scipy.sparse import csr_matrix\nimport re\n\n# Function to clean movie titles by removing special characters and lowercasing\ndef clean_movie_title(title):\n    # Remove anything in parentheses (like year)\n    title = re.sub(r'\\(.*?\\)', '', title)\n    # Remove extra spaces and lowercase the title\n    title = re.sub(r'\\s+', ' ', title).strip().lower()\n    return title\n\n# Load the dataset in chunks\nchunksize = 10**6\nchunks = pd.read_csv(r'/kaggle/input/rating/ratings.csv', chunksize=chunksize, usecols=['userId', 'movieId', 'rating'])\n\n# Initialize an empty list to store filtered data\nfiltered_chunks = []\n\n# Process each chunk individually\nfor chunk in chunks:\n    # Filter ratings of 4.0 and above for \"liked\" movies\n    filtered_chunk = chunk[chunk['rating'] >= 4.0]\n    filtered_chunks.append(filtered_chunk)\n\n# Concatenate filtered chunks\nratings_filtered = pd.concat(filtered_chunks, ignore_index=True)\n\n# Create a user-item matrix in sparse format\nuser_movie_matrix = ratings_filtered.pivot_table(index='userId', columns='movieId', aggfunc='size', fill_value=0)\nuser_movie_matrix_sparse = csr_matrix(user_movie_matrix.values)\n\n# Apply FP-Growth to find frequent item sets\nfrequent_itemsets = fpgrowth(pd.DataFrame(user_movie_matrix_sparse.todense(), columns=user_movie_matrix.columns), min_support=0.05, use_colnames=True)\n\n# Generate association rules from frequent item sets\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n\n# Sort rules by lift\nrules = rules.sort_values(by='lift', ascending=False)\n\n# Load the movies dataset\nmovies = pd.read_csv(r'/kaggle/input/movielens/movies.csv')\n\n# Apply the clean_movie_title function to standardize movie titles\nmovies['clean_title'] = movies['title'].apply(clean_movie_title)\n\n# Create a function to get movie recommendations\ndef get_movie_id(movie_title):\n    \"\"\"\n    Get the movieId for the given movie title from the movies dataset.\n    Clean the input movie title for consistency.\n    \"\"\"\n    cleaned_title = clean_movie_title(movie_title)\n    movie_id = movies[movies['clean_title'] == cleaned_title]['movieId']\n    \n    if len(movie_id) > 0:\n        return movie_id.values[0]\n    else:\n        return None\n\ndef recommend_movies(movie_title, rules, movies, top_n=3):\n    \"\"\"\n    Recommend associated movies based on the input movie title.\n    \"\"\"\n    movie_id = get_movie_id(movie_title)\n    if movie_id is None:\n        return f\"Movie '{movie_title}' not found in the dataset.\"\n\n    # Find rules where the input movie is in the antecedents\n    matching_rules = rules[rules['antecedents'].apply(lambda x: movie_id in x)]\n\n    if matching_rules.empty:\n        return f\"No associated movies found for '{movie_title}'.\"\n\n    # Extract the consequents from the association rules\n    movie_recommendations = []\n    for _, row in matching_rules.iterrows():\n        for consequent in row['consequents']:\n            if consequent != movie_id:\n                movie_recommendations.append((consequent, row['lift']))\n\n    # Remove duplicates and sort by lift value\n    movie_recommendations = sorted(list(set(movie_recommendations)), key=lambda x: x[1], reverse=True)\n\n    # Get top N movie recommendations\n    recommended_movie_ids = [rec[0] for rec in movie_recommendations[:top_n]]\n    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]['title'].values\n\n    return recommended_movies\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nmovie_name = 'Heat (1995)'  # You can try different variations like 'interstellar'\nrecommended_movies = recommend_movies(movie_name, rules, movies)\n\nif isinstance(recommended_movies, str):\n    print(recommended_movies)  # Error message\nelse:\n    print(f\"Movies recommended based on '{movie_name}':\")\n    for i, movie in enumerate(recommended_movies, 1):\n        print(f\"{i}. {movie}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# cosine matrix improved","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'D:\\abdo\\AI\\projects\\recommender\\needed features\\imdb_data.csv')\n\n# Convert data to a DataFrame\ndf = pd.DataFrame(data)\n\n# Replace NaN values in relevant columns with an empty string\ndf['overview'] = df['overview'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('')\ndf['genre'] = df['genre'].fillna('')\n\n# Combine 'title', 'overview', 'cast', 'director', 'genre', and 'reviews' for similarity comparison\ndf['combined_features'] = df['title'] + ' ' + df['overview'] + ' ' + df['cast'] + ' ' + df['director'] + ' ' + df['genre'] + ' ' + df['reviews']\n\n# Text Vectorization using TF-IDF (including bi-grams for better context)\ntfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\ntfidf_matrix = tfidf.fit_transform(df['combined_features'].fillna(''))\n\n# Calculate cosine similarity between all movies\ncosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n# Helper function to extract movie title\ndef extract_title(title):\n    if pd.isna(title):\n        return ''\n    title = title.lower()\n    title = re.sub(r'\\s*\\(\\d{4}\\)$', '', title)  # Remove year information\n    return title\n\n# Define the function to recommend movies\ndef recommend_movies(movie_title, cosine_sim=cosine_sim, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage: Recommend movies based on the title \"Heat\"\ntest = 'heat'\nrecommended_movies = recommend_movies(test)\nprint(f\"Movies recommended based on '{test}':\")\nfor movie in recommended_movies:\n    print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, IMDb ID: {movie['imdbId']}\")\n    print(f\"Director: {movie['director']}, Genre: {movie['genre']}\")\n    print(f\"Cast: {movie['cast']}\")\n    print(f\"Overview: {movie['overview']}\")\n    print(f\"Reviews: {movie['reviews']}\")\n    print(f\"Cover URL: {movie['cover_url']}\")\n    print(\"-\" * 80)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-09-16T19:24:52.271527Z","iopub.execute_input":"2024-09-16T19:24:52.271941Z","iopub.status.idle":"2024-09-16T19:25:06.295972Z","shell.execute_reply.started":"2024-09-16T19:24:52.271903Z","shell.execute_reply":"2024-09-16T19:25:06.294856Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.62.2)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# lstm 256 patch size adam w ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Combine features into a single column for text processing\ndf['combined_features'] = df['overview'] + ' ' + df['cast'] + ' ' + df['director'] + ' ' + df['genre'] + ' ' + df['tag'] + ' ' + df['reviews']\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_sequence_len))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='AdamW', metrics=['accuracy'])\nmodel.summary()\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2, random_state=42)\n\n# Train the LSTM model\nmodel.fit(X_train, y_train, epochs=5, batch_size=256, validation_data=(X_test, y_test))\n\n# Generate embeddings from the trained LSTM model\nembedding_layer = Sequential()\nembedding_layer.add(model.layers[0])\nembedding_layer.add(model.layers[1])\nembedding_layer.compile(loss='binary_crossentropy', optimizer='AdamW')\n\n# Generate embeddings for all movies\nmovie_embeddings = embedding_layer.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d = np.mean(movie_embeddings, axis=1)\n\n# Calculate cosine similarity based on 2D embeddings\ncosine_sim_lstm = cosine_similarity(movie_embeddings_2d, movie_embeddings_2d)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    # Check if the title is NaN or not a string, return an empty string in that case\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings\ndef recommend_movies_lstm(movie_title, cosine_sim=cosine_sim_lstm, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n# Example usage with LSTM model\ntest_movie = 'heat'\nrecommended_movies_lstm = recommend_movies_lstm(test_movie)\n\nif not recommended_movies_lstm:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T19:52:02.498521Z","iopub.execute_input":"2024-09-16T19:52:02.499403Z","iopub.status.idle":"2024-09-16T19:52:38.813252Z","shell.execute_reply.started":"2024-09-16T19:52:02.499352Z","shell.execute_reply":"2024-09-16T19:52:38.812295Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_14\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 5.9395e-04 - loss: -2.3343 - val_accuracy: 0.0024 - val_loss: -14.6580\nEpoch 2/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 5.0738e-04 - loss: -19.2410 - val_accuracy: 0.0024 - val_loss: -35.8940\nEpoch 3/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 8.8278e-04 - loss: -44.2356 - val_accuracy: 0.0024 - val_loss: -74.9139\nEpoch 4/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 4.3675e-04 - loss: -89.7688 - val_accuracy: 0.0024 - val_loss: -141.4596\nEpoch 5/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 2.9908e-04 - loss: -165.9277 - val_accuracy: 0.0024 - val_loss: -245.6596\n\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\nMovies recommended based on 'heat':\nTitle: Hope Springs, Rating: 6.3, User Rating: 3.15, Tags: \nDirector: David Frankel\nGenre: Comedy, Drama, Romance\n--------------------------------------------------------------------------------\nTitle: The Prophecy, Rating: 6.4, User Rating: 3.2, Tags: \nDirector: Gregory Widen\nGenre: Action, Crime, Drama, Fantasy, Horror, Mystery, Thriller\n--------------------------------------------------------------------------------\nTitle: The Flintstones, Rating: 5.0, User Rating: 2.5, Tags: \nDirector: Brian Levant\nGenre: Comedy, Family, Fantasy\n--------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# lstm 256 batch adam","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Combine features into a single column for text processing\ndf['combined_features'] = df['overview'] + ' ' + df['cast'] + ' ' + df['director'] + ' ' + df['genre'] + ' ' + df['tag'] + ' ' + df['reviews']\n\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_sequence_len))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2, random_state=42)\n\n# Train the LSTM model\nmodel.fit(X_train, y_train, epochs=5, batch_size=256, validation_data=(X_test, y_test))\n\n# Generate embeddings from the trained LSTM model\nembedding_layer = Sequential()\nembedding_layer.add(model.layers[0])\nembedding_layer.add(model.layers[1])\nembedding_layer.compile(loss='binary_crossentropy', optimizer='Adam')\n\n# Generate embeddings for all movies\nmovie_embeddings = embedding_layer.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d = np.mean(movie_embeddings, axis=1)\n\n# Calculate cosine similarity based on 2D embeddings\ncosine_sim_lstm = cosine_similarity(movie_embeddings_2d, movie_embeddings_2d)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    # Check if the title is NaN or not a string, return an empty string in that case\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings\ndef recommend_movies_lstm(movie_title, cosine_sim=cosine_sim_lstm, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n# Example usage with LSTM model\ntest_movie = 'heat'\nrecommended_movies_lstm = recommend_movies_lstm(test_movie)\n\nif not recommended_movies_lstm:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# another try ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the movie dataset\ndata = pd.read_csv(r'/kaggle/input/movies-updated/movies_with_tags_rating.csv')\n\n# Preprocessing\ndf = pd.DataFrame(data)\ndf['overview'] = df['overview'].fillna('')\ndf['cast'] = df['cast'].fillna('')\ndf['director'] = df['director'].fillna('(no data about the director is found)')\ndf['genre'] = df['genre'].fillna('')\ndf['tag'] = df['tag'].fillna('')\ndf['reviews'] = df['reviews'].fillna('')\n\n# Feature Weighting\ndf['combined_features'] = (df['overview'] * 1 + ' ' +\n                           df['cast'] * 3 + ' ' + \n                           df['director'] * 4 + ' ' + \n                           df['genre'] * 5 + ' ' + \n                           df['tag'] * 2 + ' ' + \n                           df['reviews'] * 1)\n# Tokenization and padding\nmax_words = 10000\nmax_sequence_len = 300\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['combined_features'])\nsequences = tokenizer.texts_to_sequences(df['combined_features'])\npadded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n\n# Define LSTM model\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_sequence_len))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['rating'], test_size=0.2, random_state=42)\n\n# Train the LSTM model\nmodel.fit(X_train, y_train, epochs=5, batch_size=256, validation_data=(X_test, y_test))\n\n# Generate embeddings from the trained LSTM model\nembedding_layer = Sequential()\nembedding_layer.add(model.layers[0])\nembedding_layer.add(model.layers[1])\nembedding_layer.compile(loss='binary_crossentropy', optimizer='Adam')\n\n# Generate embeddings for all movies\nmovie_embeddings = embedding_layer.predict(padded_sequences)\n\n# Flatten the LSTM output by averaging over timesteps\nmovie_embeddings_2d = np.mean(movie_embeddings, axis=1)\n\n# Calculate cosine similarity based on 2D embeddings\ncosine_sim_lstm = cosine_similarity(movie_embeddings_2d, movie_embeddings_2d)\n\n# Function to clean movie titles (remove special characters and lowercase)\ndef extract_title(title):\n    if isinstance(title, str):\n        title_cleaned = title.lower()\n        title_cleaned = re.sub(r'\\W+', ' ', title_cleaned)\n        return title_cleaned.strip()\n    else:\n        return np.nan\n\n# Function to recommend movies based on LSTM embeddings\ndef recommend_movies_lstm(movie_title, cosine_sim=cosine_sim_lstm, df=df):\n    movie_title_clean = extract_title(movie_title)\n    \n    # Check if the movie title exists in the dataset\n    if movie_title_clean in df['title'].apply(extract_title).values:\n        # Get the index of the movie that matches the title\n        idx = df.index[df['title'].apply(extract_title) == movie_title_clean][0]\n        \n        # Get the pairwise similarity scores of all movies with that movie\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        \n        # Sort the movies based on similarity scores\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        \n        # Get the top 3 most similar movies (excluding the input movie itself)\n        recommended_movies = []\n        for score in sim_scores[1:]:\n            movie_data = df.iloc[score[0]]\n            recommended_movies.append({\n                'title': movie_data['title'],\n                'film_rate': movie_data['film_rate'],\n                'imdbId': movie_data['imdbId'],\n                'cover_url': movie_data['cover_url'],\n                'director': movie_data['director'],\n                'genre': movie_data['genre'],\n                'cast': movie_data['cast'],\n                'overview': movie_data['overview'],\n                'reviews': movie_data['reviews'],\n                'tag': movie_data['tag'],\n                'rating': movie_data['rating']\n            })\n            if len(recommended_movies) == 3:  # Limit to top 3 movies\n                break\n        \n        # Return the top 3 most similar movies with all requested details\n        return recommended_movies\n    else:\n        return []  # Return an empty list if the movie is not found\n\n# Example usage with LSTM model\ntest_movie = 'heat'\nrecommended_movies_lstm = recommend_movies_lstm(test_movie)\n\nif not recommended_movies_lstm:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:00:17.304374Z","iopub.execute_input":"2024-09-16T20:00:17.304805Z","iopub.status.idle":"2024-09-16T20:00:54.157644Z","shell.execute_reply.started":"2024-09-16T20:00:17.304769Z","shell.execute_reply":"2024-09-16T20:00:54.156641Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_18\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 6.4984e-04 - loss: -2.4309 - val_accuracy: 0.0024 - val_loss: -16.3625\nEpoch 2/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.0011 - loss: -22.1989 - val_accuracy: 0.0024 - val_loss: -43.5638\nEpoch 3/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 3.4469e-04 - loss: -54.4752 - val_accuracy: 0.0024 - val_loss: -92.9786\nEpoch 4/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 9.0351e-04 - loss: -112.7620 - val_accuracy: 0.0024 - val_loss: -181.9569\nEpoch 5/5\n\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 4.4829e-04 - loss: -215.2816 - val_accuracy: 0.0024 - val_loss: -328.8805\n\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\nMovies recommended based on 'heat':\nTitle: Little Women, Rating: 7.3, User Rating: 3.65, Tags: \nDirector: George Cukor\nGenre: Drama, Family, Romance, War\n--------------------------------------------------------------------------------\nTitle: Little Women, Rating: 7.3, User Rating: 3.65, Tags: \nDirector: Mervyn LeRoy\nGenre: Drama, Family, Romance\n--------------------------------------------------------------------------------\nTitle: Happiness, Rating: 7.7, User Rating: 3.85, Tags: \nDirector: Todd Solondz\nGenre: Comedy, Drama\n--------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"test_movie = 'interstellar'\nrecommended_movies_lstm = recommend_movies_lstm(test_movie)\n\nif not recommended_movies_lstm:\n    print(f\"'{test_movie}' is not listed in our database.\")\nelse:\n    print(f\"Movies recommended based on '{test_movie}':\")\n    for movie in recommended_movies_lstm:\n        print(f\"Title: {movie['title']}, Rating: {movie['film_rate']}, User Rating: {movie['rating']}, Tags: {movie['tag']}\")\n        print(f\"Director: {movie['director']}\")\n        print(f\"Genre: {movie['genre']}\")\n        print(\"-\" * 80)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T20:01:05.125790Z","iopub.execute_input":"2024-09-16T20:01:05.126301Z","iopub.status.idle":"2024-09-16T20:01:05.219431Z","shell.execute_reply.started":"2024-09-16T20:01:05.126248Z","shell.execute_reply":"2024-09-16T20:01:05.218420Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Movies recommended based on 'interstellar':\nTitle: Material Girls, Rating: 3.9, User Rating: 1.95, Tags: \nDirector: Martha Coolidge\nGenre: Comedy, Family, Romance\n--------------------------------------------------------------------------------\nTitle: Babe, Rating: 6.9, User Rating: 3.45, Tags: \nDirector: Chris Noonan\nGenre: Comedy, Drama, Family\n--------------------------------------------------------------------------------\nTitle: How to Make an American Quilt, Rating: 6.3, User Rating: 3.15, Tags: \nDirector: Jocelyn Moorhouse\nGenre: Comedy, Drama, Romance\n--------------------------------------------------------------------------------\n","output_type":"stream"}]}]}